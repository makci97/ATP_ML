{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 14pt\">MIPT, Advanced ML, Autumn 2017</span>\n",
    "\n",
    "<span style=\"font-size: 16pt\"> HW #3: ARIMAX, Compositions\n",
    "\n",
    "<span style=\"color:blue; font-size: 12pt\">Alexey Romanenko </span>,\n",
    "<span style=\"color:blue; font-size: 12pt; font-family: 'Verdana'\">alexromsput@gmail.com</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дополнительный материал для выполнения дз**:\n",
    "- Магнус Я.Р., Катышев П.К., Пересецкий А.А. Эконометрика. Начальный курс., глава 11\n",
    "- Лукашин Ю.П. Адаптивные методы краткосрочного прогнозирования временных рядов. Финансы и статистика. 2003, главы 1,4,5,7.\n",
    "\n",
    "**Оформление дз**: \n",
    "- Присылайте выполненное задание на почту ``ml.course.mipt@gmail.com``\n",
    "- Укажите тему письма в следующем формате ``ML2017_Aut_fall <номер_группы> <фамилия>``, к примеру -- ``ML2017_Aut_fall 401 ivanov``\n",
    "- Выполненное дз сохраните в файл ``<фамилия>_<группа>_task<номер>.ipnb``, к примеру -- ``ivanov_401_task1.ipnb``\n",
    "\n",
    "**Вопросы**:\n",
    "- Присылайте вопросы на почту ``ml.course.mipt@gmail.com``\n",
    "- Укажите тему письма в следующем формате ``ML2017_Aut_fall Question <Содержание вопроса>``\n",
    "\n",
    "<span style=\"color:red; font-size: 14pt;\"> DEADLINE: 10 October 2017 23:59:59 </span>\n",
    "\n",
    "--------\n",
    "- **PS1**: Мы используем автоматические фильтры, и просто не найдем ваше дз, если вы не аккуратно его подпишите.\n",
    "- **PS2**: Напоминаем, что дедлайны жесткие, письма пришедшие после автоматически удаляются =( чтобы соблазна не было \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Check Questions (20%)</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответе на вопросы своими словами (загугленный материал надо пересказать), ответ обоснуйте (напишите и ОБЪЯСНИТЕ формулки если потребуется), если не выходит, то вернитесь к лекции дополнительным материалам:\n",
    "\n",
    "**Вопрос 1** Опишите ситуацию (аргументированно), в котором композиция Adaptive Selection будет прогнозировать хуже, чем лучший из базовых алгоритмов.\n",
    "\n",
    "<Ответ>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос 2**  Опишите ситуацию (аргументированно), в котором композиция Adaptive Composition будет прогнозировать хуже, чем лучший из базовых алгоритмов.\n",
    "\n",
    "<Ответ>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос 3** Что значит смешиваемость (mixability) игры (в теории агрегирующего алгоритма)?\n",
    "\n",
    "<Ответ>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос 4**: Верно ли, что для смешиваемых игр процесс потерь любой композиции (любого алгоритма) будет расти со временем (с ростом $t$) не быстрее, чем процесс потерь лучшего из базовых алгоритмов? Если нет - приведит пример, когда это не так.\n",
    "\n",
    "### $$ \\mathrm{Loss}_{Composition}(T)\\leq \\inf\\limits_{BA} \\mathrm{Loss}_{BA}(T)+C$$\n",
    "\n",
    "<Ответ>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">2. Contest: electricity comsumption contest (80%)</h1>\n",
    "Take part in <a href='https://www.kaggle.com/t/de102b28cae74e6ead7492ff12efbc63'>contest</a>.\n",
    "<span style='color:red'> You need to send the final code that is used for building forecast.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train = pd.read_csv(\"train.csv\", index_col='ID')\n",
    "sample_submission = pd.read_csv(\"SubmissionSample.csv\", index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PREDICTED</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16707</th>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16708</th>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16709</th>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16710</th>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16711</th>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PREDICTED\n",
       "ID              \n",
       "16707     100000\n",
       "16708     100000\n",
       "16709     100000\n",
       "16710     100000\n",
       "16711     100000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>tsID</th>\n",
       "      <th>ACTUAL</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>27.10.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>32194.01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>28.10.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>31246.77</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>29.10.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>33454.83</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>30.10.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>33894.82</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>31.10.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>33838.25</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>01.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>02.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>03.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>04.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>05.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>06.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>07.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>08.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>09.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>10.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>11.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>12.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>13.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>14.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>15.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>16.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>17.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>18.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>19.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>20.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>21.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>22.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>23.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>24.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>25.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>26.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>27.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>28.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>29.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>30.11.2012</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>01.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>34904.00</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>02.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>34681.00</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>03.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>35163.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>04.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>35921.00</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>05.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>35493.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>06.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>35492.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>07.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>34940.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>08.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>35423.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>09.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>39517.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>10.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>39715.00</td>\n",
       "      <td>-3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>11.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>39437.00</td>\n",
       "      <td>-9.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>12.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>37099.00</td>\n",
       "      <td>-10.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>13.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>36721.00</td>\n",
       "      <td>-11.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>14.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>39278.00</td>\n",
       "      <td>-13.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>15.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>39402.00</td>\n",
       "      <td>-13.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>16.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>39978.00</td>\n",
       "      <td>-13.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>17.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>39820.00</td>\n",
       "      <td>-16.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>18.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>38967.00</td>\n",
       "      <td>-18.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>19.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>37108.00</td>\n",
       "      <td>-17.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>20.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>35998.00</td>\n",
       "      <td>-14.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>21.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>38513.00</td>\n",
       "      <td>-13.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>22.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>38943.00</td>\n",
       "      <td>-12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>23.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>39997.00</td>\n",
       "      <td>-16.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>24.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>40213.00</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>25.01.2010</td>\n",
       "      <td>15</td>\n",
       "      <td>40343.00</td>\n",
       "      <td>-16.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  tsID    ACTUAL  Temp  Saturday  Sunday\n",
       "ID                                                      \n",
       "1031  27.10.2012     4  32194.01   4.0       1.0       0\n",
       "1032  28.10.2012     4  31246.77   5.0       0.0       1\n",
       "1033  29.10.2012     4  33454.83  10.5       0.0       0\n",
       "1034  30.10.2012     4  33894.82   8.8       0.0       0\n",
       "1035  31.10.2012     4  33838.25  10.5       0.0       0\n",
       "1036  01.11.2012     4       NaN   3.0       0.0       0\n",
       "1037  02.11.2012     4       NaN   1.8       0.0       0\n",
       "1038  03.11.2012     4       NaN   0.0       1.0       0\n",
       "1039  04.11.2012     4       NaN   2.0       0.0       1\n",
       "1040  05.11.2012     4       NaN   7.8       0.0       1\n",
       "1041  06.11.2012     4       NaN  10.2       0.0       0\n",
       "1042  07.11.2012     4       NaN   7.5       0.0       0\n",
       "1043  08.11.2012     4       NaN   4.2       0.0       0\n",
       "1044  09.11.2012     4       NaN   5.0       0.0       0\n",
       "1045  10.11.2012     4       NaN   3.2       1.0       0\n",
       "1046  11.11.2012     4       NaN  -2.0       0.0       1\n",
       "1047  12.11.2012     4       NaN  -2.2       0.0       0\n",
       "1048  13.11.2012     4       NaN   1.5       0.0       0\n",
       "1049  14.11.2012     4       NaN   1.8       0.0       0\n",
       "1050  15.11.2012     4       NaN   0.0       0.0       0\n",
       "1051  16.11.2012     4       NaN   2.8       0.0       0\n",
       "1052  17.11.2012     4       NaN   3.0       1.0       0\n",
       "1053  18.11.2012     4       NaN   1.5       0.0       1\n",
       "1054  19.11.2012     4       NaN   0.8       0.0       0\n",
       "1055  20.11.2012     4       NaN  -0.2       0.0       0\n",
       "1056  21.11.2012     4       NaN   0.5       0.0       0\n",
       "1057  22.11.2012     4       NaN   0.5       0.0       0\n",
       "1058  23.11.2012     4       NaN  -0.2       0.0       0\n",
       "1059  24.11.2012     4       NaN  -1.0       1.0       0\n",
       "1060  25.11.2012     4       NaN  -3.0       0.0       1\n",
       "1061  26.11.2012     4       NaN  -1.8       0.0       0\n",
       "1062  27.11.2012     4       NaN  -1.5       0.0       0\n",
       "1063  28.11.2012     4       NaN  -1.5       0.0       0\n",
       "1064  29.11.2012     4       NaN  -1.0       0.0       0\n",
       "1065  30.11.2012     4       NaN   4.0       0.0       0\n",
       "1097  01.01.2010    15  34904.00   6.2       0.0       1\n",
       "1098  02.01.2010    15  34681.00   6.8       0.0       1\n",
       "1099  03.01.2010    15  35163.00   6.0       0.0       1\n",
       "1100  04.01.2010    15  35921.00   5.8       0.0       1\n",
       "1101  05.01.2010    15  35493.00   1.8       1.0       0\n",
       "1102  06.01.2010    15  35492.00   0.0       0.0       1\n",
       "1103  07.01.2010    15  34940.00   0.2       0.0       1\n",
       "1104  08.01.2010    15  35423.00   1.2       0.0       1\n",
       "1105  09.01.2010    15  39517.00   1.0       0.0       0\n",
       "1106  10.01.2010    15  39715.00  -3.2       0.0       0\n",
       "1107  11.01.2010    15  39437.00  -9.5       0.0       0\n",
       "1108  12.01.2010    15  37099.00 -10.8       1.0       0\n",
       "1109  13.01.2010    15  36721.00 -11.8       0.0       1\n",
       "1110  14.01.2010    15  39278.00 -13.8       0.0       0\n",
       "1111  15.01.2010    15  39402.00 -13.5       0.0       0\n",
       "1112  16.01.2010    15  39978.00 -13.2       0.0       0\n",
       "1113  17.01.2010    15  39820.00 -16.2       0.0       0\n",
       "1114  18.01.2010    15  38967.00 -18.8       0.0       0\n",
       "1115  19.01.2010    15  37108.00 -17.5       1.0       0\n",
       "1116  20.01.2010    15  35998.00 -14.2       0.0       1\n",
       "1117  21.01.2010    15  38513.00 -13.8       0.0       0\n",
       "1118  22.01.2010    15  38943.00 -12.8       0.0       0\n",
       "1119  23.01.2010    15  39997.00 -16.8       0.0       0\n",
       "1120  24.01.2010    15  40213.00 -19.0       0.0       0\n",
       "1121  25.01.2010    15  40343.00 -16.8       0.0       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train[1030:1090]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}\n"
     ]
    }
   ],
   "source": [
    "ids = set(ind[0] for ind in Train[['tsID']].values)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['01.01.2010', 22, 407219.0, 13.0, 0.0, 1],\n",
       "       ['02.01.2010', 22, 412267.0, 9.0, 0.0, 1],\n",
       "       ['03.01.2010', 22, 420959.0, 7.8, 0.0, 1],\n",
       "       ..., \n",
       "       ['10.05.2012', 22, nan, -26.7, 0.0, 1],\n",
       "       ['11.05.2012', 22, nan, -19.6, 0.0, 0],\n",
       "       ['12.05.2012', 22, nan, -12.9, 0.0, 0]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train[Train['tsID'] == 22].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For first ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DAYS_FOR_PREDICT = 30\n",
    "cur_tsID = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    scaler = 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_ts(train, cur_tsID):\n",
    "        ts = train[train['tsID'] == cur_tsID].drop(['tsID'], axis=1)\n",
    "        ts.index = [pd.Timestamp(x) for x in ts['Date']]\n",
    "        ts = ts.drop(labels='Date', axis=1)\n",
    "        return ts.dropna()\n",
    "        \n",
    "    @staticmethod\n",
    "    def scale(ts):\n",
    "        Preprocessor.scaler = MinMaxScaler()\n",
    "        return Preprocessor.scaler.fit_transform(ts.values)\n",
    "    \n",
    "    @staticmethod\n",
    "    def invert_scaling(y):\n",
    "        inv_y = np.array([[[actual, 0, 0, 0] for actual in line] for line in y])\n",
    "        inv_y = inv_y.reshape((inv_y.shape[0], inv_y.shape[1], 4))\n",
    "\n",
    "        inv_y = np.array([Preprocessor.scaler.inverse_transform(inv_y[i]) for i, v in enumerate(inv_y)])\n",
    "        inv_y = inv_y[:,:, 0]\n",
    "        return inv_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = Preprocessor.prepare_ts(Train, cur_tsID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACTUAL</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>90468.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-01</th>\n",
       "      <td>92764.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-01</th>\n",
       "      <td>94791.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-01</th>\n",
       "      <td>98010.0</td>\n",
       "      <td>16.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-01</th>\n",
       "      <td>100975.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ACTUAL  Temp  Saturday  Sunday\n",
       "2010-01-01   90468.0  22.5       0.0       1\n",
       "2010-02-01   92764.0  20.0       1.0       0\n",
       "2010-03-01   94791.0  21.0       0.0       1\n",
       "2010-04-01   98010.0  16.8       0.0       1\n",
       "2010-05-01  100975.0  19.2       0.0       1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x117668588>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD6CAYAAABd9xscAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117645908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ts['ACTUAL'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled = Preprocessor.scale(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1035, 4)\n",
      "[[ 0.43329581  0.90243902  0.          1.        ]\n",
      " [ 0.47368421  0.86178862  1.          0.        ]\n",
      " [ 0.5093407   0.87804878  0.          1.        ]\n",
      " ..., \n",
      " [ 0.7128483   0.40650407  0.          0.        ]\n",
      " [ 0.71411483  0.37398374  0.          0.        ]\n",
      " [ 0.73237405  0.3902439   0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(scaled.shape)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SamplesGenerator:\n",
    "    \n",
    "    @staticmethod\n",
    "    def _clean_supervised(n_vars, n_out):\n",
    "        # names of columns, which won'n predict\n",
    "        n_vars = 1 if type(scaled) is list else scaled.shape[1]\n",
    "        bad_names = list()\n",
    "        for i in range(0, n_out):\n",
    "            if i == 0:\n",
    "                bad_names += [('var%d(t)' % (j+1)) for j in range(1, n_vars)]\n",
    "            else:\n",
    "                bad_names += [('var%d(t+%d)' % (j+1, i)) for j in range(1, n_vars)]\n",
    "        return bad_names\n",
    "    \n",
    "    @staticmethod\n",
    "    def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "        n_vars = 1 if type(data) is list else data.shape[1]\n",
    "        df = pd.DataFrame(data)\n",
    "        cols, names = list(), list()\n",
    "        # input sequence (t-n, ... t-1)\n",
    "        for i in range(n_in, 0, -1):\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        # forecast sequence (t, t+1, ... t+n)\n",
    "        for i in range(0, n_out):\n",
    "            cols.append(df.shift(-i))\n",
    "            if i == 0:\n",
    "                names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "            else:\n",
    "                names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        # put it all together\n",
    "        agg = pd.concat(cols, axis=1)\n",
    "        agg.columns = names\n",
    "        # drop rows with NaN values\n",
    "        if dropnan:\n",
    "            agg.dropna(inplace=True)\n",
    "            \n",
    "        # drop columns, which won'n predict\n",
    "        bad_names = SamplesGenerator._clean_supervised(n_vars, n_out)\n",
    "        agg = agg.drop(labels=bad_names, axis=1)\n",
    "        return agg\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_and_test(reframed, test_prop=0.2):\n",
    "        # split into train and test sets\n",
    "        values = reframed.values\n",
    "        if isinstance(test_prop, float):\n",
    "            n_train_days = (int)(reframed.shape[0] * (1-test_prop))\n",
    "        elif isinstance(test_prop, int):\n",
    "            n_train_days = (int)(reframed.shape[0] -test_prop)\n",
    "        train = values[:n_train_days, :]\n",
    "        test = values[n_train_days:, :]\n",
    "        # split into input and outputs\n",
    "        train_X, train_y = train[:, :-DAYS_FOR_PREDICT], train[:, -DAYS_FOR_PREDICT:]\n",
    "        test_X, test_y = test[:, :-DAYS_FOR_PREDICT], test[:, -DAYS_FOR_PREDICT:]\n",
    "        return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reframed = SamplesGenerator.series_to_supervised(scaled, n_in=(int)(2*DAYS_FOR_PREDICT), n_out=DAYS_FOR_PREDICT)\n",
    "#reframed.drop(reframed.columns[[22, 23]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var1(t-60)', 'var2(t-60)', 'var3(t-60)', 'var4(t-60)', 'var1(t-59)',\n",
       "       'var2(t-59)', 'var3(t-59)', 'var4(t-59)', 'var1(t-58)', 'var2(t-58)',\n",
       "       ...\n",
       "       'var1(t+20)', 'var1(t+21)', 'var1(t+22)', 'var1(t+23)', 'var1(t+24)',\n",
       "       'var1(t+25)', 'var1(t+26)', 'var1(t+27)', 'var1(t+28)', 'var1(t+29)'],\n",
       "      dtype='object', length=270)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reframed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-60)</th>\n",
       "      <th>var2(t-60)</th>\n",
       "      <th>var3(t-60)</th>\n",
       "      <th>var4(t-60)</th>\n",
       "      <th>var1(t-59)</th>\n",
       "      <th>var2(t-59)</th>\n",
       "      <th>var3(t-59)</th>\n",
       "      <th>var4(t-59)</th>\n",
       "      <th>var1(t-58)</th>\n",
       "      <th>var2(t-58)</th>\n",
       "      <th>...</th>\n",
       "      <th>var1(t+20)</th>\n",
       "      <th>var1(t+21)</th>\n",
       "      <th>var1(t+22)</th>\n",
       "      <th>var1(t+23)</th>\n",
       "      <th>var1(t+24)</th>\n",
       "      <th>var1(t+25)</th>\n",
       "      <th>var1(t+26)</th>\n",
       "      <th>var1(t+27)</th>\n",
       "      <th>var1(t+28)</th>\n",
       "      <th>var1(t+29)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.433296</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.861789</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509341</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669628</td>\n",
       "      <td>0.676189</td>\n",
       "      <td>0.659601</td>\n",
       "      <td>0.641711</td>\n",
       "      <td>0.617242</td>\n",
       "      <td>0.468882</td>\n",
       "      <td>0.364745</td>\n",
       "      <td>0.575975</td>\n",
       "      <td>0.555042</td>\n",
       "      <td>0.532437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.861789</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509341</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.565965</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676189</td>\n",
       "      <td>0.659601</td>\n",
       "      <td>0.641711</td>\n",
       "      <td>0.617242</td>\n",
       "      <td>0.468882</td>\n",
       "      <td>0.364745</td>\n",
       "      <td>0.575975</td>\n",
       "      <td>0.555042</td>\n",
       "      <td>0.532437</td>\n",
       "      <td>0.521901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.509341</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.565965</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618122</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659601</td>\n",
       "      <td>0.641711</td>\n",
       "      <td>0.617242</td>\n",
       "      <td>0.468882</td>\n",
       "      <td>0.364745</td>\n",
       "      <td>0.575975</td>\n",
       "      <td>0.555042</td>\n",
       "      <td>0.532437</td>\n",
       "      <td>0.521901</td>\n",
       "      <td>0.489551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.565965</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618122</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665687</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641711</td>\n",
       "      <td>0.617242</td>\n",
       "      <td>0.468882</td>\n",
       "      <td>0.364745</td>\n",
       "      <td>0.575975</td>\n",
       "      <td>0.555042</td>\n",
       "      <td>0.532437</td>\n",
       "      <td>0.521901</td>\n",
       "      <td>0.489551</td>\n",
       "      <td>0.359854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.618122</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665687</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.664016</td>\n",
       "      <td>0.822764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617242</td>\n",
       "      <td>0.468882</td>\n",
       "      <td>0.364745</td>\n",
       "      <td>0.575975</td>\n",
       "      <td>0.555042</td>\n",
       "      <td>0.532437</td>\n",
       "      <td>0.521901</td>\n",
       "      <td>0.489551</td>\n",
       "      <td>0.359854</td>\n",
       "      <td>0.322280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.665687</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.664016</td>\n",
       "      <td>0.822764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.685547</td>\n",
       "      <td>0.845528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.468882</td>\n",
       "      <td>0.364745</td>\n",
       "      <td>0.575975</td>\n",
       "      <td>0.555042</td>\n",
       "      <td>0.532437</td>\n",
       "      <td>0.521901</td>\n",
       "      <td>0.489551</td>\n",
       "      <td>0.359854</td>\n",
       "      <td>0.322280</td>\n",
       "      <td>0.484450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.664016</td>\n",
       "      <td>0.822764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.685547</td>\n",
       "      <td>0.845528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.661114</td>\n",
       "      <td>0.788618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364745</td>\n",
       "      <td>0.575975</td>\n",
       "      <td>0.555042</td>\n",
       "      <td>0.532437</td>\n",
       "      <td>0.521901</td>\n",
       "      <td>0.489551</td>\n",
       "      <td>0.359854</td>\n",
       "      <td>0.322280</td>\n",
       "      <td>0.484450</td>\n",
       "      <td>0.413876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.685547</td>\n",
       "      <td>0.845528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.661114</td>\n",
       "      <td>0.788618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.655309</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575975</td>\n",
       "      <td>0.555042</td>\n",
       "      <td>0.532437</td>\n",
       "      <td>0.521901</td>\n",
       "      <td>0.489551</td>\n",
       "      <td>0.359854</td>\n",
       "      <td>0.322280</td>\n",
       "      <td>0.484450</td>\n",
       "      <td>0.413876</td>\n",
       "      <td>0.390761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.661114</td>\n",
       "      <td>0.788618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.655309</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.843249</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555042</td>\n",
       "      <td>0.532437</td>\n",
       "      <td>0.521901</td>\n",
       "      <td>0.489551</td>\n",
       "      <td>0.359854</td>\n",
       "      <td>0.322280</td>\n",
       "      <td>0.484450</td>\n",
       "      <td>0.413876</td>\n",
       "      <td>0.390761</td>\n",
       "      <td>0.389864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.655309</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.843249</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.853064</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.532437</td>\n",
       "      <td>0.521901</td>\n",
       "      <td>0.489551</td>\n",
       "      <td>0.359854</td>\n",
       "      <td>0.322280</td>\n",
       "      <td>0.484450</td>\n",
       "      <td>0.413876</td>\n",
       "      <td>0.390761</td>\n",
       "      <td>0.389864</td>\n",
       "      <td>0.404904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.843249</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.853064</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.852554</td>\n",
       "      <td>0.681301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.521901</td>\n",
       "      <td>0.489551</td>\n",
       "      <td>0.359854</td>\n",
       "      <td>0.322280</td>\n",
       "      <td>0.484450</td>\n",
       "      <td>0.413876</td>\n",
       "      <td>0.390761</td>\n",
       "      <td>0.389864</td>\n",
       "      <td>0.404904</td>\n",
       "      <td>0.301435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.853064</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.852554</td>\n",
       "      <td>0.681301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.835790</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489551</td>\n",
       "      <td>0.359854</td>\n",
       "      <td>0.322280</td>\n",
       "      <td>0.484450</td>\n",
       "      <td>0.413876</td>\n",
       "      <td>0.390761</td>\n",
       "      <td>0.389864</td>\n",
       "      <td>0.404904</td>\n",
       "      <td>0.301435</td>\n",
       "      <td>0.262067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.852554</td>\n",
       "      <td>0.681301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.835790</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.822140</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359854</td>\n",
       "      <td>0.322280</td>\n",
       "      <td>0.484450</td>\n",
       "      <td>0.413876</td>\n",
       "      <td>0.390761</td>\n",
       "      <td>0.389864</td>\n",
       "      <td>0.404904</td>\n",
       "      <td>0.301435</td>\n",
       "      <td>0.262067</td>\n",
       "      <td>0.420156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.835790</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.822140</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.677315</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322280</td>\n",
       "      <td>0.484450</td>\n",
       "      <td>0.413876</td>\n",
       "      <td>0.390761</td>\n",
       "      <td>0.389864</td>\n",
       "      <td>0.404904</td>\n",
       "      <td>0.301435</td>\n",
       "      <td>0.262067</td>\n",
       "      <td>0.420156</td>\n",
       "      <td>0.453842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.822140</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.677315</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.661096</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484450</td>\n",
       "      <td>0.413876</td>\n",
       "      <td>0.390761</td>\n",
       "      <td>0.389864</td>\n",
       "      <td>0.404904</td>\n",
       "      <td>0.301435</td>\n",
       "      <td>0.262067</td>\n",
       "      <td>0.420156</td>\n",
       "      <td>0.453842</td>\n",
       "      <td>0.475373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.677315</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.661096</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.828613</td>\n",
       "      <td>0.689431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413876</td>\n",
       "      <td>0.390761</td>\n",
       "      <td>0.389864</td>\n",
       "      <td>0.404904</td>\n",
       "      <td>0.301435</td>\n",
       "      <td>0.262067</td>\n",
       "      <td>0.420156</td>\n",
       "      <td>0.453842</td>\n",
       "      <td>0.475373</td>\n",
       "      <td>0.472066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.661096</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.828613</td>\n",
       "      <td>0.689431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.862898</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.390761</td>\n",
       "      <td>0.389864</td>\n",
       "      <td>0.404904</td>\n",
       "      <td>0.301435</td>\n",
       "      <td>0.262067</td>\n",
       "      <td>0.420156</td>\n",
       "      <td>0.453842</td>\n",
       "      <td>0.475373</td>\n",
       "      <td>0.472066</td>\n",
       "      <td>0.452259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.828613</td>\n",
       "      <td>0.689431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.862898</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.881280</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389864</td>\n",
       "      <td>0.404904</td>\n",
       "      <td>0.301435</td>\n",
       "      <td>0.262067</td>\n",
       "      <td>0.420156</td>\n",
       "      <td>0.453842</td>\n",
       "      <td>0.475373</td>\n",
       "      <td>0.472066</td>\n",
       "      <td>0.452259</td>\n",
       "      <td>0.322615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.862898</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.881280</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877920</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404904</td>\n",
       "      <td>0.301435</td>\n",
       "      <td>0.262067</td>\n",
       "      <td>0.420156</td>\n",
       "      <td>0.453842</td>\n",
       "      <td>0.475373</td>\n",
       "      <td>0.472066</td>\n",
       "      <td>0.452259</td>\n",
       "      <td>0.322615</td>\n",
       "      <td>0.274486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.881280</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877920</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.876654</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.301435</td>\n",
       "      <td>0.262067</td>\n",
       "      <td>0.420156</td>\n",
       "      <td>0.453842</td>\n",
       "      <td>0.475373</td>\n",
       "      <td>0.472066</td>\n",
       "      <td>0.452259</td>\n",
       "      <td>0.322615</td>\n",
       "      <td>0.274486</td>\n",
       "      <td>0.440684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.877920</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.876654</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.764090</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262067</td>\n",
       "      <td>0.420156</td>\n",
       "      <td>0.453842</td>\n",
       "      <td>0.475373</td>\n",
       "      <td>0.472066</td>\n",
       "      <td>0.452259</td>\n",
       "      <td>0.322615</td>\n",
       "      <td>0.274486</td>\n",
       "      <td>0.440684</td>\n",
       "      <td>0.430323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.876654</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.764090</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.723244</td>\n",
       "      <td>0.747967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420156</td>\n",
       "      <td>0.453842</td>\n",
       "      <td>0.475373</td>\n",
       "      <td>0.472066</td>\n",
       "      <td>0.452259</td>\n",
       "      <td>0.322615</td>\n",
       "      <td>0.274486</td>\n",
       "      <td>0.440684</td>\n",
       "      <td>0.430323</td>\n",
       "      <td>0.405801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.764090</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.723244</td>\n",
       "      <td>0.747967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.894227</td>\n",
       "      <td>0.663415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453842</td>\n",
       "      <td>0.475373</td>\n",
       "      <td>0.472066</td>\n",
       "      <td>0.452259</td>\n",
       "      <td>0.322615</td>\n",
       "      <td>0.274486</td>\n",
       "      <td>0.440684</td>\n",
       "      <td>0.430323</td>\n",
       "      <td>0.405801</td>\n",
       "      <td>0.402547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.723244</td>\n",
       "      <td>0.747967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.894227</td>\n",
       "      <td>0.663415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.920120</td>\n",
       "      <td>0.650407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475373</td>\n",
       "      <td>0.472066</td>\n",
       "      <td>0.452259</td>\n",
       "      <td>0.322615</td>\n",
       "      <td>0.274486</td>\n",
       "      <td>0.440684</td>\n",
       "      <td>0.430323</td>\n",
       "      <td>0.405801</td>\n",
       "      <td>0.402547</td>\n",
       "      <td>0.381808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.894227</td>\n",
       "      <td>0.663415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.920120</td>\n",
       "      <td>0.650407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907561</td>\n",
       "      <td>0.678049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.472066</td>\n",
       "      <td>0.452259</td>\n",
       "      <td>0.322615</td>\n",
       "      <td>0.274486</td>\n",
       "      <td>0.440684</td>\n",
       "      <td>0.430323</td>\n",
       "      <td>0.405801</td>\n",
       "      <td>0.402547</td>\n",
       "      <td>0.381808</td>\n",
       "      <td>0.258039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.920120</td>\n",
       "      <td>0.650407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907561</td>\n",
       "      <td>0.678049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.902090</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452259</td>\n",
       "      <td>0.322615</td>\n",
       "      <td>0.274486</td>\n",
       "      <td>0.440684</td>\n",
       "      <td>0.430323</td>\n",
       "      <td>0.405801</td>\n",
       "      <td>0.402547</td>\n",
       "      <td>0.381808</td>\n",
       "      <td>0.258039</td>\n",
       "      <td>0.220448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.907561</td>\n",
       "      <td>0.678049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.902090</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.900331</td>\n",
       "      <td>0.691057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322615</td>\n",
       "      <td>0.274486</td>\n",
       "      <td>0.440684</td>\n",
       "      <td>0.430323</td>\n",
       "      <td>0.405801</td>\n",
       "      <td>0.402547</td>\n",
       "      <td>0.381808</td>\n",
       "      <td>0.258039</td>\n",
       "      <td>0.220448</td>\n",
       "      <td>0.371675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.902090</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.900331</td>\n",
       "      <td>0.691057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.772059</td>\n",
       "      <td>0.635772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274486</td>\n",
       "      <td>0.440684</td>\n",
       "      <td>0.430323</td>\n",
       "      <td>0.405801</td>\n",
       "      <td>0.402547</td>\n",
       "      <td>0.381808</td>\n",
       "      <td>0.258039</td>\n",
       "      <td>0.220448</td>\n",
       "      <td>0.371675</td>\n",
       "      <td>0.391430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.900331</td>\n",
       "      <td>0.691057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.772059</td>\n",
       "      <td>0.635772</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.740782</td>\n",
       "      <td>0.640650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440684</td>\n",
       "      <td>0.430323</td>\n",
       "      <td>0.405801</td>\n",
       "      <td>0.402547</td>\n",
       "      <td>0.381808</td>\n",
       "      <td>0.258039</td>\n",
       "      <td>0.220448</td>\n",
       "      <td>0.371675</td>\n",
       "      <td>0.391430</td>\n",
       "      <td>0.418449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.772059</td>\n",
       "      <td>0.635772</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.740782</td>\n",
       "      <td>0.640650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.879503</td>\n",
       "      <td>0.694309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430323</td>\n",
       "      <td>0.405801</td>\n",
       "      <td>0.402547</td>\n",
       "      <td>0.381808</td>\n",
       "      <td>0.258039</td>\n",
       "      <td>0.220448</td>\n",
       "      <td>0.371675</td>\n",
       "      <td>0.391430</td>\n",
       "      <td>0.418449</td>\n",
       "      <td>0.394174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>0.321964</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>0.569106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183806</td>\n",
       "      <td>0.552846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275683</td>\n",
       "      <td>0.460333</td>\n",
       "      <td>0.482216</td>\n",
       "      <td>0.492893</td>\n",
       "      <td>0.497696</td>\n",
       "      <td>0.499754</td>\n",
       "      <td>0.350074</td>\n",
       "      <td>0.291743</td>\n",
       "      <td>0.499912</td>\n",
       "      <td>0.528163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>0.314066</td>\n",
       "      <td>0.569106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183806</td>\n",
       "      <td>0.552846</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.460333</td>\n",
       "      <td>0.482216</td>\n",
       "      <td>0.492893</td>\n",
       "      <td>0.497696</td>\n",
       "      <td>0.499754</td>\n",
       "      <td>0.350074</td>\n",
       "      <td>0.291743</td>\n",
       "      <td>0.499912</td>\n",
       "      <td>0.528163</td>\n",
       "      <td>0.528708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>0.183806</td>\n",
       "      <td>0.552846</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.288665</td>\n",
       "      <td>0.471545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482216</td>\n",
       "      <td>0.492893</td>\n",
       "      <td>0.497696</td>\n",
       "      <td>0.499754</td>\n",
       "      <td>0.350074</td>\n",
       "      <td>0.291743</td>\n",
       "      <td>0.499912</td>\n",
       "      <td>0.528163</td>\n",
       "      <td>0.528708</td>\n",
       "      <td>0.544206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.288665</td>\n",
       "      <td>0.471545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314189</td>\n",
       "      <td>0.325203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.492893</td>\n",
       "      <td>0.497696</td>\n",
       "      <td>0.499754</td>\n",
       "      <td>0.350074</td>\n",
       "      <td>0.291743</td>\n",
       "      <td>0.499912</td>\n",
       "      <td>0.528163</td>\n",
       "      <td>0.528708</td>\n",
       "      <td>0.544206</td>\n",
       "      <td>0.544487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0.288665</td>\n",
       "      <td>0.471545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314189</td>\n",
       "      <td>0.325203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325640</td>\n",
       "      <td>0.308943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497696</td>\n",
       "      <td>0.499754</td>\n",
       "      <td>0.350074</td>\n",
       "      <td>0.291743</td>\n",
       "      <td>0.499912</td>\n",
       "      <td>0.528163</td>\n",
       "      <td>0.528708</td>\n",
       "      <td>0.544206</td>\n",
       "      <td>0.544487</td>\n",
       "      <td>0.391324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>0.314189</td>\n",
       "      <td>0.325203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325640</td>\n",
       "      <td>0.308943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317302</td>\n",
       "      <td>0.357724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499754</td>\n",
       "      <td>0.350074</td>\n",
       "      <td>0.291743</td>\n",
       "      <td>0.499912</td>\n",
       "      <td>0.528163</td>\n",
       "      <td>0.528708</td>\n",
       "      <td>0.544206</td>\n",
       "      <td>0.544487</td>\n",
       "      <td>0.391324</td>\n",
       "      <td>0.336072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>0.325640</td>\n",
       "      <td>0.308943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317302</td>\n",
       "      <td>0.357724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.329493</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350074</td>\n",
       "      <td>0.291743</td>\n",
       "      <td>0.499912</td>\n",
       "      <td>0.528163</td>\n",
       "      <td>0.528708</td>\n",
       "      <td>0.544206</td>\n",
       "      <td>0.544487</td>\n",
       "      <td>0.391324</td>\n",
       "      <td>0.336072</td>\n",
       "      <td>0.538049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>0.317302</td>\n",
       "      <td>0.357724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.329493</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182539</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291743</td>\n",
       "      <td>0.499912</td>\n",
       "      <td>0.528163</td>\n",
       "      <td>0.528708</td>\n",
       "      <td>0.544206</td>\n",
       "      <td>0.544487</td>\n",
       "      <td>0.391324</td>\n",
       "      <td>0.336072</td>\n",
       "      <td>0.538049</td>\n",
       "      <td>0.531118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>0.329493</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182539</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150964</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499912</td>\n",
       "      <td>0.528163</td>\n",
       "      <td>0.528708</td>\n",
       "      <td>0.544206</td>\n",
       "      <td>0.544487</td>\n",
       "      <td>0.391324</td>\n",
       "      <td>0.336072</td>\n",
       "      <td>0.538049</td>\n",
       "      <td>0.531118</td>\n",
       "      <td>0.510150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>0.182539</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150964</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.313309</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528163</td>\n",
       "      <td>0.528708</td>\n",
       "      <td>0.544206</td>\n",
       "      <td>0.544487</td>\n",
       "      <td>0.391324</td>\n",
       "      <td>0.336072</td>\n",
       "      <td>0.538049</td>\n",
       "      <td>0.531118</td>\n",
       "      <td>0.510150</td>\n",
       "      <td>0.500018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>0.150964</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.313309</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325447</td>\n",
       "      <td>0.471545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528708</td>\n",
       "      <td>0.544206</td>\n",
       "      <td>0.544487</td>\n",
       "      <td>0.391324</td>\n",
       "      <td>0.336072</td>\n",
       "      <td>0.538049</td>\n",
       "      <td>0.531118</td>\n",
       "      <td>0.510150</td>\n",
       "      <td>0.500018</td>\n",
       "      <td>0.486332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>0.313309</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325447</td>\n",
       "      <td>0.471545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.298216</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.544206</td>\n",
       "      <td>0.544487</td>\n",
       "      <td>0.391324</td>\n",
       "      <td>0.336072</td>\n",
       "      <td>0.538049</td>\n",
       "      <td>0.531118</td>\n",
       "      <td>0.510150</td>\n",
       "      <td>0.500018</td>\n",
       "      <td>0.486332</td>\n",
       "      <td>0.368140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>0.325447</td>\n",
       "      <td>0.471545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.298216</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.301136</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.544487</td>\n",
       "      <td>0.391324</td>\n",
       "      <td>0.336072</td>\n",
       "      <td>0.538049</td>\n",
       "      <td>0.531118</td>\n",
       "      <td>0.510150</td>\n",
       "      <td>0.500018</td>\n",
       "      <td>0.486332</td>\n",
       "      <td>0.368140</td>\n",
       "      <td>0.342035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>0.298216</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.301136</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272815</td>\n",
       "      <td>0.479675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391324</td>\n",
       "      <td>0.336072</td>\n",
       "      <td>0.538049</td>\n",
       "      <td>0.531118</td>\n",
       "      <td>0.510150</td>\n",
       "      <td>0.500018</td>\n",
       "      <td>0.486332</td>\n",
       "      <td>0.368140</td>\n",
       "      <td>0.342035</td>\n",
       "      <td>0.511469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>0.301136</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272815</td>\n",
       "      <td>0.479675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143910</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336072</td>\n",
       "      <td>0.538049</td>\n",
       "      <td>0.531118</td>\n",
       "      <td>0.510150</td>\n",
       "      <td>0.500018</td>\n",
       "      <td>0.486332</td>\n",
       "      <td>0.368140</td>\n",
       "      <td>0.342035</td>\n",
       "      <td>0.511469</td>\n",
       "      <td>0.545103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>0.272815</td>\n",
       "      <td>0.479675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143910</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128536</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538049</td>\n",
       "      <td>0.531118</td>\n",
       "      <td>0.510150</td>\n",
       "      <td>0.500018</td>\n",
       "      <td>0.486332</td>\n",
       "      <td>0.368140</td>\n",
       "      <td>0.342035</td>\n",
       "      <td>0.511469</td>\n",
       "      <td>0.545103</td>\n",
       "      <td>0.583433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>0.143910</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128536</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.289456</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531118</td>\n",
       "      <td>0.510150</td>\n",
       "      <td>0.500018</td>\n",
       "      <td>0.486332</td>\n",
       "      <td>0.368140</td>\n",
       "      <td>0.342035</td>\n",
       "      <td>0.511469</td>\n",
       "      <td>0.545103</td>\n",
       "      <td>0.583433</td>\n",
       "      <td>0.608394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>0.128536</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.289456</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.288137</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510150</td>\n",
       "      <td>0.500018</td>\n",
       "      <td>0.486332</td>\n",
       "      <td>0.368140</td>\n",
       "      <td>0.342035</td>\n",
       "      <td>0.511469</td>\n",
       "      <td>0.545103</td>\n",
       "      <td>0.583433</td>\n",
       "      <td>0.608394</td>\n",
       "      <td>0.617805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>0.289456</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.288137</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.322738</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500018</td>\n",
       "      <td>0.486332</td>\n",
       "      <td>0.368140</td>\n",
       "      <td>0.342035</td>\n",
       "      <td>0.511469</td>\n",
       "      <td>0.545103</td>\n",
       "      <td>0.583433</td>\n",
       "      <td>0.608394</td>\n",
       "      <td>0.617805</td>\n",
       "      <td>0.506104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.288137</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.322738</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.302614</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486332</td>\n",
       "      <td>0.368140</td>\n",
       "      <td>0.342035</td>\n",
       "      <td>0.511469</td>\n",
       "      <td>0.545103</td>\n",
       "      <td>0.583433</td>\n",
       "      <td>0.608394</td>\n",
       "      <td>0.617805</td>\n",
       "      <td>0.506104</td>\n",
       "      <td>0.439329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.322738</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.302614</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.282297</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368140</td>\n",
       "      <td>0.342035</td>\n",
       "      <td>0.511469</td>\n",
       "      <td>0.545103</td>\n",
       "      <td>0.583433</td>\n",
       "      <td>0.608394</td>\n",
       "      <td>0.617805</td>\n",
       "      <td>0.506104</td>\n",
       "      <td>0.439329</td>\n",
       "      <td>0.609925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.302614</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.282297</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190438</td>\n",
       "      <td>0.211382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342035</td>\n",
       "      <td>0.511469</td>\n",
       "      <td>0.545103</td>\n",
       "      <td>0.583433</td>\n",
       "      <td>0.608394</td>\n",
       "      <td>0.617805</td>\n",
       "      <td>0.506104</td>\n",
       "      <td>0.439329</td>\n",
       "      <td>0.609925</td>\n",
       "      <td>0.643189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.282297</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190438</td>\n",
       "      <td>0.211382</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155608</td>\n",
       "      <td>0.203252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511469</td>\n",
       "      <td>0.545103</td>\n",
       "      <td>0.583433</td>\n",
       "      <td>0.608394</td>\n",
       "      <td>0.617805</td>\n",
       "      <td>0.506104</td>\n",
       "      <td>0.439329</td>\n",
       "      <td>0.609925</td>\n",
       "      <td>0.643189</td>\n",
       "      <td>0.680042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.190438</td>\n",
       "      <td>0.211382</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155608</td>\n",
       "      <td>0.203252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.307399</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545103</td>\n",
       "      <td>0.583433</td>\n",
       "      <td>0.608394</td>\n",
       "      <td>0.617805</td>\n",
       "      <td>0.506104</td>\n",
       "      <td>0.439329</td>\n",
       "      <td>0.609925</td>\n",
       "      <td>0.643189</td>\n",
       "      <td>0.680042</td>\n",
       "      <td>0.673920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.155608</td>\n",
       "      <td>0.203252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.307399</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.318252</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583433</td>\n",
       "      <td>0.608394</td>\n",
       "      <td>0.617805</td>\n",
       "      <td>0.506104</td>\n",
       "      <td>0.439329</td>\n",
       "      <td>0.609925</td>\n",
       "      <td>0.643189</td>\n",
       "      <td>0.680042</td>\n",
       "      <td>0.673920</td>\n",
       "      <td>0.693129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>0.307399</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.318252</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285656</td>\n",
       "      <td>0.422764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608394</td>\n",
       "      <td>0.617805</td>\n",
       "      <td>0.506104</td>\n",
       "      <td>0.439329</td>\n",
       "      <td>0.609925</td>\n",
       "      <td>0.643189</td>\n",
       "      <td>0.680042</td>\n",
       "      <td>0.673920</td>\n",
       "      <td>0.693129</td>\n",
       "      <td>0.583028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>0.318252</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285656</td>\n",
       "      <td>0.422764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.299201</td>\n",
       "      <td>0.422764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617805</td>\n",
       "      <td>0.506104</td>\n",
       "      <td>0.439329</td>\n",
       "      <td>0.609925</td>\n",
       "      <td>0.643189</td>\n",
       "      <td>0.680042</td>\n",
       "      <td>0.673920</td>\n",
       "      <td>0.693129</td>\n",
       "      <td>0.583028</td>\n",
       "      <td>0.544135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>0.285656</td>\n",
       "      <td>0.422764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.299201</td>\n",
       "      <td>0.422764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.297671</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506104</td>\n",
       "      <td>0.439329</td>\n",
       "      <td>0.609925</td>\n",
       "      <td>0.643189</td>\n",
       "      <td>0.680042</td>\n",
       "      <td>0.673920</td>\n",
       "      <td>0.693129</td>\n",
       "      <td>0.583028</td>\n",
       "      <td>0.544135</td>\n",
       "      <td>0.712848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>0.299201</td>\n",
       "      <td>0.422764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.297671</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.178159</td>\n",
       "      <td>0.455285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439329</td>\n",
       "      <td>0.609925</td>\n",
       "      <td>0.643189</td>\n",
       "      <td>0.680042</td>\n",
       "      <td>0.673920</td>\n",
       "      <td>0.693129</td>\n",
       "      <td>0.583028</td>\n",
       "      <td>0.544135</td>\n",
       "      <td>0.712848</td>\n",
       "      <td>0.714115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>0.297671</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.178159</td>\n",
       "      <td>0.455285</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133813</td>\n",
       "      <td>0.455285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609925</td>\n",
       "      <td>0.643189</td>\n",
       "      <td>0.680042</td>\n",
       "      <td>0.673920</td>\n",
       "      <td>0.693129</td>\n",
       "      <td>0.583028</td>\n",
       "      <td>0.544135</td>\n",
       "      <td>0.712848</td>\n",
       "      <td>0.714115</td>\n",
       "      <td>0.732374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>946 rows × 270 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      var1(t-60)  var2(t-60)  var3(t-60)  var4(t-60)  var1(t-59)  var2(t-59)  \\\n",
       "60      0.433296    0.902439         0.0         1.0    0.473684    0.861789   \n",
       "61      0.473684    0.861789         1.0         0.0    0.509341    0.878049   \n",
       "62      0.509341    0.878049         0.0         1.0    0.565965    0.809756   \n",
       "63      0.565965    0.809756         0.0         1.0    0.618122    0.848780   \n",
       "64      0.618122    0.848780         0.0         1.0    0.665687    0.813008   \n",
       "65      0.665687    0.813008         0.0         1.0    0.664016    0.822764   \n",
       "66      0.664016    0.822764         0.0         1.0    0.685547    0.845528   \n",
       "67      0.685547    0.845528         0.0         1.0    0.661114    0.788618   \n",
       "68      0.661114    0.788618         1.0         0.0    0.655309    0.804878   \n",
       "69      0.655309    0.804878         0.0         1.0    0.843249    0.731707   \n",
       "70      0.843249    0.731707         0.0         0.0    0.853064    0.699187   \n",
       "71      0.853064    0.699187         0.0         0.0    0.852554    0.681301   \n",
       "72      0.852554    0.681301         0.0         0.0    0.835790    0.699187   \n",
       "73      0.835790    0.699187         0.0         0.0    0.822140    0.756098   \n",
       "74      0.822140    0.756098         0.0         0.0    0.677315    0.780488   \n",
       "75      0.677315    0.780488         1.0         0.0    0.661096    0.772358   \n",
       "76      0.661096    0.772358         0.0         1.0    0.828613    0.689431   \n",
       "77      0.828613    0.689431         0.0         0.0    0.862898    0.715447   \n",
       "78      0.862898    0.715447         0.0         0.0    0.881280    0.715447   \n",
       "79      0.881280    0.715447         0.0         0.0    0.877920    0.731707   \n",
       "80      0.877920    0.731707         0.0         0.0    0.876654    0.739837   \n",
       "81      0.876654    0.739837         0.0         0.0    0.764090    0.764228   \n",
       "82      0.764090    0.764228         1.0         0.0    0.723244    0.747967   \n",
       "83      0.723244    0.747967         0.0         1.0    0.894227    0.663415   \n",
       "84      0.894227    0.663415         0.0         0.0    0.920120    0.650407   \n",
       "85      0.920120    0.650407         0.0         0.0    0.907561    0.678049   \n",
       "86      0.907561    0.678049         0.0         0.0    0.902090    0.707317   \n",
       "87      0.902090    0.707317         0.0         0.0    0.900331    0.691057   \n",
       "88      0.900331    0.691057         0.0         0.0    0.772059    0.635772   \n",
       "89      0.772059    0.635772         1.0         0.0    0.740782    0.640650   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "976     0.321964    0.536585         0.0         0.0    0.314066    0.569106   \n",
       "977     0.314066    0.569106         0.0         0.0    0.183806    0.552846   \n",
       "978     0.183806    0.552846         1.0         0.0    0.138211    0.585366   \n",
       "979     0.138211    0.585366         0.0         1.0    0.288665    0.471545   \n",
       "980     0.288665    0.471545         0.0         0.0    0.314189    0.325203   \n",
       "981     0.314189    0.325203         0.0         0.0    0.325640    0.308943   \n",
       "982     0.325640    0.308943         0.0         0.0    0.317302    0.357724   \n",
       "983     0.317302    0.357724         0.0         0.0    0.329493    0.365854   \n",
       "984     0.329493    0.365854         0.0         0.0    0.182539    0.414634   \n",
       "985     0.182539    0.414634         1.0         0.0    0.150964    0.430894   \n",
       "986     0.150964    0.430894         0.0         1.0    0.313309    0.406504   \n",
       "987     0.313309    0.406504         0.0         0.0    0.325447    0.471545   \n",
       "988     0.325447    0.471545         0.0         0.0    0.298216    0.463415   \n",
       "989     0.298216    0.463415         0.0         0.0    0.301136    0.504065   \n",
       "990     0.301136    0.504065         0.0         0.0    0.272815    0.479675   \n",
       "991     0.272815    0.479675         0.0         0.0    0.143910    0.439024   \n",
       "992     0.143910    0.439024         1.0         0.0    0.128536    0.414634   \n",
       "993     0.128536    0.414634         0.0         1.0    0.289456    0.414634   \n",
       "994     0.289456    0.414634         0.0         0.0    0.288137    0.406504   \n",
       "995     0.288137    0.406504         0.0         0.0    0.322738    0.439024   \n",
       "996     0.322738    0.439024         0.0         0.0    0.302614    0.414634   \n",
       "997     0.302614    0.414634         0.0         0.0    0.282297    0.317073   \n",
       "998     0.282297    0.317073         0.0         0.0    0.190438    0.211382   \n",
       "999     0.190438    0.211382         1.0         0.0    0.155608    0.203252   \n",
       "1000    0.155608    0.203252         0.0         1.0    0.307399    0.365854   \n",
       "1001    0.307399    0.365854         0.0         0.0    0.318252    0.406504   \n",
       "1002    0.318252    0.406504         0.0         0.0    0.285656    0.422764   \n",
       "1003    0.285656    0.422764         0.0         0.0    0.299201    0.422764   \n",
       "1004    0.299201    0.422764         0.0         0.0    0.297671    0.430894   \n",
       "1005    0.297671    0.430894         0.0         0.0    0.178159    0.455285   \n",
       "\n",
       "      var3(t-59)  var4(t-59)  var1(t-58)  var2(t-58)     ...      var1(t+20)  \\\n",
       "60           1.0         0.0    0.509341    0.878049     ...        0.669628   \n",
       "61           0.0         1.0    0.565965    0.809756     ...        0.676189   \n",
       "62           0.0         1.0    0.618122    0.848780     ...        0.659601   \n",
       "63           0.0         1.0    0.665687    0.813008     ...        0.641711   \n",
       "64           0.0         1.0    0.664016    0.822764     ...        0.617242   \n",
       "65           0.0         1.0    0.685547    0.845528     ...        0.468882   \n",
       "66           0.0         1.0    0.661114    0.788618     ...        0.364745   \n",
       "67           1.0         0.0    0.655309    0.804878     ...        0.575975   \n",
       "68           0.0         1.0    0.843249    0.731707     ...        0.555042   \n",
       "69           0.0         0.0    0.853064    0.699187     ...        0.532437   \n",
       "70           0.0         0.0    0.852554    0.681301     ...        0.521901   \n",
       "71           0.0         0.0    0.835790    0.699187     ...        0.489551   \n",
       "72           0.0         0.0    0.822140    0.756098     ...        0.359854   \n",
       "73           0.0         0.0    0.677315    0.780488     ...        0.322280   \n",
       "74           1.0         0.0    0.661096    0.772358     ...        0.484450   \n",
       "75           0.0         1.0    0.828613    0.689431     ...        0.413876   \n",
       "76           0.0         0.0    0.862898    0.715447     ...        0.390761   \n",
       "77           0.0         0.0    0.881280    0.715447     ...        0.389864   \n",
       "78           0.0         0.0    0.877920    0.731707     ...        0.404904   \n",
       "79           0.0         0.0    0.876654    0.739837     ...        0.301435   \n",
       "80           0.0         0.0    0.764090    0.764228     ...        0.262067   \n",
       "81           1.0         0.0    0.723244    0.747967     ...        0.420156   \n",
       "82           0.0         1.0    0.894227    0.663415     ...        0.453842   \n",
       "83           0.0         0.0    0.920120    0.650407     ...        0.475373   \n",
       "84           0.0         0.0    0.907561    0.678049     ...        0.472066   \n",
       "85           0.0         0.0    0.902090    0.707317     ...        0.452259   \n",
       "86           0.0         0.0    0.900331    0.691057     ...        0.322615   \n",
       "87           0.0         0.0    0.772059    0.635772     ...        0.274486   \n",
       "88           1.0         0.0    0.740782    0.640650     ...        0.440684   \n",
       "89           0.0         1.0    0.879503    0.694309     ...        0.430323   \n",
       "...          ...         ...         ...         ...     ...             ...   \n",
       "976          0.0         0.0    0.183806    0.552846     ...        0.275683   \n",
       "977          1.0         0.0    0.138211    0.585366     ...        0.460333   \n",
       "978          0.0         1.0    0.288665    0.471545     ...        0.482216   \n",
       "979          0.0         0.0    0.314189    0.325203     ...        0.492893   \n",
       "980          0.0         0.0    0.325640    0.308943     ...        0.497696   \n",
       "981          0.0         0.0    0.317302    0.357724     ...        0.499754   \n",
       "982          0.0         0.0    0.329493    0.365854     ...        0.350074   \n",
       "983          0.0         0.0    0.182539    0.414634     ...        0.291743   \n",
       "984          1.0         0.0    0.150964    0.430894     ...        0.499912   \n",
       "985          0.0         1.0    0.313309    0.406504     ...        0.528163   \n",
       "986          0.0         0.0    0.325447    0.471545     ...        0.528708   \n",
       "987          0.0         0.0    0.298216    0.463415     ...        0.544206   \n",
       "988          0.0         0.0    0.301136    0.504065     ...        0.544487   \n",
       "989          0.0         0.0    0.272815    0.479675     ...        0.391324   \n",
       "990          0.0         0.0    0.143910    0.439024     ...        0.336072   \n",
       "991          1.0         0.0    0.128536    0.414634     ...        0.538049   \n",
       "992          0.0         1.0    0.289456    0.414634     ...        0.531118   \n",
       "993          0.0         0.0    0.288137    0.406504     ...        0.510150   \n",
       "994          0.0         0.0    0.322738    0.439024     ...        0.500018   \n",
       "995          0.0         0.0    0.302614    0.414634     ...        0.486332   \n",
       "996          0.0         0.0    0.282297    0.317073     ...        0.368140   \n",
       "997          0.0         0.0    0.190438    0.211382     ...        0.342035   \n",
       "998          1.0         0.0    0.155608    0.203252     ...        0.511469   \n",
       "999          0.0         1.0    0.307399    0.365854     ...        0.545103   \n",
       "1000         0.0         0.0    0.318252    0.406504     ...        0.583433   \n",
       "1001         0.0         0.0    0.285656    0.422764     ...        0.608394   \n",
       "1002         0.0         0.0    0.299201    0.422764     ...        0.617805   \n",
       "1003         0.0         0.0    0.297671    0.430894     ...        0.506104   \n",
       "1004         0.0         0.0    0.178159    0.455285     ...        0.439329   \n",
       "1005         1.0         0.0    0.133813    0.455285     ...        0.609925   \n",
       "\n",
       "      var1(t+21)  var1(t+22)  var1(t+23)  var1(t+24)  var1(t+25)  var1(t+26)  \\\n",
       "60      0.676189    0.659601    0.641711    0.617242    0.468882    0.364745   \n",
       "61      0.659601    0.641711    0.617242    0.468882    0.364745    0.575975   \n",
       "62      0.641711    0.617242    0.468882    0.364745    0.575975    0.555042   \n",
       "63      0.617242    0.468882    0.364745    0.575975    0.555042    0.532437   \n",
       "64      0.468882    0.364745    0.575975    0.555042    0.532437    0.521901   \n",
       "65      0.364745    0.575975    0.555042    0.532437    0.521901    0.489551   \n",
       "66      0.575975    0.555042    0.532437    0.521901    0.489551    0.359854   \n",
       "67      0.555042    0.532437    0.521901    0.489551    0.359854    0.322280   \n",
       "68      0.532437    0.521901    0.489551    0.359854    0.322280    0.484450   \n",
       "69      0.521901    0.489551    0.359854    0.322280    0.484450    0.413876   \n",
       "70      0.489551    0.359854    0.322280    0.484450    0.413876    0.390761   \n",
       "71      0.359854    0.322280    0.484450    0.413876    0.390761    0.389864   \n",
       "72      0.322280    0.484450    0.413876    0.390761    0.389864    0.404904   \n",
       "73      0.484450    0.413876    0.390761    0.389864    0.404904    0.301435   \n",
       "74      0.413876    0.390761    0.389864    0.404904    0.301435    0.262067   \n",
       "75      0.390761    0.389864    0.404904    0.301435    0.262067    0.420156   \n",
       "76      0.389864    0.404904    0.301435    0.262067    0.420156    0.453842   \n",
       "77      0.404904    0.301435    0.262067    0.420156    0.453842    0.475373   \n",
       "78      0.301435    0.262067    0.420156    0.453842    0.475373    0.472066   \n",
       "79      0.262067    0.420156    0.453842    0.475373    0.472066    0.452259   \n",
       "80      0.420156    0.453842    0.475373    0.472066    0.452259    0.322615   \n",
       "81      0.453842    0.475373    0.472066    0.452259    0.322615    0.274486   \n",
       "82      0.475373    0.472066    0.452259    0.322615    0.274486    0.440684   \n",
       "83      0.472066    0.452259    0.322615    0.274486    0.440684    0.430323   \n",
       "84      0.452259    0.322615    0.274486    0.440684    0.430323    0.405801   \n",
       "85      0.322615    0.274486    0.440684    0.430323    0.405801    0.402547   \n",
       "86      0.274486    0.440684    0.430323    0.405801    0.402547    0.381808   \n",
       "87      0.440684    0.430323    0.405801    0.402547    0.381808    0.258039   \n",
       "88      0.430323    0.405801    0.402547    0.381808    0.258039    0.220448   \n",
       "89      0.405801    0.402547    0.381808    0.258039    0.220448    0.371675   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "976     0.460333    0.482216    0.492893    0.497696    0.499754    0.350074   \n",
       "977     0.482216    0.492893    0.497696    0.499754    0.350074    0.291743   \n",
       "978     0.492893    0.497696    0.499754    0.350074    0.291743    0.499912   \n",
       "979     0.497696    0.499754    0.350074    0.291743    0.499912    0.528163   \n",
       "980     0.499754    0.350074    0.291743    0.499912    0.528163    0.528708   \n",
       "981     0.350074    0.291743    0.499912    0.528163    0.528708    0.544206   \n",
       "982     0.291743    0.499912    0.528163    0.528708    0.544206    0.544487   \n",
       "983     0.499912    0.528163    0.528708    0.544206    0.544487    0.391324   \n",
       "984     0.528163    0.528708    0.544206    0.544487    0.391324    0.336072   \n",
       "985     0.528708    0.544206    0.544487    0.391324    0.336072    0.538049   \n",
       "986     0.544206    0.544487    0.391324    0.336072    0.538049    0.531118   \n",
       "987     0.544487    0.391324    0.336072    0.538049    0.531118    0.510150   \n",
       "988     0.391324    0.336072    0.538049    0.531118    0.510150    0.500018   \n",
       "989     0.336072    0.538049    0.531118    0.510150    0.500018    0.486332   \n",
       "990     0.538049    0.531118    0.510150    0.500018    0.486332    0.368140   \n",
       "991     0.531118    0.510150    0.500018    0.486332    0.368140    0.342035   \n",
       "992     0.510150    0.500018    0.486332    0.368140    0.342035    0.511469   \n",
       "993     0.500018    0.486332    0.368140    0.342035    0.511469    0.545103   \n",
       "994     0.486332    0.368140    0.342035    0.511469    0.545103    0.583433   \n",
       "995     0.368140    0.342035    0.511469    0.545103    0.583433    0.608394   \n",
       "996     0.342035    0.511469    0.545103    0.583433    0.608394    0.617805   \n",
       "997     0.511469    0.545103    0.583433    0.608394    0.617805    0.506104   \n",
       "998     0.545103    0.583433    0.608394    0.617805    0.506104    0.439329   \n",
       "999     0.583433    0.608394    0.617805    0.506104    0.439329    0.609925   \n",
       "1000    0.608394    0.617805    0.506104    0.439329    0.609925    0.643189   \n",
       "1001    0.617805    0.506104    0.439329    0.609925    0.643189    0.680042   \n",
       "1002    0.506104    0.439329    0.609925    0.643189    0.680042    0.673920   \n",
       "1003    0.439329    0.609925    0.643189    0.680042    0.673920    0.693129   \n",
       "1004    0.609925    0.643189    0.680042    0.673920    0.693129    0.583028   \n",
       "1005    0.643189    0.680042    0.673920    0.693129    0.583028    0.544135   \n",
       "\n",
       "      var1(t+27)  var1(t+28)  var1(t+29)  \n",
       "60      0.575975    0.555042    0.532437  \n",
       "61      0.555042    0.532437    0.521901  \n",
       "62      0.532437    0.521901    0.489551  \n",
       "63      0.521901    0.489551    0.359854  \n",
       "64      0.489551    0.359854    0.322280  \n",
       "65      0.359854    0.322280    0.484450  \n",
       "66      0.322280    0.484450    0.413876  \n",
       "67      0.484450    0.413876    0.390761  \n",
       "68      0.413876    0.390761    0.389864  \n",
       "69      0.390761    0.389864    0.404904  \n",
       "70      0.389864    0.404904    0.301435  \n",
       "71      0.404904    0.301435    0.262067  \n",
       "72      0.301435    0.262067    0.420156  \n",
       "73      0.262067    0.420156    0.453842  \n",
       "74      0.420156    0.453842    0.475373  \n",
       "75      0.453842    0.475373    0.472066  \n",
       "76      0.475373    0.472066    0.452259  \n",
       "77      0.472066    0.452259    0.322615  \n",
       "78      0.452259    0.322615    0.274486  \n",
       "79      0.322615    0.274486    0.440684  \n",
       "80      0.274486    0.440684    0.430323  \n",
       "81      0.440684    0.430323    0.405801  \n",
       "82      0.430323    0.405801    0.402547  \n",
       "83      0.405801    0.402547    0.381808  \n",
       "84      0.402547    0.381808    0.258039  \n",
       "85      0.381808    0.258039    0.220448  \n",
       "86      0.258039    0.220448    0.371675  \n",
       "87      0.220448    0.371675    0.391430  \n",
       "88      0.371675    0.391430    0.418449  \n",
       "89      0.391430    0.418449    0.394174  \n",
       "...          ...         ...         ...  \n",
       "976     0.291743    0.499912    0.528163  \n",
       "977     0.499912    0.528163    0.528708  \n",
       "978     0.528163    0.528708    0.544206  \n",
       "979     0.528708    0.544206    0.544487  \n",
       "980     0.544206    0.544487    0.391324  \n",
       "981     0.544487    0.391324    0.336072  \n",
       "982     0.391324    0.336072    0.538049  \n",
       "983     0.336072    0.538049    0.531118  \n",
       "984     0.538049    0.531118    0.510150  \n",
       "985     0.531118    0.510150    0.500018  \n",
       "986     0.510150    0.500018    0.486332  \n",
       "987     0.500018    0.486332    0.368140  \n",
       "988     0.486332    0.368140    0.342035  \n",
       "989     0.368140    0.342035    0.511469  \n",
       "990     0.342035    0.511469    0.545103  \n",
       "991     0.511469    0.545103    0.583433  \n",
       "992     0.545103    0.583433    0.608394  \n",
       "993     0.583433    0.608394    0.617805  \n",
       "994     0.608394    0.617805    0.506104  \n",
       "995     0.617805    0.506104    0.439329  \n",
       "996     0.506104    0.439329    0.609925  \n",
       "997     0.439329    0.609925    0.643189  \n",
       "998     0.609925    0.643189    0.680042  \n",
       "999     0.643189    0.680042    0.673920  \n",
       "1000    0.680042    0.673920    0.693129  \n",
       "1001    0.673920    0.693129    0.583028  \n",
       "1002    0.693129    0.583028    0.544135  \n",
       "1003    0.583028    0.544135    0.712848  \n",
       "1004    0.544135    0.712848    0.714115  \n",
       "1005    0.712848    0.714115    0.732374  \n",
       "\n",
       "[946 rows x 270 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reframed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(945, 240) (945, 30) (1, 240) (1, 30)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, test_X, test_y = SamplesGenerator.train_and_test(reframed, test_prop=1)\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self):\n",
    "        self.lr = LinearRegression()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.lr.fit(X.squeeze(), y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # make a prediction\n",
    "        lr_yhat = self.lr.predict(X)\n",
    "        # invert scaling for forecast\n",
    "        return Preprocessor.invert_scaling(lr_yhat)\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_forecast(Train, cur_tsID, lr_inv_yhat_last):\n",
    "        indexes = Train[Train['tsID'] == cur_tsID][pd.isnull(Train['ACTUAL'])].index\n",
    "        forecast = pd.DataFrame(lr_inv_yhat_last, index=indexes, columns=['PREDICTED'])\n",
    "        sample_submission.loc[indexes, :] = forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/scipy/linalg/basic.py:1018: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = Predictor()\n",
    "lr.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_inv_yhat = lr.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_y = test_y.reshape((len(test_y), DAYS_FOR_PREDICT))\n",
    "# invert scaling for actual\n",
    "inv_y = Preprocessor.invert_scaling(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 2285.81266851\n"
     ]
    }
   ],
   "source": [
    "rmse2 = np.sqrt(mean_squared_error(inv_y, lr_inv_yhat))\n",
    "\n",
    "print('Test RMSE:', rmse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def qualityMAPE(x,y):\n",
    "    # Mean absolute percentage error\n",
    "    # x - real values\n",
    "    # y - forecasts\n",
    "    qlt = (np.abs((x-y)/x))\n",
    "    return qlt.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAPE: 0.0193141147517\n"
     ]
    }
   ],
   "source": [
    "print('Test MAPE:', np.mean([qualityMAPE(inv_y[i, :], lr_inv_yhat[i, :]) for i in range(inv_y.shape[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1118e0518>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4MAAAJCCAYAAAB6VBJfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1179e8978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(inv_y[-1], label='Real')\n",
    "plt.plot(lr_inv_yhat[-1], ':', label='LR')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "Predictor.write_forecast(Train, cur_tsID, lr_inv_yhat[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PREDICTED</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16707</th>\n",
       "      <td>94867.255762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16708</th>\n",
       "      <td>95655.354239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16709</th>\n",
       "      <td>95775.266836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16710</th>\n",
       "      <td>93799.071801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16711</th>\n",
       "      <td>87505.423006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          PREDICTED\n",
       "ID                 \n",
       "16707  94867.255762\n",
       "16708  95655.354239\n",
       "16709  95775.266836\n",
       "16710  93799.071801\n",
       "16711  87505.423006"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast for all (linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DAYS_FOR_PREDICT = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:00<?, ?it/s]/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  app.launch_new_instance()\n",
      "100%|██████████| 22/22 [00:05<00:00,  4.79it/s]\n"
     ]
    }
   ],
   "source": [
    "for cur_tsID in tqdm(ids):\n",
    "    # prepare data\n",
    "    ts = Preprocessor.prepare_ts(Train, cur_tsID)\n",
    "    scaled = Preprocessor.scale(ts)\n",
    "    reframed = SamplesGenerator.series_to_supervised(scaled, n_in=2*DAYS_FOR_PREDICT, n_out=DAYS_FOR_PREDICT)\n",
    "    train_X, train_y, test_X, test_y = SamplesGenerator.train_and_test(reframed, test_prop=1)\n",
    "    \n",
    "    # fitting model\n",
    "    lr = Predictor()\n",
    "    lr.fit(train_X, train_y)\n",
    "    lr_inv_yhat = lr.predict(test_X)\n",
    "    \n",
    "    test_y = test_y.reshape((len(test_y), DAYS_FOR_PREDICT))\n",
    "    # invert scaling for actual\n",
    "    inv_y = Preprocessor.invert_scaling(test_y)\n",
    "    \n",
    "    # write forecast to sample_submission\n",
    "    Predictor.write_forecast(Train, cur_tsID, lr_inv_yhat[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast for all (neural networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DAYS_FOR_PREDICT = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualityMAPE_neural(x,y):\n",
    "    # Mean absolute percentage error\n",
    "    # x - real values\n",
    "    # y - forecasts\n",
    "    qlt = (tf.abs((x-y)/x))\n",
    "    return tf.reduce_mean(qlt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 756 samples, validate on 190 samples\n",
      "Epoch 1/150\n",
      "756/756 [==============================] - 10s - loss: 0.4220 - val_loss: 0.2362\n",
      "Epoch 2/150\n",
      "756/756 [==============================] - 0s - loss: 0.3238 - val_loss: 0.1819\n",
      "Epoch 3/150\n",
      "756/756 [==============================] - 0s - loss: 0.2602 - val_loss: 0.1523\n",
      "Epoch 4/150\n",
      "756/756 [==============================] - 0s - loss: 0.2242 - val_loss: 0.1509\n",
      "Epoch 5/150\n",
      "756/756 [==============================] - 0s - loss: 0.2083 - val_loss: 0.1559\n",
      "Epoch 6/150\n",
      "756/756 [==============================] - 0s - loss: 0.1989 - val_loss: 0.1527\n",
      "Epoch 7/150\n",
      "756/756 [==============================] - 0s - loss: 0.1902 - val_loss: 0.1447\n",
      "Epoch 8/150\n",
      "756/756 [==============================] - 0s - loss: 0.1810 - val_loss: 0.1359\n",
      "Epoch 9/150\n",
      "756/756 [==============================] - 0s - loss: 0.1714 - val_loss: 0.1265\n",
      "Epoch 10/150\n",
      "756/756 [==============================] - 0s - loss: 0.1612 - val_loss: 0.1170\n",
      "Epoch 11/150\n",
      "756/756 [==============================] - 0s - loss: 0.1512 - val_loss: 0.1087\n",
      "Epoch 12/150\n",
      "756/756 [==============================] - 0s - loss: 0.1421 - val_loss: 0.1019\n",
      "Epoch 13/150\n",
      "756/756 [==============================] - 0s - loss: 0.1342 - val_loss: 0.0951\n",
      "Epoch 14/150\n",
      "756/756 [==============================] - 0s - loss: 0.1269 - val_loss: 0.0882\n",
      "Epoch 15/150\n",
      "756/756 [==============================] - 0s - loss: 0.1198 - val_loss: 0.0822\n",
      "Epoch 16/150\n",
      "756/756 [==============================] - 0s - loss: 0.1131 - val_loss: 0.0768\n",
      "Epoch 17/150\n",
      "756/756 [==============================] - 0s - loss: 0.1073 - val_loss: 0.0719\n",
      "Epoch 18/150\n",
      "756/756 [==============================] - 0s - loss: 0.1017 - val_loss: 0.0677\n",
      "Epoch 19/150\n",
      "756/756 [==============================] - 0s - loss: 0.0970 - val_loss: 0.0640\n",
      "Epoch 20/150\n",
      "756/756 [==============================] - 0s - loss: 0.0927 - val_loss: 0.0607\n",
      "Epoch 21/150\n",
      "756/756 [==============================] - 0s - loss: 0.0894 - val_loss: 0.0581\n",
      "Epoch 22/150\n",
      "756/756 [==============================] - 0s - loss: 0.0862 - val_loss: 0.0561\n",
      "Epoch 23/150\n",
      "756/756 [==============================] - 0s - loss: 0.0841 - val_loss: 0.0545\n",
      "Epoch 24/150\n",
      "756/756 [==============================] - 0s - loss: 0.0820 - val_loss: 0.0532\n",
      "Epoch 25/150\n",
      "756/756 [==============================] - 0s - loss: 0.0806 - val_loss: 0.0522\n",
      "Epoch 26/150\n",
      "756/756 [==============================] - 0s - loss: 0.0789 - val_loss: 0.0513\n",
      "Epoch 27/150\n",
      "756/756 [==============================] - 0s - loss: 0.0778 - val_loss: 0.0506\n",
      "Epoch 28/150\n",
      "756/756 [==============================] - 0s - loss: 0.0765 - val_loss: 0.0500\n",
      "Epoch 29/150\n",
      "756/756 [==============================] - 0s - loss: 0.0755 - val_loss: 0.0495\n",
      "Epoch 30/150\n",
      "756/756 [==============================] - 0s - loss: 0.0745 - val_loss: 0.0492\n",
      "Epoch 31/150\n",
      "756/756 [==============================] - 0s - loss: 0.0735 - val_loss: 0.0488\n",
      "Epoch 32/150\n",
      "756/756 [==============================] - 0s - loss: 0.0728 - val_loss: 0.0485\n",
      "Epoch 33/150\n",
      "756/756 [==============================] - 0s - loss: 0.0717 - val_loss: 0.0483\n",
      "Epoch 34/150\n",
      "756/756 [==============================] - 0s - loss: 0.0711 - val_loss: 0.0481\n",
      "Epoch 35/150\n",
      "756/756 [==============================] - 0s - loss: 0.0702 - val_loss: 0.0480\n",
      "Epoch 36/150\n",
      "756/756 [==============================] - 0s - loss: 0.0695 - val_loss: 0.0478\n",
      "Epoch 37/150\n",
      "756/756 [==============================] - 0s - loss: 0.0688 - val_loss: 0.0479\n",
      "Epoch 38/150\n",
      "756/756 [==============================] - 0s - loss: 0.0680 - val_loss: 0.0477\n",
      "Epoch 39/150\n",
      "756/756 [==============================] - 0s - loss: 0.0674 - val_loss: 0.0477\n",
      "Epoch 40/150\n",
      "756/756 [==============================] - 0s - loss: 0.0667 - val_loss: 0.0477\n",
      "Epoch 41/150\n",
      "756/756 [==============================] - 0s - loss: 0.0662 - val_loss: 0.0476\n",
      "Epoch 42/150\n",
      "756/756 [==============================] - 0s - loss: 0.0655 - val_loss: 0.0478\n",
      "Epoch 43/150\n",
      "756/756 [==============================] - 0s - loss: 0.0649 - val_loss: 0.0474\n",
      "Epoch 44/150\n",
      "756/756 [==============================] - 0s - loss: 0.0645 - val_loss: 0.0477\n",
      "Epoch 45/150\n",
      "756/756 [==============================] - 0s - loss: 0.0636 - val_loss: 0.0479\n",
      "Epoch 46/150\n",
      "756/756 [==============================] - 0s - loss: 0.0632 - val_loss: 0.0475\n",
      "Epoch 47/150\n",
      "756/756 [==============================] - 0s - loss: 0.0628 - val_loss: 0.0477\n",
      "Epoch 48/150\n",
      "756/756 [==============================] - 0s - loss: 0.0621 - val_loss: 0.0479\n",
      "Epoch 49/150\n",
      "756/756 [==============================] - 0s - loss: 0.0615 - val_loss: 0.0478\n",
      "Epoch 50/150\n",
      "756/756 [==============================] - 0s - loss: 0.0610 - val_loss: 0.0476\n",
      "Epoch 51/150\n",
      "756/756 [==============================] - 0s - loss: 0.0606 - val_loss: 0.0478\n",
      "Epoch 52/150\n",
      "756/756 [==============================] - 0s - loss: 0.0600 - val_loss: 0.0481\n",
      "Epoch 53/150\n",
      "756/756 [==============================] - 0s - loss: 0.0595 - val_loss: 0.0480\n",
      "Epoch 54/150\n",
      "756/756 [==============================] - 0s - loss: 0.0590 - val_loss: 0.0479\n",
      "Epoch 55/150\n",
      "756/756 [==============================] - 0s - loss: 0.0586 - val_loss: 0.0483\n",
      "Epoch 56/150\n",
      "756/756 [==============================] - 0s - loss: 0.0581 - val_loss: 0.0481\n",
      "Epoch 57/150\n",
      "756/756 [==============================] - 0s - loss: 0.0578 - val_loss: 0.0484\n",
      "Epoch 58/150\n",
      "756/756 [==============================] - 0s - loss: 0.0572 - val_loss: 0.0483\n",
      "Epoch 59/150\n",
      "756/756 [==============================] - 0s - loss: 0.0567 - val_loss: 0.0483\n",
      "Epoch 60/150\n",
      "756/756 [==============================] - 0s - loss: 0.0566 - val_loss: 0.0483\n",
      "Epoch 61/150\n",
      "756/756 [==============================] - 0s - loss: 0.0563 - val_loss: 0.0487\n",
      "Epoch 62/150\n",
      "756/756 [==============================] - 0s - loss: 0.0556 - val_loss: 0.0490\n",
      "Epoch 63/150\n",
      "756/756 [==============================] - 0s - loss: 0.0552 - val_loss: 0.0484\n",
      "Epoch 64/150\n",
      "756/756 [==============================] - 0s - loss: 0.0552 - val_loss: 0.0484\n",
      "Epoch 65/150\n",
      "756/756 [==============================] - 0s - loss: 0.0550 - val_loss: 0.0495\n",
      "Epoch 66/150\n",
      "756/756 [==============================] - 0s - loss: 0.0540 - val_loss: 0.0491\n",
      "Epoch 67/150\n",
      "756/756 [==============================] - 0s - loss: 0.0539 - val_loss: 0.0484\n",
      "Epoch 68/150\n",
      "756/756 [==============================] - 0s - loss: 0.0538 - val_loss: 0.0486\n",
      "Epoch 69/150\n",
      "756/756 [==============================] - 0s - loss: 0.0537 - val_loss: 0.0502\n",
      "Epoch 70/150\n",
      "756/756 [==============================] - 0s - loss: 0.0529 - val_loss: 0.0499\n",
      "Epoch 71/150\n",
      "756/756 [==============================] - 0s - loss: 0.0524 - val_loss: 0.0485\n",
      "Epoch 72/150\n",
      "756/756 [==============================] - 0s - loss: 0.0525 - val_loss: 0.0484\n",
      "Epoch 73/150\n",
      "756/756 [==============================] - 0s - loss: 0.0527 - val_loss: 0.0496\n",
      "Epoch 74/150\n",
      "756/756 [==============================] - 0s - loss: 0.0523 - val_loss: 0.0509\n",
      "Epoch 75/150\n",
      "756/756 [==============================] - 0s - loss: 0.0514 - val_loss: 0.0498\n",
      "Epoch 76/150\n",
      "756/756 [==============================] - 0s - loss: 0.0512 - val_loss: 0.0483\n",
      "Epoch 77/150\n",
      "756/756 [==============================] - 0s - loss: 0.0516 - val_loss: 0.0489\n",
      "Epoch 78/150\n",
      "756/756 [==============================] - 0s - loss: 0.0515 - val_loss: 0.0502\n",
      "Epoch 79/150\n",
      "756/756 [==============================] - 0s - loss: 0.0510 - val_loss: 0.0508\n",
      "Epoch 80/150\n",
      "756/756 [==============================] - 0s - loss: 0.0503 - val_loss: 0.0497\n",
      "Epoch 81/150\n",
      "756/756 [==============================] - 0s - loss: 0.0501 - val_loss: 0.0489\n",
      "Epoch 82/150\n",
      "756/756 [==============================] - 0s - loss: 0.0502 - val_loss: 0.0489\n",
      "Epoch 83/150\n",
      "756/756 [==============================] - 0s - loss: 0.0503 - val_loss: 0.0500\n",
      "Epoch 84/150\n",
      "756/756 [==============================] - 0s - loss: 0.0503 - val_loss: 0.0516\n",
      "Epoch 85/150\n",
      "756/756 [==============================] - 0s - loss: 0.0494 - val_loss: 0.0509\n",
      "Epoch 86/150\n",
      "756/756 [==============================] - 0s - loss: 0.0489 - val_loss: 0.0491\n",
      "Epoch 87/150\n",
      "756/756 [==============================] - 0s - loss: 0.0489 - val_loss: 0.0486\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 0s - loss: 0.0492 - val_loss: 0.0490\n",
      "Epoch 89/150\n",
      "756/756 [==============================] - 0s - loss: 0.0495 - val_loss: 0.0506\n",
      "Epoch 90/150\n",
      "756/756 [==============================] - 0s - loss: 0.0497 - val_loss: 0.0528\n",
      "Epoch 91/150\n",
      "756/756 [==============================] - 0s - loss: 0.0486 - val_loss: 0.0518\n",
      "Epoch 92/150\n",
      "756/756 [==============================] - 0s - loss: 0.0478 - val_loss: 0.0491\n",
      "Epoch 93/150\n",
      "756/756 [==============================] - 0s - loss: 0.0481 - val_loss: 0.0482\n",
      "Epoch 94/150\n",
      "756/756 [==============================] - 0s - loss: 0.0485 - val_loss: 0.0488\n",
      "Epoch 95/150\n",
      "756/756 [==============================] - 0s - loss: 0.0490 - val_loss: 0.0520\n",
      "Epoch 96/150\n",
      "756/756 [==============================] - 0s - loss: 0.0489 - val_loss: 0.0543\n",
      "Epoch 97/150\n",
      "756/756 [==============================] - 0s - loss: 0.0475 - val_loss: 0.0510\n",
      "Epoch 98/150\n",
      "756/756 [==============================] - 0s - loss: 0.0469 - val_loss: 0.0484\n",
      "Epoch 99/150\n",
      "756/756 [==============================] - 0s - loss: 0.0475 - val_loss: 0.0481\n",
      "Epoch 100/150\n",
      "756/756 [==============================] - 0s - loss: 0.0481 - val_loss: 0.0494\n",
      "Epoch 101/150\n",
      "756/756 [==============================] - 0s - loss: 0.0490 - val_loss: 0.0550\n",
      "Epoch 102/150\n",
      "756/756 [==============================] - 0s - loss: 0.0480 - val_loss: 0.0540\n",
      "Epoch 103/150\n",
      "756/756 [==============================] - 0s - loss: 0.0463 - val_loss: 0.0495\n",
      "Epoch 104/150\n",
      "756/756 [==============================] - 0s - loss: 0.0464 - val_loss: 0.0481\n",
      "Epoch 105/150\n",
      "756/756 [==============================] - 0s - loss: 0.0472 - val_loss: 0.0487\n",
      "Epoch 106/150\n",
      "756/756 [==============================] - 0s - loss: 0.0479 - val_loss: 0.0524\n",
      "Epoch 107/150\n",
      "756/756 [==============================] - 0s - loss: 0.0479 - val_loss: 0.0552\n",
      "Epoch 108/150\n",
      "756/756 [==============================] - 0s - loss: 0.0463 - val_loss: 0.0517\n",
      "Epoch 109/150\n",
      "756/756 [==============================] - 0s - loss: 0.0455 - val_loss: 0.0487\n",
      "Epoch 110/150\n",
      "756/756 [==============================] - 0s - loss: 0.0461 - val_loss: 0.0481\n",
      "Epoch 111/150\n",
      "756/756 [==============================] - 0s - loss: 0.0468 - val_loss: 0.0496\n",
      "Epoch 112/150\n",
      "756/756 [==============================] - 0s - loss: 0.0477 - val_loss: 0.0551\n",
      "Epoch 113/150\n",
      "756/756 [==============================] - 0s - loss: 0.0471 - val_loss: 0.0550\n",
      "Epoch 114/150\n",
      "756/756 [==============================] - 0s - loss: 0.0454 - val_loss: 0.0503\n",
      "Epoch 115/150\n",
      "756/756 [==============================] - 0s - loss: 0.0449 - val_loss: 0.0483\n",
      "Epoch 116/150\n",
      "756/756 [==============================] - 0s - loss: 0.0458 - val_loss: 0.0484\n",
      "Epoch 117/150\n",
      "756/756 [==============================] - 0s - loss: 0.0464 - val_loss: 0.0508\n",
      "Epoch 118/150\n",
      "756/756 [==============================] - 0s - loss: 0.0475 - val_loss: 0.0570\n",
      "Epoch 119/150\n",
      "756/756 [==============================] - 0s - loss: 0.0464 - val_loss: 0.0542\n",
      "Epoch 120/150\n",
      "756/756 [==============================] - 0s - loss: 0.0445 - val_loss: 0.0496\n",
      "Epoch 121/150\n",
      "756/756 [==============================] - 0s - loss: 0.0446 - val_loss: 0.0481\n",
      "Epoch 122/150\n",
      "756/756 [==============================] - 0s - loss: 0.0456 - val_loss: 0.0483\n",
      "Epoch 123/150\n",
      "756/756 [==============================] - 0s - loss: 0.0462 - val_loss: 0.0520\n",
      "Epoch 124/150\n",
      "756/756 [==============================] - 0s - loss: 0.0471 - val_loss: 0.0582\n",
      "Epoch 125/150\n",
      "756/756 [==============================] - 0s - loss: 0.0458 - val_loss: 0.0536\n",
      "Epoch 126/150\n",
      "756/756 [==============================] - 0s - loss: 0.0438 - val_loss: 0.0490\n",
      "Epoch 127/150\n",
      "756/756 [==============================] - 0s - loss: 0.0444 - val_loss: 0.0481\n",
      "Epoch 128/150\n",
      "756/756 [==============================] - 0s - loss: 0.0454 - val_loss: 0.0489\n",
      "Epoch 129/150\n",
      "756/756 [==============================] - 0s - loss: 0.0462 - val_loss: 0.0547\n",
      "Epoch 130/150\n",
      "756/756 [==============================] - 0s - loss: 0.0467 - val_loss: 0.0580\n",
      "Epoch 131/150\n",
      "756/756 [==============================] - 0s - loss: 0.0446 - val_loss: 0.0517\n",
      "Epoch 132/150\n",
      "756/756 [==============================] - 0s - loss: 0.0434 - val_loss: 0.0482\n",
      "Epoch 133/150\n",
      "756/756 [==============================] - 0s - loss: 0.0446 - val_loss: 0.0481\n",
      "Epoch 134/150\n",
      "756/756 [==============================] - 0s - loss: 0.0451 - val_loss: 0.0501\n",
      "Epoch 135/150\n",
      "756/756 [==============================] - 0s - loss: 0.0461 - val_loss: 0.0574\n",
      "Epoch 136/150\n",
      "756/756 [==============================] - 0s - loss: 0.0459 - val_loss: 0.0564\n",
      "Epoch 137/150\n",
      "756/756 [==============================] - 0s - loss: 0.0435 - val_loss: 0.0501\n",
      "Epoch 138/150\n",
      "756/756 [==============================] - 0s - loss: 0.0431 - val_loss: 0.0482\n",
      "Epoch 139/150\n",
      "756/756 [==============================] - 0s - loss: 0.0442 - val_loss: 0.0483\n",
      "Epoch 140/150\n",
      "756/756 [==============================] - 0s - loss: 0.0449 - val_loss: 0.0514\n",
      "Epoch 141/150\n",
      "756/756 [==============================] - 0s - loss: 0.0460 - val_loss: 0.0589\n",
      "Epoch 142/150\n",
      "756/756 [==============================] - 0s - loss: 0.0450 - val_loss: 0.0548\n",
      "Epoch 143/150\n",
      "756/756 [==============================] - 0s - loss: 0.0427 - val_loss: 0.0493\n",
      "Epoch 144/150\n",
      "756/756 [==============================] - 0s - loss: 0.0432 - val_loss: 0.0481\n",
      "Epoch 145/150\n",
      "756/756 [==============================] - 0s - loss: 0.0442 - val_loss: 0.0488\n",
      "Epoch 146/150\n",
      "756/756 [==============================] - 0s - loss: 0.0448 - val_loss: 0.0547\n",
      "Epoch 147/150\n",
      "756/756 [==============================] - 0s - loss: 0.0454 - val_loss: 0.0580\n",
      "Epoch 148/150\n",
      "756/756 [==============================] - 0s - loss: 0.0436 - val_loss: 0.0524\n",
      "Epoch 149/150\n",
      "756/756 [==============================] - 0s - loss: 0.0420 - val_loss: 0.0489\n",
      "Epoch 150/150\n",
      "756/756 [==============================] - 0s - loss: 0.0429 - val_loss: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  app.launch_new_instance()\n",
      "\r",
      "  5%|▍         | 1/22 [00:31<11:08, 31.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 756 samples, validate on 190 samples\n",
      "Epoch 1/150\n",
      "756/756 [==============================] - 8s - loss: 0.3531 - val_loss: 0.2527\n",
      "Epoch 2/150\n",
      "756/756 [==============================] - 0s - loss: 0.2848 - val_loss: 0.1861\n",
      "Epoch 3/150\n",
      "756/756 [==============================] - 0s - loss: 0.2444 - val_loss: 0.1440\n",
      "Epoch 4/150\n",
      "756/756 [==============================] - 0s - loss: 0.2163 - val_loss: 0.1154\n",
      "Epoch 5/150\n",
      "756/756 [==============================] - 0s - loss: 0.1945 - val_loss: 0.0976\n",
      "Epoch 6/150\n",
      "756/756 [==============================] - 0s - loss: 0.1791 - val_loss: 0.0915\n",
      "Epoch 7/150\n",
      "756/756 [==============================] - 0s - loss: 0.1676 - val_loss: 0.0912\n",
      "Epoch 8/150\n",
      "756/756 [==============================] - 0s - loss: 0.1574 - val_loss: 0.0909\n",
      "Epoch 9/150\n",
      "756/756 [==============================] - 0s - loss: 0.1477 - val_loss: 0.0901\n",
      "Epoch 10/150\n",
      "756/756 [==============================] - 0s - loss: 0.1384 - val_loss: 0.0901\n",
      "Epoch 11/150\n",
      "756/756 [==============================] - 0s - loss: 0.1298 - val_loss: 0.0890\n",
      "Epoch 12/150\n",
      "756/756 [==============================] - 0s - loss: 0.1218 - val_loss: 0.0860\n",
      "Epoch 13/150\n",
      "756/756 [==============================] - 0s - loss: 0.1143 - val_loss: 0.0827\n",
      "Epoch 14/150\n",
      "756/756 [==============================] - 0s - loss: 0.1078 - val_loss: 0.0804\n",
      "Epoch 15/150\n",
      "756/756 [==============================] - 0s - loss: 0.1022 - val_loss: 0.0800\n",
      "Epoch 16/150\n",
      "756/756 [==============================] - 0s - loss: 0.0971 - val_loss: 0.0793\n",
      "Epoch 17/150\n",
      "756/756 [==============================] - 0s - loss: 0.0930 - val_loss: 0.0787\n",
      "Epoch 18/150\n",
      "756/756 [==============================] - 0s - loss: 0.0896 - val_loss: 0.0777\n",
      "Epoch 19/150\n",
      "756/756 [==============================] - 0s - loss: 0.0864 - val_loss: 0.0767\n",
      "Epoch 20/150\n",
      "756/756 [==============================] - 0s - loss: 0.0837 - val_loss: 0.0765\n",
      "Epoch 21/150\n",
      "756/756 [==============================] - 0s - loss: 0.0812 - val_loss: 0.0766\n",
      "Epoch 22/150\n",
      "756/756 [==============================] - 0s - loss: 0.0787 - val_loss: 0.0762\n",
      "Epoch 23/150\n",
      "756/756 [==============================] - 0s - loss: 0.0768 - val_loss: 0.0762\n",
      "Epoch 24/150\n",
      "756/756 [==============================] - 0s - loss: 0.0748 - val_loss: 0.0756\n",
      "Epoch 25/150\n",
      "756/756 [==============================] - 0s - loss: 0.0732 - val_loss: 0.0753\n",
      "Epoch 26/150\n",
      "756/756 [==============================] - 0s - loss: 0.0714 - val_loss: 0.0751\n",
      "Epoch 27/150\n",
      "756/756 [==============================] - 0s - loss: 0.0699 - val_loss: 0.0747\n",
      "Epoch 28/150\n",
      "756/756 [==============================] - 0s - loss: 0.0684 - val_loss: 0.0750\n",
      "Epoch 29/150\n",
      "756/756 [==============================] - 0s - loss: 0.0670 - val_loss: 0.0738\n",
      "Epoch 30/150\n",
      "756/756 [==============================] - 0s - loss: 0.0657 - val_loss: 0.0745\n",
      "Epoch 31/150\n",
      "756/756 [==============================] - 0s - loss: 0.0643 - val_loss: 0.0733\n",
      "Epoch 32/150\n",
      "756/756 [==============================] - 0s - loss: 0.0632 - val_loss: 0.0729\n",
      "Epoch 33/150\n",
      "756/756 [==============================] - 0s - loss: 0.0619 - val_loss: 0.0735\n",
      "Epoch 34/150\n",
      "756/756 [==============================] - 0s - loss: 0.0607 - val_loss: 0.0716\n",
      "Epoch 35/150\n",
      "756/756 [==============================] - 0s - loss: 0.0597 - val_loss: 0.0715\n",
      "Epoch 36/150\n",
      "756/756 [==============================] - 0s - loss: 0.0585 - val_loss: 0.0731\n",
      "Epoch 37/150\n",
      "756/756 [==============================] - 0s - loss: 0.0573 - val_loss: 0.0701\n",
      "Epoch 38/150\n",
      "756/756 [==============================] - 0s - loss: 0.0566 - val_loss: 0.0710\n",
      "Epoch 39/150\n",
      "756/756 [==============================] - 0s - loss: 0.0554 - val_loss: 0.0733\n",
      "Epoch 40/150\n",
      "756/756 [==============================] - 0s - loss: 0.0543 - val_loss: 0.0683\n",
      "Epoch 41/150\n",
      "756/756 [==============================] - 0s - loss: 0.0542 - val_loss: 0.0708\n",
      "Epoch 42/150\n",
      "756/756 [==============================] - 0s - loss: 0.0527 - val_loss: 0.0738\n",
      "Epoch 43/150\n",
      "756/756 [==============================] - 0s - loss: 0.0519 - val_loss: 0.0672\n",
      "Epoch 44/150\n",
      "756/756 [==============================] - 0s - loss: 0.0521 - val_loss: 0.0707\n",
      "Epoch 45/150\n",
      "756/756 [==============================] - 0s - loss: 0.0505 - val_loss: 0.0736\n",
      "Epoch 46/150\n",
      "756/756 [==============================] - 0s - loss: 0.0498 - val_loss: 0.0664\n",
      "Epoch 47/150\n",
      "756/756 [==============================] - 0s - loss: 0.0504 - val_loss: 0.0696\n",
      "Epoch 48/150\n",
      "756/756 [==============================] - 0s - loss: 0.0489 - val_loss: 0.0742\n",
      "Epoch 49/150\n",
      "756/756 [==============================] - 0s - loss: 0.0481 - val_loss: 0.0660\n",
      "Epoch 50/150\n",
      "756/756 [==============================] - 0s - loss: 0.0488 - val_loss: 0.0680\n",
      "Epoch 51/150\n",
      "756/756 [==============================] - 0s - loss: 0.0478 - val_loss: 0.0755\n",
      "Epoch 52/150\n",
      "756/756 [==============================] - 0s - loss: 0.0467 - val_loss: 0.0658\n",
      "Epoch 53/150\n",
      "756/756 [==============================] - 0s - loss: 0.0474 - val_loss: 0.0662\n",
      "Epoch 54/150\n",
      "756/756 [==============================] - 0s - loss: 0.0470 - val_loss: 0.0762\n",
      "Epoch 55/150\n",
      "756/756 [==============================] - 0s - loss: 0.0455 - val_loss: 0.0660\n",
      "Epoch 56/150\n",
      "756/756 [==============================] - 0s - loss: 0.0462 - val_loss: 0.0643\n",
      "Epoch 57/150\n",
      "756/756 [==============================] - 0s - loss: 0.0466 - val_loss: 0.0777\n",
      "Epoch 58/150\n",
      "756/756 [==============================] - ETA: 0s - loss: 0.041 - 0s - loss: 0.0445 - val_loss: 0.0658\n",
      "Epoch 59/150\n",
      "756/756 [==============================] - 0s - loss: 0.0452 - val_loss: 0.0632\n",
      "Epoch 60/150\n",
      "756/756 [==============================] - 0s - loss: 0.0461 - val_loss: 0.0788\n",
      "Epoch 61/150\n",
      "756/756 [==============================] - 0s - loss: 0.0436 - val_loss: 0.0660\n",
      "Epoch 62/150\n",
      "756/756 [==============================] - 0s - loss: 0.0444 - val_loss: 0.0628\n",
      "Epoch 63/150\n",
      "756/756 [==============================] - 0s - loss: 0.0456 - val_loss: 0.0794\n",
      "Epoch 64/150\n",
      "756/756 [==============================] - 0s - loss: 0.0429 - val_loss: 0.0658\n",
      "Epoch 65/150\n",
      "756/756 [==============================] - 0s - loss: 0.0438 - val_loss: 0.0628\n",
      "Epoch 66/150\n",
      "756/756 [==============================] - 0s - loss: 0.0450 - val_loss: 0.0790\n",
      "Epoch 67/150\n",
      "756/756 [==============================] - 0s - loss: 0.0423 - val_loss: 0.0663\n",
      "Epoch 68/150\n",
      "756/756 [==============================] - 0s - loss: 0.0431 - val_loss: 0.0628\n",
      "Epoch 69/150\n",
      "756/756 [==============================] - 0s - loss: 0.0444 - val_loss: 0.0786\n",
      "Epoch 70/150\n",
      "756/756 [==============================] - 0s - loss: 0.0417 - val_loss: 0.0663\n",
      "Epoch 71/150\n",
      "756/756 [==============================] - 0s - loss: 0.0425 - val_loss: 0.0631\n",
      "Epoch 72/150\n",
      "756/756 [==============================] - 0s - loss: 0.0438 - val_loss: 0.0782\n",
      "Epoch 73/150\n",
      "756/756 [==============================] - 0s - loss: 0.0412 - val_loss: 0.0674\n",
      "Epoch 74/150\n",
      "756/756 [==============================] - 0s - loss: 0.0417 - val_loss: 0.0636\n",
      "Epoch 75/150\n",
      "756/756 [==============================] - 0s - loss: 0.0430 - val_loss: 0.0768\n",
      "Epoch 76/150\n",
      "756/756 [==============================] - 0s - loss: 0.0407 - val_loss: 0.0685\n",
      "Epoch 77/150\n",
      "756/756 [==============================] - 0s - loss: 0.0410 - val_loss: 0.0638\n",
      "Epoch 78/150\n",
      "756/756 [==============================] - 0s - loss: 0.0423 - val_loss: 0.0749\n",
      "Epoch 79/150\n",
      "756/756 [==============================] - 0s - loss: 0.0404 - val_loss: 0.0707\n",
      "Epoch 80/150\n",
      "756/756 [==============================] - 0s - loss: 0.0403 - val_loss: 0.0633\n",
      "Epoch 81/150\n",
      "756/756 [==============================] - 0s - loss: 0.0417 - val_loss: 0.0726\n",
      "Epoch 82/150\n",
      "756/756 [==============================] - 0s - loss: 0.0403 - val_loss: 0.0745\n",
      "Epoch 83/150\n",
      "756/756 [==============================] - 0s - loss: 0.0397 - val_loss: 0.0639\n",
      "Epoch 84/150\n",
      "756/756 [==============================] - 0s - loss: 0.0409 - val_loss: 0.0673\n",
      "Epoch 85/150\n",
      "756/756 [==============================] - 0s - loss: 0.0408 - val_loss: 0.0790\n",
      "Epoch 86/150\n",
      "756/756 [==============================] - 0s - loss: 0.0399 - val_loss: 0.0671\n",
      "Epoch 87/150\n",
      "756/756 [==============================] - 0s - loss: 0.0400 - val_loss: 0.0617\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 0s - loss: 0.0417 - val_loss: 0.0813\n",
      "Epoch 89/150\n",
      "756/756 [==============================] - 0s - loss: 0.0401 - val_loss: 0.0709\n",
      "Epoch 90/150\n",
      "756/756 [==============================] - 0s - loss: 0.0393 - val_loss: 0.0587\n",
      "Epoch 91/150\n",
      "756/756 [==============================] - 0s - loss: 0.0425 - val_loss: 0.0805\n",
      "Epoch 92/150\n",
      "756/756 [==============================] - 0s - loss: 0.0397 - val_loss: 0.0737\n",
      "Epoch 93/150\n",
      "756/756 [==============================] - 0s - loss: 0.0389 - val_loss: 0.0592\n",
      "Epoch 94/150\n",
      "756/756 [==============================] - 0s - loss: 0.0420 - val_loss: 0.0758\n",
      "Epoch 95/150\n",
      "756/756 [==============================] - 0s - loss: 0.0392 - val_loss: 0.0771\n",
      "Epoch 96/150\n",
      "756/756 [==============================] - 0s - loss: 0.0388 - val_loss: 0.0627\n",
      "Epoch 97/150\n",
      "756/756 [==============================] - 0s - loss: 0.0403 - val_loss: 0.0670\n",
      "Epoch 98/150\n",
      "756/756 [==============================] - 0s - loss: 0.0399 - val_loss: 0.0818\n",
      "Epoch 99/150\n",
      "756/756 [==============================] - 0s - loss: 0.0393 - val_loss: 0.0676\n",
      "Epoch 100/150\n",
      "756/756 [==============================] - 0s - loss: 0.0389 - val_loss: 0.0598\n",
      "Epoch 101/150\n",
      "756/756 [==============================] - 0s - loss: 0.0416 - val_loss: 0.0851\n",
      "Epoch 102/150\n",
      "756/756 [==============================] - 0s - loss: 0.0397 - val_loss: 0.0709\n",
      "Epoch 103/150\n",
      "756/756 [==============================] - 0s - loss: 0.0386 - val_loss: 0.0584\n",
      "Epoch 104/150\n",
      "756/756 [==============================] - 0s - loss: 0.0423 - val_loss: 0.0841\n",
      "Epoch 105/150\n",
      "756/756 [==============================] - 0s - loss: 0.0388 - val_loss: 0.0716\n",
      "Epoch 106/150\n",
      "756/756 [==============================] - 0s - loss: 0.0382 - val_loss: 0.0602\n",
      "Epoch 107/150\n",
      "756/756 [==============================] - 0s - loss: 0.0412 - val_loss: 0.0795\n",
      "Epoch 108/150\n",
      "756/756 [==============================] - 0s - loss: 0.0381 - val_loss: 0.0743\n",
      "Epoch 109/150\n",
      "756/756 [==============================] - 0s - loss: 0.0375 - val_loss: 0.0623\n",
      "Epoch 110/150\n",
      "756/756 [==============================] - 0s - loss: 0.0395 - val_loss: 0.0728\n",
      "Epoch 111/150\n",
      "756/756 [==============================] - 0s - loss: 0.0380 - val_loss: 0.0784\n",
      "Epoch 112/150\n",
      "756/756 [==============================] - 0s - loss: 0.0375 - val_loss: 0.0670\n",
      "Epoch 113/150\n",
      "756/756 [==============================] - 0s - loss: 0.0377 - val_loss: 0.0640\n",
      "Epoch 114/150\n",
      "756/756 [==============================] - 0s - loss: 0.0390 - val_loss: 0.0826\n",
      "Epoch 115/150\n",
      "756/756 [==============================] - 0s - loss: 0.0382 - val_loss: 0.0720\n",
      "Epoch 116/150\n",
      "756/756 [==============================] - 0s - loss: 0.0370 - val_loss: 0.0593\n",
      "Epoch 117/150\n",
      "756/756 [==============================] - 0s - loss: 0.0402 - val_loss: 0.0812\n",
      "Epoch 118/150\n",
      "756/756 [==============================] - 0s - loss: 0.0379 - val_loss: 0.0763\n",
      "Epoch 119/150\n",
      "756/756 [==============================] - 0s - loss: 0.0367 - val_loss: 0.0597\n",
      "Epoch 120/150\n",
      "756/756 [==============================] - 0s - loss: 0.0398 - val_loss: 0.0742\n",
      "Epoch 121/150\n",
      "756/756 [==============================] - 0s - loss: 0.0376 - val_loss: 0.0810\n",
      "Epoch 122/150\n",
      "756/756 [==============================] - 0s - loss: 0.0372 - val_loss: 0.0645\n",
      "Epoch 123/150\n",
      "756/756 [==============================] - 0s - loss: 0.0379 - val_loss: 0.0636\n",
      "Epoch 124/150\n",
      "756/756 [==============================] - 0s - loss: 0.0390 - val_loss: 0.0865\n",
      "Epoch 125/150\n",
      "756/756 [==============================] - 0s - loss: 0.0385 - val_loss: 0.0698\n",
      "Epoch 126/150\n",
      "756/756 [==============================] - 0s - loss: 0.0370 - val_loss: 0.0587\n",
      "Epoch 127/150\n",
      "756/756 [==============================] - 0s - loss: 0.0409 - val_loss: 0.0889\n",
      "Epoch 128/150\n",
      "756/756 [==============================] - 0s - loss: 0.0383 - val_loss: 0.0700\n",
      "Epoch 129/150\n",
      "756/756 [==============================] - 0s - loss: 0.0371 - val_loss: 0.0598\n",
      "Epoch 130/150\n",
      "756/756 [==============================] - 0s - loss: 0.0406 - val_loss: 0.0866\n",
      "Epoch 131/150\n",
      "756/756 [==============================] - 0s - loss: 0.0374 - val_loss: 0.0706\n",
      "Epoch 132/150\n",
      "756/756 [==============================] - 0s - loss: 0.0366 - val_loss: 0.0620\n",
      "Epoch 133/150\n",
      "756/756 [==============================] - 0s - loss: 0.0390 - val_loss: 0.0817\n",
      "Epoch 134/150\n",
      "756/756 [==============================] - 0s - loss: 0.0365 - val_loss: 0.0734\n",
      "Epoch 135/150\n",
      "756/756 [==============================] - 0s - loss: 0.0357 - val_loss: 0.0635\n",
      "Epoch 136/150\n",
      "756/756 [==============================] - 0s - loss: 0.0376 - val_loss: 0.0748\n",
      "Epoch 137/150\n",
      "756/756 [==============================] - 0s - loss: 0.0362 - val_loss: 0.0791\n",
      "Epoch 138/150\n",
      "756/756 [==============================] - 0s - loss: 0.0357 - val_loss: 0.0666\n",
      "Epoch 139/150\n",
      "756/756 [==============================] - 0s - loss: 0.0361 - val_loss: 0.0655\n",
      "Epoch 140/150\n",
      "756/756 [==============================] - 0s - loss: 0.0371 - val_loss: 0.0835\n",
      "Epoch 141/150\n",
      "756/756 [==============================] - 0s - loss: 0.0367 - val_loss: 0.0721\n",
      "Epoch 142/150\n",
      "756/756 [==============================] - 0s - loss: 0.0353 - val_loss: 0.0610\n",
      "Epoch 143/150\n",
      "756/756 [==============================] - 0s - loss: 0.0378 - val_loss: 0.0804\n",
      "Epoch 144/150\n",
      "756/756 [==============================] - 0s - loss: 0.0364 - val_loss: 0.0786\n",
      "Epoch 145/150\n",
      "756/756 [==============================] - 0s - loss: 0.0351 - val_loss: 0.0611\n",
      "Epoch 146/150\n",
      "756/756 [==============================] - 0s - loss: 0.0376 - val_loss: 0.0719\n",
      "Epoch 147/150\n",
      "756/756 [==============================] - 0s - loss: 0.0363 - val_loss: 0.0842\n",
      "Epoch 148/150\n",
      "756/756 [==============================] - 0s - loss: 0.0362 - val_loss: 0.0665\n",
      "Epoch 149/150\n",
      "756/756 [==============================] - 0s - loss: 0.0360 - val_loss: 0.0618\n",
      "Epoch 150/150\n",
      "756/756 [==============================] - 0s - loss: 0.0382 - val_loss: 0.0894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 2/22 [01:07<11:02, 33.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 756 samples, validate on 190 samples\n",
      "Epoch 1/150\n",
      "756/756 [==============================] - 5s - loss: 0.4642 - val_loss: 0.3190\n",
      "Epoch 2/150\n",
      "756/756 [==============================] - 0s - loss: 0.3441 - val_loss: 0.2485\n",
      "Epoch 3/150\n",
      "756/756 [==============================] - 0s - loss: 0.2783 - val_loss: 0.1997\n",
      "Epoch 4/150\n",
      "756/756 [==============================] - 0s - loss: 0.2353 - val_loss: 0.1639\n",
      "Epoch 5/150\n",
      "756/756 [==============================] - 0s - loss: 0.2036 - val_loss: 0.1303\n",
      "Epoch 6/150\n",
      "756/756 [==============================] - 0s - loss: 0.1803 - val_loss: 0.1118\n",
      "Epoch 7/150\n",
      "756/756 [==============================] - 0s - loss: 0.1655 - val_loss: 0.1035\n",
      "Epoch 8/150\n",
      "756/756 [==============================] - 0s - loss: 0.1556 - val_loss: 0.1012\n",
      "Epoch 9/150\n",
      "756/756 [==============================] - 0s - loss: 0.1477 - val_loss: 0.0985\n",
      "Epoch 10/150\n",
      "756/756 [==============================] - 0s - loss: 0.1395 - val_loss: 0.0946\n",
      "Epoch 11/150\n",
      "756/756 [==============================] - 0s - loss: 0.1326 - val_loss: 0.0912\n",
      "Epoch 12/150\n",
      "756/756 [==============================] - 0s - loss: 0.1267 - val_loss: 0.0882\n",
      "Epoch 13/150\n",
      "756/756 [==============================] - 0s - loss: 0.1211 - val_loss: 0.0858\n",
      "Epoch 14/150\n",
      "756/756 [==============================] - 0s - loss: 0.1158 - val_loss: 0.0837\n",
      "Epoch 15/150\n",
      "756/756 [==============================] - 0s - loss: 0.1107 - val_loss: 0.0814\n",
      "Epoch 16/150\n",
      "756/756 [==============================] - 0s - loss: 0.1063 - val_loss: 0.0789\n",
      "Epoch 17/150\n",
      "756/756 [==============================] - 0s - loss: 0.1023 - val_loss: 0.0764\n",
      "Epoch 18/150\n",
      "756/756 [==============================] - 0s - loss: 0.0985 - val_loss: 0.0742\n",
      "Epoch 19/150\n",
      "756/756 [==============================] - 0s - loss: 0.0953 - val_loss: 0.0720\n",
      "Epoch 20/150\n",
      "756/756 [==============================] - 0s - loss: 0.0925 - val_loss: 0.0700\n",
      "Epoch 21/150\n",
      "756/756 [==============================] - 0s - loss: 0.0901 - val_loss: 0.0680\n",
      "Epoch 22/150\n",
      "756/756 [==============================] - 0s - loss: 0.0876 - val_loss: 0.0659\n",
      "Epoch 23/150\n",
      "756/756 [==============================] - 0s - loss: 0.0858 - val_loss: 0.0642\n",
      "Epoch 24/150\n",
      "756/756 [==============================] - 0s - loss: 0.0837 - val_loss: 0.0625\n",
      "Epoch 25/150\n",
      "756/756 [==============================] - 0s - loss: 0.0822 - val_loss: 0.0610\n",
      "Epoch 26/150\n",
      "756/756 [==============================] - 0s - loss: 0.0803 - val_loss: 0.0598\n",
      "Epoch 27/150\n",
      "756/756 [==============================] - 0s - loss: 0.0791 - val_loss: 0.0587\n",
      "Epoch 28/150\n",
      "756/756 [==============================] - 0s - loss: 0.0773 - val_loss: 0.0575\n",
      "Epoch 29/150\n",
      "756/756 [==============================] - 0s - loss: 0.0763 - val_loss: 0.0565\n",
      "Epoch 30/150\n",
      "756/756 [==============================] - 0s - loss: 0.0746 - val_loss: 0.0557\n",
      "Epoch 31/150\n",
      "756/756 [==============================] - 0s - loss: 0.0737 - val_loss: 0.0548\n",
      "Epoch 32/150\n",
      "756/756 [==============================] - 0s - loss: 0.0723 - val_loss: 0.0540\n",
      "Epoch 33/150\n",
      "756/756 [==============================] - 0s - loss: 0.0714 - val_loss: 0.0531\n",
      "Epoch 34/150\n",
      "756/756 [==============================] - 0s - loss: 0.0702 - val_loss: 0.0526\n",
      "Epoch 35/150\n",
      "756/756 [==============================] - 0s - loss: 0.0694 - val_loss: 0.0516\n",
      "Epoch 36/150\n",
      "756/756 [==============================] - 0s - loss: 0.0684 - val_loss: 0.0513\n",
      "Epoch 37/150\n",
      "756/756 [==============================] - 0s - loss: 0.0677 - val_loss: 0.0503\n",
      "Epoch 38/150\n",
      "756/756 [==============================] - 0s - loss: 0.0669 - val_loss: 0.0503\n",
      "Epoch 39/150\n",
      "756/756 [==============================] - 0s - loss: 0.0662 - val_loss: 0.0492\n",
      "Epoch 40/150\n",
      "756/756 [==============================] - 0s - loss: 0.0656 - val_loss: 0.0496\n",
      "Epoch 41/150\n",
      "756/756 [==============================] - 0s - loss: 0.0649 - val_loss: 0.0485\n",
      "Epoch 42/150\n",
      "756/756 [==============================] - 0s - loss: 0.0646 - val_loss: 0.0490\n",
      "Epoch 43/150\n",
      "756/756 [==============================] - 0s - loss: 0.0638 - val_loss: 0.0479\n",
      "Epoch 44/150\n",
      "756/756 [==============================] - 0s - loss: 0.0636 - val_loss: 0.0487\n",
      "Epoch 45/150\n",
      "756/756 [==============================] - 0s - loss: 0.0630 - val_loss: 0.0473\n",
      "Epoch 46/150\n",
      "756/756 [==============================] - 0s - loss: 0.0629 - val_loss: 0.0482\n",
      "Epoch 47/150\n",
      "756/756 [==============================] - 0s - loss: 0.0622 - val_loss: 0.0471\n",
      "Epoch 48/150\n",
      "756/756 [==============================] - 0s - loss: 0.0622 - val_loss: 0.0477\n",
      "Epoch 49/150\n",
      "756/756 [==============================] - 0s - loss: 0.0617 - val_loss: 0.0468\n",
      "Epoch 50/150\n",
      "756/756 [==============================] - 0s - loss: 0.0618 - val_loss: 0.0474\n",
      "Epoch 51/150\n",
      "756/756 [==============================] - 0s - loss: 0.0611 - val_loss: 0.0465\n",
      "Epoch 52/150\n",
      "756/756 [==============================] - 0s - loss: 0.0611 - val_loss: 0.0469\n",
      "Epoch 53/150\n",
      "756/756 [==============================] - 0s - loss: 0.0603 - val_loss: 0.0463\n",
      "Epoch 54/150\n",
      "756/756 [==============================] - 0s - loss: 0.0602 - val_loss: 0.0464\n",
      "Epoch 55/150\n",
      "756/756 [==============================] - 0s - loss: 0.0597 - val_loss: 0.0458\n",
      "Epoch 56/150\n",
      "756/756 [==============================] - 0s - loss: 0.0595 - val_loss: 0.0461\n",
      "Epoch 57/150\n",
      "756/756 [==============================] - 0s - loss: 0.0592 - val_loss: 0.0455\n",
      "Epoch 58/150\n",
      "756/756 [==============================] - 0s - loss: 0.0588 - val_loss: 0.0459\n",
      "Epoch 59/150\n",
      "756/756 [==============================] - 0s - loss: 0.0589 - val_loss: 0.0453\n",
      "Epoch 60/150\n",
      "756/756 [==============================] - 0s - loss: 0.0582 - val_loss: 0.0456\n",
      "Epoch 61/150\n",
      "756/756 [==============================] - 0s - loss: 0.0585 - val_loss: 0.0450\n",
      "Epoch 62/150\n",
      "756/756 [==============================] - 0s - loss: 0.0576 - val_loss: 0.0454\n",
      "Epoch 63/150\n",
      "756/756 [==============================] - 0s - loss: 0.0581 - val_loss: 0.0447\n",
      "Epoch 64/150\n",
      "756/756 [==============================] - 0s - loss: 0.0571 - val_loss: 0.0452\n",
      "Epoch 65/150\n",
      "756/756 [==============================] - 0s - loss: 0.0576 - val_loss: 0.0443\n",
      "Epoch 66/150\n",
      "756/756 [==============================] - 0s - loss: 0.0568 - val_loss: 0.0451\n",
      "Epoch 67/150\n",
      "756/756 [==============================] - 0s - loss: 0.0572 - val_loss: 0.0441\n",
      "Epoch 68/150\n",
      "756/756 [==============================] - 0s - loss: 0.0563 - val_loss: 0.0447\n",
      "Epoch 69/150\n",
      "756/756 [==============================] - 0s - loss: 0.0568 - val_loss: 0.0439\n",
      "Epoch 70/150\n",
      "756/756 [==============================] - 0s - loss: 0.0559 - val_loss: 0.0445\n",
      "Epoch 71/150\n",
      "756/756 [==============================] - 0s - loss: 0.0564 - val_loss: 0.0437\n",
      "Epoch 72/150\n",
      "756/756 [==============================] - 0s - loss: 0.0555 - val_loss: 0.0444\n",
      "Epoch 73/150\n",
      "756/756 [==============================] - 0s - loss: 0.0559 - val_loss: 0.0436\n",
      "Epoch 74/150\n",
      "756/756 [==============================] - 0s - loss: 0.0551 - val_loss: 0.0440\n",
      "Epoch 75/150\n",
      "756/756 [==============================] - 0s - loss: 0.0557 - val_loss: 0.0434\n",
      "Epoch 76/150\n",
      "756/756 [==============================] - 0s - loss: 0.0547 - val_loss: 0.0439\n",
      "Epoch 77/150\n",
      "756/756 [==============================] - 0s - loss: 0.0552 - val_loss: 0.0432\n",
      "Epoch 78/150\n",
      "756/756 [==============================] - 0s - loss: 0.0545 - val_loss: 0.0436\n",
      "Epoch 79/150\n",
      "756/756 [==============================] - 0s - loss: 0.0549 - val_loss: 0.0434\n",
      "Epoch 80/150\n",
      "756/756 [==============================] - 0s - loss: 0.0540 - val_loss: 0.0436\n",
      "Epoch 81/150\n",
      "756/756 [==============================] - 0s - loss: 0.0545 - val_loss: 0.0428\n",
      "Epoch 82/150\n",
      "756/756 [==============================] - 0s - loss: 0.0539 - val_loss: 0.0437\n",
      "Epoch 83/150\n",
      "756/756 [==============================] - 0s - loss: 0.0539 - val_loss: 0.0431\n",
      "Epoch 84/150\n",
      "756/756 [==============================] - 0s - loss: 0.0534 - val_loss: 0.0431\n",
      "Epoch 85/150\n",
      "756/756 [==============================] - 0s - loss: 0.0538 - val_loss: 0.0426\n",
      "Epoch 86/150\n",
      "756/756 [==============================] - 0s - loss: 0.0532 - val_loss: 0.0439\n",
      "Epoch 87/150\n",
      "756/756 [==============================] - 0s - loss: 0.0531 - val_loss: 0.0424\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 0s - loss: 0.0530 - val_loss: 0.0427\n",
      "Epoch 89/150\n",
      "756/756 [==============================] - 0s - loss: 0.0530 - val_loss: 0.0432\n",
      "Epoch 90/150\n",
      "756/756 [==============================] - 0s - loss: 0.0523 - val_loss: 0.0434\n",
      "Epoch 91/150\n",
      "756/756 [==============================] - 0s - loss: 0.0525 - val_loss: 0.0414\n",
      "Epoch 92/150\n",
      "756/756 [==============================] - 0s - loss: 0.0528 - val_loss: 0.0428\n",
      "Epoch 93/150\n",
      "756/756 [==============================] - 0s - loss: 0.0522 - val_loss: 0.0444\n",
      "Epoch 94/150\n",
      "756/756 [==============================] - 0s - loss: 0.0517 - val_loss: 0.0418\n",
      "Epoch 95/150\n",
      "756/756 [==============================] - 0s - loss: 0.0528 - val_loss: 0.0411\n",
      "Epoch 96/150\n",
      "756/756 [==============================] - 0s - loss: 0.0521 - val_loss: 0.0451\n",
      "Epoch 97/150\n",
      "756/756 [==============================] - 0s - loss: 0.0514 - val_loss: 0.0422\n",
      "Epoch 98/150\n",
      "756/756 [==============================] - 0s - loss: 0.0519 - val_loss: 0.0409\n",
      "Epoch 99/150\n",
      "756/756 [==============================] - 0s - loss: 0.0524 - val_loss: 0.0440\n",
      "Epoch 100/150\n",
      "756/756 [==============================] - 0s - loss: 0.0508 - val_loss: 0.0442\n",
      "Epoch 101/150\n",
      "756/756 [==============================] - 0s - loss: 0.0513 - val_loss: 0.0402\n",
      "Epoch 102/150\n",
      "756/756 [==============================] - 0s - loss: 0.0524 - val_loss: 0.0421\n",
      "Epoch 103/150\n",
      "756/756 [==============================] - 0s - loss: 0.0509 - val_loss: 0.0454\n",
      "Epoch 104/150\n",
      "756/756 [==============================] - 0s - loss: 0.0506 - val_loss: 0.0414\n",
      "Epoch 105/150\n",
      "756/756 [==============================] - 0s - loss: 0.0518 - val_loss: 0.0403\n",
      "Epoch 106/150\n",
      "756/756 [==============================] - 0s - loss: 0.0513 - val_loss: 0.0453\n",
      "Epoch 107/150\n",
      "756/756 [==============================] - 0s - loss: 0.0501 - val_loss: 0.0421\n",
      "Epoch 108/150\n",
      "756/756 [==============================] - 0s - loss: 0.0507 - val_loss: 0.0402\n",
      "Epoch 109/150\n",
      "756/756 [==============================] - 0s - loss: 0.0513 - val_loss: 0.0435\n",
      "Epoch 110/150\n",
      "756/756 [==============================] - 0s - loss: 0.0496 - val_loss: 0.0445\n",
      "Epoch 111/150\n",
      "756/756 [==============================] - 0s - loss: 0.0500 - val_loss: 0.0399\n",
      "Epoch 112/150\n",
      "756/756 [==============================] - 0s - loss: 0.0514 - val_loss: 0.0414\n",
      "Epoch 113/150\n",
      "756/756 [==============================] - 0s - loss: 0.0499 - val_loss: 0.0463\n",
      "Epoch 114/150\n",
      "756/756 [==============================] - 0s - loss: 0.0494 - val_loss: 0.0412\n",
      "Epoch 115/150\n",
      "756/756 [==============================] - 0s - loss: 0.0505 - val_loss: 0.0394\n",
      "Epoch 116/150\n",
      "756/756 [==============================] - 0s - loss: 0.0507 - val_loss: 0.0457\n",
      "Epoch 117/150\n",
      "756/756 [==============================] - 0s - loss: 0.0490 - val_loss: 0.0431\n",
      "Epoch 118/150\n",
      "756/756 [==============================] - 0s - loss: 0.0492 - val_loss: 0.0394\n",
      "Epoch 119/150\n",
      "756/756 [==============================] - 0s - loss: 0.0509 - val_loss: 0.0426\n",
      "Epoch 120/150\n",
      "756/756 [==============================] - 0s - loss: 0.0487 - val_loss: 0.0467\n",
      "Epoch 121/150\n",
      "756/756 [==============================] - 0s - loss: 0.0488 - val_loss: 0.0398\n",
      "Epoch 122/150\n",
      "756/756 [==============================] - 0s - loss: 0.0505 - val_loss: 0.0399\n",
      "Epoch 123/150\n",
      "756/756 [==============================] - 0s - loss: 0.0494 - val_loss: 0.0469\n",
      "Epoch 124/150\n",
      "756/756 [==============================] - 0s - loss: 0.0484 - val_loss: 0.0418\n",
      "Epoch 125/150\n",
      "756/756 [==============================] - 0s - loss: 0.0490 - val_loss: 0.0388\n",
      "Epoch 126/150\n",
      "756/756 [==============================] - 0s - loss: 0.0506 - val_loss: 0.0451\n",
      "Epoch 127/150\n",
      "756/756 [==============================] - 0s - loss: 0.0480 - val_loss: 0.0453\n",
      "Epoch 128/150\n",
      "756/756 [==============================] - 0s - loss: 0.0482 - val_loss: 0.0392\n",
      "Epoch 129/150\n",
      "756/756 [==============================] - 0s - loss: 0.0508 - val_loss: 0.0412\n",
      "Epoch 130/150\n",
      "756/756 [==============================] - 0s - loss: 0.0481 - val_loss: 0.0481\n",
      "Epoch 131/150\n",
      "756/756 [==============================] - 0s - loss: 0.0481 - val_loss: 0.0402\n",
      "Epoch 132/150\n",
      "756/756 [==============================] - 0s - loss: 0.0494 - val_loss: 0.0388\n",
      "Epoch 133/150\n",
      "756/756 [==============================] - 0s - loss: 0.0492 - val_loss: 0.0470\n",
      "Epoch 134/150\n",
      "756/756 [==============================] - 0s - loss: 0.0475 - val_loss: 0.0433\n",
      "Epoch 135/150\n",
      "756/756 [==============================] - 0s - loss: 0.0477 - val_loss: 0.0386\n",
      "Epoch 136/150\n",
      "756/756 [==============================] - 0s - loss: 0.0504 - val_loss: 0.0436\n",
      "Epoch 137/150\n",
      "756/756 [==============================] - 0s - loss: 0.0470 - val_loss: 0.0473\n",
      "Epoch 138/150\n",
      "756/756 [==============================] - 0s - loss: 0.0473 - val_loss: 0.0394\n",
      "Epoch 139/150\n",
      "756/756 [==============================] - 0s - loss: 0.0496 - val_loss: 0.0397\n",
      "Epoch 140/150\n",
      "756/756 [==============================] - 0s - loss: 0.0477 - val_loss: 0.0483\n",
      "Epoch 141/150\n",
      "756/756 [==============================] - 0s - loss: 0.0471 - val_loss: 0.0411\n",
      "Epoch 142/150\n",
      "756/756 [==============================] - 0s - loss: 0.0476 - val_loss: 0.0384\n",
      "Epoch 143/150\n",
      "756/756 [==============================] - 0s - loss: 0.0492 - val_loss: 0.0457\n",
      "Epoch 144/150\n",
      "756/756 [==============================] - 0s - loss: 0.0465 - val_loss: 0.0458\n",
      "Epoch 145/150\n",
      "756/756 [==============================] - 0s - loss: 0.0466 - val_loss: 0.0390\n",
      "Epoch 146/150\n",
      "756/756 [==============================] - 0s - loss: 0.0496 - val_loss: 0.0413\n",
      "Epoch 147/150\n",
      "756/756 [==============================] - 0s - loss: 0.0465 - val_loss: 0.0487\n",
      "Epoch 148/150\n",
      "756/756 [==============================] - 0s - loss: 0.0465 - val_loss: 0.0399\n",
      "Epoch 149/150\n",
      "756/756 [==============================] - 0s - loss: 0.0478 - val_loss: 0.0383\n",
      "Epoch 150/150\n",
      "756/756 [==============================] - 0s - loss: 0.0480 - val_loss: 0.0480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▎        | 3/22 [01:31<09:33, 30.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 756 samples, validate on 190 samples\n",
      "Epoch 1/150\n",
      "756/756 [==============================] - 5s - loss: 0.3981 - val_loss: 0.1510\n",
      "Epoch 2/150\n",
      "756/756 [==============================] - 0s - loss: 0.2961 - val_loss: 0.1575\n",
      "Epoch 3/150\n",
      "756/756 [==============================] - 0s - loss: 0.2413 - val_loss: 0.1741\n",
      "Epoch 4/150\n",
      "756/756 [==============================] - 0s - loss: 0.2056 - val_loss: 0.1845ss: 0.195\n",
      "Epoch 5/150\n",
      "756/756 [==============================] - 0s - loss: 0.1835 - val_loss: 0.1891\n",
      "Epoch 6/150\n",
      "756/756 [==============================] - 0s - loss: 0.1715 - val_loss: 0.1839\n",
      "Epoch 7/150\n",
      "756/756 [==============================] - 0s - loss: 0.1628 - val_loss: 0.1736\n",
      "Epoch 8/150\n",
      "756/756 [==============================] - 0s - loss: 0.1551 - val_loss: 0.1606\n",
      "Epoch 9/150\n",
      "756/756 [==============================] - 0s - loss: 0.1480 - val_loss: 0.1457\n",
      "Epoch 10/150\n",
      "756/756 [==============================] - 0s - loss: 0.1415 - val_loss: 0.1310\n",
      "Epoch 11/150\n",
      "756/756 [==============================] - 0s - loss: 0.1355 - val_loss: 0.1206\n",
      "Epoch 12/150\n",
      "756/756 [==============================] - 0s - loss: 0.1296 - val_loss: 0.1148\n",
      "Epoch 13/150\n",
      "756/756 [==============================] - 0s - loss: 0.1238 - val_loss: 0.1105\n",
      "Epoch 14/150\n",
      "756/756 [==============================] - 0s - loss: 0.1180 - val_loss: 0.1079\n",
      "Epoch 15/150\n",
      "756/756 [==============================] - 0s - loss: 0.1121 - val_loss: 0.1060\n",
      "Epoch 16/150\n",
      "756/756 [==============================] - 0s - loss: 0.1066 - val_loss: 0.1043\n",
      "Epoch 17/150\n",
      "756/756 [==============================] - 0s - loss: 0.1015 - val_loss: 0.1036\n",
      "Epoch 18/150\n",
      "756/756 [==============================] - 0s - loss: 0.0971 - val_loss: 0.1035\n",
      "Epoch 19/150\n",
      "756/756 [==============================] - 0s - loss: 0.0933 - val_loss: 0.1038\n",
      "Epoch 20/150\n",
      "756/756 [==============================] - 0s - loss: 0.0900 - val_loss: 0.1040\n",
      "Epoch 21/150\n",
      "756/756 [==============================] - 0s - loss: 0.0871 - val_loss: 0.1035\n",
      "Epoch 22/150\n",
      "756/756 [==============================] - 0s - loss: 0.0844 - val_loss: 0.1031\n",
      "Epoch 23/150\n",
      "756/756 [==============================] - 0s - loss: 0.0820 - val_loss: 0.1017\n",
      "Epoch 24/150\n",
      "756/756 [==============================] - 0s - loss: 0.0799 - val_loss: 0.1010\n",
      "Epoch 25/150\n",
      "756/756 [==============================] - 0s - loss: 0.0780 - val_loss: 0.1005\n",
      "Epoch 26/150\n",
      "756/756 [==============================] - 0s - loss: 0.0764 - val_loss: 0.0998\n",
      "Epoch 27/150\n",
      "756/756 [==============================] - 0s - loss: 0.0750 - val_loss: 0.0991\n",
      "Epoch 28/150\n",
      "756/756 [==============================] - 0s - loss: 0.0738 - val_loss: 0.0986\n",
      "Epoch 29/150\n",
      "756/756 [==============================] - 0s - loss: 0.0727 - val_loss: 0.0975\n",
      "Epoch 30/150\n",
      "756/756 [==============================] - 0s - loss: 0.0717 - val_loss: 0.0970\n",
      "Epoch 31/150\n",
      "756/756 [==============================] - 0s - loss: 0.0709 - val_loss: 0.0959\n",
      "Epoch 32/150\n",
      "756/756 [==============================] - 0s - loss: 0.0701 - val_loss: 0.0953\n",
      "Epoch 33/150\n",
      "756/756 [==============================] - 0s - loss: 0.0694 - val_loss: 0.0952\n",
      "Epoch 34/150\n",
      "756/756 [==============================] - 0s - loss: 0.0689 - val_loss: 0.0941\n",
      "Epoch 35/150\n",
      "756/756 [==============================] - 0s - loss: 0.0683 - val_loss: 0.0934\n",
      "Epoch 36/150\n",
      "756/756 [==============================] - 0s - loss: 0.0677 - val_loss: 0.0937\n",
      "Epoch 37/150\n",
      "756/756 [==============================] - 0s - loss: 0.0673 - val_loss: 0.0925\n",
      "Epoch 38/150\n",
      "756/756 [==============================] - 0s - loss: 0.0668 - val_loss: 0.0920\n",
      "Epoch 39/150\n",
      "756/756 [==============================] - 0s - loss: 0.0663 - val_loss: 0.0925\n",
      "Epoch 40/150\n",
      "756/756 [==============================] - 0s - loss: 0.0660 - val_loss: 0.0917\n",
      "Epoch 41/150\n",
      "756/756 [==============================] - 0s - loss: 0.0656 - val_loss: 0.0908\n",
      "Epoch 42/150\n",
      "756/756 [==============================] - 0s - loss: 0.0652 - val_loss: 0.0914\n",
      "Epoch 43/150\n",
      "756/756 [==============================] - 0s - loss: 0.0648 - val_loss: 0.0912\n",
      "Epoch 44/150\n",
      "756/756 [==============================] - 0s - loss: 0.0645 - val_loss: 0.0902\n",
      "Epoch 45/150\n",
      "756/756 [==============================] - 0s - loss: 0.0642 - val_loss: 0.0903\n",
      "Epoch 46/150\n",
      "756/756 [==============================] - 0s - loss: 0.0638 - val_loss: 0.0906\n",
      "Epoch 47/150\n",
      "756/756 [==============================] - 0s - loss: 0.0635 - val_loss: 0.0908\n",
      "Epoch 48/150\n",
      "756/756 [==============================] - 0s - loss: 0.0633 - val_loss: 0.0896\n",
      "Epoch 49/150\n",
      "756/756 [==============================] - 0s - loss: 0.0629 - val_loss: 0.0896\n",
      "Epoch 50/150\n",
      "756/756 [==============================] - 0s - loss: 0.0626 - val_loss: 0.0907\n",
      "Epoch 51/150\n",
      "756/756 [==============================] - 0s - loss: 0.0625 - val_loss: 0.0906\n",
      "Epoch 52/150\n",
      "756/756 [==============================] - 0s - loss: 0.0623 - val_loss: 0.0892\n",
      "Epoch 53/150\n",
      "756/756 [==============================] - 0s - loss: 0.0619 - val_loss: 0.0888\n",
      "Epoch 54/150\n",
      "756/756 [==============================] - 0s - loss: 0.0616 - val_loss: 0.0904\n",
      "Epoch 55/150\n",
      "756/756 [==============================] - 0s - loss: 0.0614 - val_loss: 0.0906\n",
      "Epoch 56/150\n",
      "756/756 [==============================] - 0s - loss: 0.0613 - val_loss: 0.0893\n",
      "Epoch 57/150\n",
      "756/756 [==============================] - 0s - loss: 0.0610 - val_loss: 0.0890\n",
      "Epoch 58/150\n",
      "756/756 [==============================] - 0s - loss: 0.0607 - val_loss: 0.0897\n",
      "Epoch 59/150\n",
      "756/756 [==============================] - 0s - loss: 0.0604 - val_loss: 0.0901\n",
      "Epoch 60/150\n",
      "756/756 [==============================] - 0s - loss: 0.0603 - val_loss: 0.0905\n",
      "Epoch 61/150\n",
      "756/756 [==============================] - 0s - loss: 0.0602 - val_loss: 0.0888\n",
      "Epoch 62/150\n",
      "756/756 [==============================] - 0s - loss: 0.0599 - val_loss: 0.0879\n",
      "Epoch 63/150\n",
      "756/756 [==============================] - 0s - loss: 0.0595 - val_loss: 0.0908\n",
      "Epoch 64/150\n",
      "756/756 [==============================] - 0s - loss: 0.0595 - val_loss: 0.0916\n",
      "Epoch 65/150\n",
      "756/756 [==============================] - 0s - loss: 0.0595 - val_loss: 0.0894\n",
      "Epoch 66/150\n",
      "756/756 [==============================] - 0s - loss: 0.0592 - val_loss: 0.0884\n",
      "Epoch 67/150\n",
      "756/756 [==============================] - 0s - loss: 0.0589 - val_loss: 0.0892\n",
      "Epoch 68/150\n",
      "756/756 [==============================] - 0s - loss: 0.0587 - val_loss: 0.0914\n",
      "Epoch 69/150\n",
      "756/756 [==============================] - 0s - loss: 0.0586 - val_loss: 0.0912\n",
      "Epoch 70/150\n",
      "756/756 [==============================] - 0s - loss: 0.0585 - val_loss: 0.0893\n",
      "Epoch 71/150\n",
      "756/756 [==============================] - 0s - loss: 0.0583 - val_loss: 0.0879\n",
      "Epoch 72/150\n",
      "756/756 [==============================] - 0s - loss: 0.0580 - val_loss: 0.0901\n",
      "Epoch 73/150\n",
      "756/756 [==============================] - 0s - loss: 0.0577 - val_loss: 0.0922\n",
      "Epoch 74/150\n",
      "756/756 [==============================] - 0s - loss: 0.0579 - val_loss: 0.0919\n",
      "Epoch 75/150\n",
      "756/756 [==============================] - 0s - loss: 0.0577 - val_loss: 0.0883\n",
      "Epoch 76/150\n",
      "756/756 [==============================] - 0s - loss: 0.0575 - val_loss: 0.0887\n",
      "Epoch 77/150\n",
      "756/756 [==============================] - 0s - loss: 0.0571 - val_loss: 0.0899\n",
      "Epoch 78/150\n",
      "756/756 [==============================] - 0s - loss: 0.0570 - val_loss: 0.0938\n",
      "Epoch 79/150\n",
      "756/756 [==============================] - 0s - loss: 0.0572 - val_loss: 0.0913\n",
      "Epoch 80/150\n",
      "756/756 [==============================] - 0s - loss: 0.0570 - val_loss: 0.0890\n",
      "Epoch 81/150\n",
      "756/756 [==============================] - 0s - loss: 0.0568 - val_loss: 0.0885\n",
      "Epoch 82/150\n",
      "756/756 [==============================] - 0s - loss: 0.0564 - val_loss: 0.0910\n",
      "Epoch 83/150\n",
      "756/756 [==============================] - 0s - loss: 0.0563 - val_loss: 0.0930\n",
      "Epoch 84/150\n",
      "756/756 [==============================] - 0s - loss: 0.0565 - val_loss: 0.0914\n",
      "Epoch 85/150\n",
      "756/756 [==============================] - 0s - loss: 0.0564 - val_loss: 0.0899\n",
      "Epoch 86/150\n",
      "756/756 [==============================] - 0s - loss: 0.0560 - val_loss: 0.0892\n",
      "Epoch 87/150\n",
      "756/756 [==============================] - 0s - loss: 0.0558 - val_loss: 0.0909\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 0s - loss: 0.0556 - val_loss: 0.0921\n",
      "Epoch 89/150\n",
      "756/756 [==============================] - 0s - loss: 0.0557 - val_loss: 0.0924\n",
      "Epoch 90/150\n",
      "756/756 [==============================] - 0s - loss: 0.0556 - val_loss: 0.0909\n",
      "Epoch 91/150\n",
      "756/756 [==============================] - 0s - loss: 0.0556 - val_loss: 0.0902\n",
      "Epoch 92/150\n",
      "756/756 [==============================] - ETA: 0s - loss: 0.052 - 0s - loss: 0.0552 - val_loss: 0.0894\n",
      "Epoch 93/150\n",
      "756/756 [==============================] - 0s - loss: 0.0550 - val_loss: 0.0915\n",
      "Epoch 94/150\n",
      "756/756 [==============================] - 0s - loss: 0.0549 - val_loss: 0.0923\n",
      "Epoch 95/150\n",
      "756/756 [==============================] - 0s - loss: 0.0551 - val_loss: 0.0938\n",
      "Epoch 96/150\n",
      "756/756 [==============================] - 0s - loss: 0.0548 - val_loss: 0.0899\n",
      "Epoch 97/150\n",
      "756/756 [==============================] - 0s - loss: 0.0549 - val_loss: 0.0907\n",
      "Epoch 98/150\n",
      "756/756 [==============================] - 0s - loss: 0.0544 - val_loss: 0.0894\n",
      "Epoch 99/150\n",
      "756/756 [==============================] - 0s - loss: 0.0545 - val_loss: 0.0939\n",
      "Epoch 100/150\n",
      "756/756 [==============================] - 0s - loss: 0.0542 - val_loss: 0.0925\n",
      "Epoch 101/150\n",
      "756/756 [==============================] - 0s - loss: 0.0544 - val_loss: 0.0933\n",
      "Epoch 102/150\n",
      "756/756 [==============================] - 0s - loss: 0.0541 - val_loss: 0.0899\n",
      "Epoch 103/150\n",
      "756/756 [==============================] - 0s - loss: 0.0543 - val_loss: 0.0932\n",
      "Epoch 104/150\n",
      "756/756 [==============================] - 0s - loss: 0.0536 - val_loss: 0.0906\n",
      "Epoch 105/150\n",
      "756/756 [==============================] - 0s - loss: 0.0539 - val_loss: 0.0932\n",
      "Epoch 106/150\n",
      "756/756 [==============================] - 0s - loss: 0.0535 - val_loss: 0.0909\n",
      "Epoch 107/150\n",
      "756/756 [==============================] - 0s - loss: 0.0539 - val_loss: 0.0960\n",
      "Epoch 108/150\n",
      "756/756 [==============================] - 0s - loss: 0.0535 - val_loss: 0.0919\n",
      "Epoch 109/150\n",
      "756/756 [==============================] - 0s - loss: 0.0538 - val_loss: 0.0933\n",
      "Epoch 110/150\n",
      "756/756 [==============================] - 0s - loss: 0.0532 - val_loss: 0.0893\n",
      "Epoch 111/150\n",
      "756/756 [==============================] - 0s - loss: 0.0536 - val_loss: 0.0935\n",
      "Epoch 112/150\n",
      "756/756 [==============================] - 0s - loss: 0.0528 - val_loss: 0.0909\n",
      "Epoch 113/150\n",
      "756/756 [==============================] - 0s - loss: 0.0532 - val_loss: 0.0942\n",
      "Epoch 114/150\n",
      "756/756 [==============================] - 0s - loss: 0.0526 - val_loss: 0.0910\n",
      "Epoch 115/150\n",
      "756/756 [==============================] - 0s - loss: 0.0531 - val_loss: 0.0961\n",
      "Epoch 116/150\n",
      "756/756 [==============================] - 0s - loss: 0.0525 - val_loss: 0.0923\n",
      "Epoch 117/150\n",
      "756/756 [==============================] - 0s - loss: 0.0530 - val_loss: 0.0961\n",
      "Epoch 118/150\n",
      "756/756 [==============================] - 0s - loss: 0.0525 - val_loss: 0.0903\n",
      "Epoch 119/150\n",
      "756/756 [==============================] - 0s - loss: 0.0531 - val_loss: 0.0934\n",
      "Epoch 120/150\n",
      "756/756 [==============================] - 0s - loss: 0.0520 - val_loss: 0.0896\n",
      "Epoch 121/150\n",
      "756/756 [==============================] - 0s - loss: 0.0527 - val_loss: 0.0949\n",
      "Epoch 122/150\n",
      "756/756 [==============================] - 0s - loss: 0.0517 - val_loss: 0.0911\n",
      "Epoch 123/150\n",
      "756/756 [==============================] - 0s - loss: 0.0524 - val_loss: 0.0954\n",
      "Epoch 124/150\n",
      "756/756 [==============================] - 0s - loss: 0.0516 - val_loss: 0.0918\n",
      "Epoch 125/150\n",
      "756/756 [==============================] - 0s - loss: 0.0523 - val_loss: 0.0978\n",
      "Epoch 126/150\n",
      "756/756 [==============================] - 0s - loss: 0.0518 - val_loss: 0.0925\n",
      "Epoch 127/150\n",
      "756/756 [==============================] - 0s - loss: 0.0522 - val_loss: 0.0955\n",
      "Epoch 128/150\n",
      "756/756 [==============================] - 0s - loss: 0.0515 - val_loss: 0.0895\n",
      "Epoch 129/150\n",
      "756/756 [==============================] - 0s - loss: 0.0522 - val_loss: 0.0943\n",
      "Epoch 130/150\n",
      "756/756 [==============================] - 0s - loss: 0.0509 - val_loss: 0.0905\n",
      "Epoch 131/150\n",
      "756/756 [==============================] - 0s - loss: 0.0518 - val_loss: 0.0960\n",
      "Epoch 132/150\n",
      "756/756 [==============================] - 0s - loss: 0.0508 - val_loss: 0.0914\n",
      "Epoch 133/150\n",
      "756/756 [==============================] - 0s - loss: 0.0515 - val_loss: 0.0964\n",
      "Epoch 134/150\n",
      "756/756 [==============================] - 0s - loss: 0.0508 - val_loss: 0.0923\n",
      "Epoch 135/150\n",
      "756/756 [==============================] - 0s - loss: 0.0515 - val_loss: 0.0990\n",
      "Epoch 136/150\n",
      "756/756 [==============================] - 0s - loss: 0.0512 - val_loss: 0.0931\n",
      "Epoch 137/150\n",
      "756/756 [==============================] - 0s - loss: 0.0515 - val_loss: 0.0972\n",
      "Epoch 138/150\n",
      "756/756 [==============================] - 0s - loss: 0.0509 - val_loss: 0.0902\n",
      "Epoch 139/150\n",
      "756/756 [==============================] - 0s - loss: 0.0518 - val_loss: 0.0937\n",
      "Epoch 140/150\n",
      "756/756 [==============================] - 0s - loss: 0.0503 - val_loss: 0.0894\n",
      "Epoch 141/150\n",
      "756/756 [==============================] - 0s - loss: 0.0512 - val_loss: 0.0971\n",
      "Epoch 142/150\n",
      "756/756 [==============================] - 0s - loss: 0.0502 - val_loss: 0.0931\n",
      "Epoch 143/150\n",
      "756/756 [==============================] - 0s - loss: 0.0508 - val_loss: 0.0982\n",
      "Epoch 144/150\n",
      "756/756 [==============================] - 0s - loss: 0.0503 - val_loss: 0.0933\n",
      "Epoch 145/150\n",
      "756/756 [==============================] - 0s - loss: 0.0510 - val_loss: 0.0975\n",
      "Epoch 146/150\n",
      "756/756 [==============================] - 0s - loss: 0.0502 - val_loss: 0.0929\n",
      "Epoch 147/150\n",
      "756/756 [==============================] - 0s - loss: 0.0508 - val_loss: 0.0958\n",
      "Epoch 148/150\n",
      "756/756 [==============================] - 0s - loss: 0.0500 - val_loss: 0.0898\n",
      "Epoch 149/150\n",
      "756/756 [==============================] - 0s - loss: 0.0509 - val_loss: 0.0940\n",
      "Epoch 150/150\n",
      "756/756 [==============================] - 0s - loss: 0.0494 - val_loss: 0.0918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 4/22 [01:55<08:31, 28.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 756 samples, validate on 190 samples\n",
      "Epoch 1/150\n",
      "756/756 [==============================] - 6s - loss: 0.4051 - val_loss: 0.1881\n",
      "Epoch 2/150\n",
      "756/756 [==============================] - 0s - loss: 0.2715 - val_loss: 0.1619\n",
      "Epoch 3/150\n",
      "756/756 [==============================] - 0s - loss: 0.2151 - val_loss: 0.1715\n",
      "Epoch 4/150\n",
      "756/756 [==============================] - 0s - loss: 0.1894 - val_loss: 0.1653\n",
      "Epoch 5/150\n",
      "756/756 [==============================] - 0s - loss: 0.1735 - val_loss: 0.1465\n",
      "Epoch 6/150\n",
      "756/756 [==============================] - 0s - loss: 0.1613 - val_loss: 0.1295\n",
      "Epoch 7/150\n",
      "756/756 [==============================] - 0s - loss: 0.1505 - val_loss: 0.1197\n",
      "Epoch 8/150\n",
      "756/756 [==============================] - 0s - loss: 0.1406 - val_loss: 0.1151\n",
      "Epoch 9/150\n",
      "756/756 [==============================] - 0s - loss: 0.1322 - val_loss: 0.1118\n",
      "Epoch 10/150\n",
      "756/756 [==============================] - 0s - loss: 0.1251 - val_loss: 0.1081\n",
      "Epoch 11/150\n",
      "756/756 [==============================] - 0s - loss: 0.1186 - val_loss: 0.1056\n",
      "Epoch 12/150\n",
      "756/756 [==============================] - 0s - loss: 0.1124 - val_loss: 0.1032\n",
      "Epoch 13/150\n",
      "756/756 [==============================] - 0s - loss: 0.1066 - val_loss: 0.1005\n",
      "Epoch 14/150\n",
      "756/756 [==============================] - 0s - loss: 0.1013 - val_loss: 0.0984\n",
      "Epoch 15/150\n",
      "756/756 [==============================] - 0s - loss: 0.0967 - val_loss: 0.0969\n",
      "Epoch 16/150\n",
      "756/756 [==============================] - 0s - loss: 0.0930 - val_loss: 0.0957\n",
      "Epoch 17/150\n",
      "756/756 [==============================] - 0s - loss: 0.0900 - val_loss: 0.0940\n",
      "Epoch 18/150\n",
      "756/756 [==============================] - 0s - loss: 0.0872 - val_loss: 0.0925\n",
      "Epoch 19/150\n",
      "756/756 [==============================] - 0s - loss: 0.0849 - val_loss: 0.0906\n",
      "Epoch 20/150\n",
      "756/756 [==============================] - 0s - loss: 0.0827 - val_loss: 0.0888\n",
      "Epoch 21/150\n",
      "756/756 [==============================] - 0s - loss: 0.0807 - val_loss: 0.0871\n",
      "Epoch 22/150\n",
      "756/756 [==============================] - 0s - loss: 0.0789 - val_loss: 0.0854\n",
      "Epoch 23/150\n",
      "756/756 [==============================] - 0s - loss: 0.0772 - val_loss: 0.0839\n",
      "Epoch 24/150\n",
      "756/756 [==============================] - 0s - loss: 0.0756 - val_loss: 0.0822\n",
      "Epoch 25/150\n",
      "756/756 [==============================] - 0s - loss: 0.0742 - val_loss: 0.0810\n",
      "Epoch 26/150\n",
      "756/756 [==============================] - 0s - loss: 0.0729 - val_loss: 0.0797\n",
      "Epoch 27/150\n",
      "756/756 [==============================] - 0s - loss: 0.0717 - val_loss: 0.0782\n",
      "Epoch 28/150\n",
      "756/756 [==============================] - 0s - loss: 0.0707 - val_loss: 0.0771\n",
      "Epoch 29/150\n",
      "756/756 [==============================] - 0s - loss: 0.0697 - val_loss: 0.0759\n",
      "Epoch 30/150\n",
      "756/756 [==============================] - 0s - loss: 0.0687 - val_loss: 0.0748\n",
      "Epoch 31/150\n",
      "756/756 [==============================] - 0s - loss: 0.0678 - val_loss: 0.0741\n",
      "Epoch 32/150\n",
      "756/756 [==============================] - 0s - loss: 0.0671 - val_loss: 0.0730\n",
      "Epoch 33/150\n",
      "756/756 [==============================] - 0s - loss: 0.0663 - val_loss: 0.0720\n",
      "Epoch 34/150\n",
      "756/756 [==============================] - 0s - loss: 0.0655 - val_loss: 0.0715\n",
      "Epoch 35/150\n",
      "756/756 [==============================] - 0s - loss: 0.0649 - val_loss: 0.0709\n",
      "Epoch 36/150\n",
      "756/756 [==============================] - 0s - loss: 0.0644 - val_loss: 0.0695\n",
      "Epoch 37/150\n",
      "756/756 [==============================] - 0s - loss: 0.0635 - val_loss: 0.0693\n",
      "Epoch 38/150\n",
      "756/756 [==============================] - 0s - loss: 0.0630 - val_loss: 0.0694\n",
      "Epoch 39/150\n",
      "756/756 [==============================] - 0s - loss: 0.0629 - val_loss: 0.0678\n",
      "Epoch 40/150\n",
      "756/756 [==============================] - 0s - loss: 0.0621 - val_loss: 0.0668\n",
      "Epoch 41/150\n",
      "756/756 [==============================] - 0s - loss: 0.0612 - val_loss: 0.0674\n",
      "Epoch 42/150\n",
      "756/756 [==============================] - 0s - loss: 0.0611 - val_loss: 0.0671\n",
      "Epoch 43/150\n",
      "756/756 [==============================] - 0s - loss: 0.0610 - val_loss: 0.0656\n",
      "Epoch 44/150\n",
      "756/756 [==============================] - 0s - loss: 0.0602 - val_loss: 0.0648\n",
      "Epoch 45/150\n",
      "756/756 [==============================] - 0s - loss: 0.0592 - val_loss: 0.0657\n",
      "Epoch 46/150\n",
      "756/756 [==============================] - 0s - loss: 0.0593 - val_loss: 0.0655\n",
      "Epoch 47/150\n",
      "756/756 [==============================] - 0s - loss: 0.0593 - val_loss: 0.0645\n",
      "Epoch 48/150\n",
      "756/756 [==============================] - 0s - loss: 0.0587 - val_loss: 0.0636\n",
      "Epoch 49/150\n",
      "756/756 [==============================] - 0s - loss: 0.0579 - val_loss: 0.0634\n",
      "Epoch 50/150\n",
      "756/756 [==============================] - 0s - loss: 0.0574 - val_loss: 0.0640\n",
      "Epoch 51/150\n",
      "756/756 [==============================] - 0s - loss: 0.0575 - val_loss: 0.0641\n",
      "Epoch 52/150\n",
      "756/756 [==============================] - 0s - loss: 0.0575 - val_loss: 0.0631\n",
      "Epoch 53/150\n",
      "756/756 [==============================] - 0s - loss: 0.0570 - val_loss: 0.0621\n",
      "Epoch 54/150\n",
      "756/756 [==============================] - 0s - loss: 0.0563 - val_loss: 0.0622\n",
      "Epoch 55/150\n",
      "756/756 [==============================] - 0s - loss: 0.0559 - val_loss: 0.0626\n",
      "Epoch 56/150\n",
      "756/756 [==============================] - 0s - loss: 0.0558 - val_loss: 0.0626\n",
      "Epoch 57/150\n",
      "756/756 [==============================] - 0s - loss: 0.0558 - val_loss: 0.0617\n",
      "Epoch 58/150\n",
      "756/756 [==============================] - 0s - loss: 0.0554 - val_loss: 0.0612\n",
      "Epoch 59/150\n",
      "756/756 [==============================] - 0s - loss: 0.0548 - val_loss: 0.0617\n",
      "Epoch 60/150\n",
      "756/756 [==============================] - 0s - loss: 0.0545 - val_loss: 0.0622\n",
      "Epoch 61/150\n",
      "756/756 [==============================] - 0s - loss: 0.0546 - val_loss: 0.0621\n",
      "Epoch 62/150\n",
      "756/756 [==============================] - 0s - loss: 0.0549 - val_loss: 0.0610\n",
      "Epoch 63/150\n",
      "756/756 [==============================] - 0s - loss: 0.0546 - val_loss: 0.0597\n",
      "Epoch 64/150\n",
      "756/756 [==============================] - 0s - loss: 0.0536 - val_loss: 0.0602\n",
      "Epoch 65/150\n",
      "756/756 [==============================] - 0s - loss: 0.0531 - val_loss: 0.0610\n",
      "Epoch 66/150\n",
      "756/756 [==============================] - 0s - loss: 0.0533 - val_loss: 0.0616\n",
      "Epoch 67/150\n",
      "756/756 [==============================] - 0s - loss: 0.0534 - val_loss: 0.0615\n",
      "Epoch 68/150\n",
      "756/756 [==============================] - 0s - loss: 0.0535 - val_loss: 0.0604\n",
      "Epoch 69/150\n",
      "756/756 [==============================] - 0s - loss: 0.0535 - val_loss: 0.0590\n",
      "Epoch 70/150\n",
      "756/756 [==============================] - 0s - loss: 0.0526 - val_loss: 0.0587\n",
      "Epoch 71/150\n",
      "756/756 [==============================] - 0s - loss: 0.0516 - val_loss: 0.0603\n",
      "Epoch 72/150\n",
      "756/756 [==============================] - 0s - loss: 0.0519 - val_loss: 0.0611\n",
      "Epoch 73/150\n",
      "756/756 [==============================] - 0s - loss: 0.0522 - val_loss: 0.0612\n",
      "Epoch 74/150\n",
      "756/756 [==============================] - 0s - loss: 0.0525 - val_loss: 0.0598\n",
      "Epoch 75/150\n",
      "756/756 [==============================] - 0s - loss: 0.0526 - val_loss: 0.0582\n",
      "Epoch 76/150\n",
      "756/756 [==============================] - 0s - loss: 0.0518 - val_loss: 0.0577\n",
      "Epoch 77/150\n",
      "756/756 [==============================] - 0s - loss: 0.0508 - val_loss: 0.0588\n",
      "Epoch 78/150\n",
      "756/756 [==============================] - 0s - loss: 0.0506 - val_loss: 0.0600\n",
      "Epoch 79/150\n",
      "756/756 [==============================] - 0s - loss: 0.0508 - val_loss: 0.0606\n",
      "Epoch 80/150\n",
      "756/756 [==============================] - 0s - loss: 0.0511 - val_loss: 0.0605\n",
      "Epoch 81/150\n",
      "756/756 [==============================] - 0s - loss: 0.0516 - val_loss: 0.0591\n",
      "Epoch 82/150\n",
      "756/756 [==============================] - 0s - loss: 0.0519 - val_loss: 0.0572\n",
      "Epoch 83/150\n",
      "756/756 [==============================] - 0s - loss: 0.0509 - val_loss: 0.0568\n",
      "Epoch 84/150\n",
      "756/756 [==============================] - 0s - loss: 0.0498 - val_loss: 0.0581\n",
      "Epoch 85/150\n",
      "756/756 [==============================] - 0s - loss: 0.0496 - val_loss: 0.0597\n",
      "Epoch 86/150\n",
      "756/756 [==============================] - 0s - loss: 0.0499 - val_loss: 0.0605\n",
      "Epoch 87/150\n",
      "756/756 [==============================] - 0s - loss: 0.0503 - val_loss: 0.0599\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 0s - loss: 0.0508 - val_loss: 0.0582\n",
      "Epoch 89/150\n",
      "756/756 [==============================] - 0s - loss: 0.0506 - val_loss: 0.0568\n",
      "Epoch 90/150\n",
      "756/756 [==============================] - 0s - loss: 0.0496 - val_loss: 0.0572\n",
      "Epoch 91/150\n",
      "756/756 [==============================] - 0s - loss: 0.0491 - val_loss: 0.0580\n",
      "Epoch 92/150\n",
      "756/756 [==============================] - 0s - loss: 0.0492 - val_loss: 0.0587\n",
      "Epoch 93/150\n",
      "756/756 [==============================] - 0s - loss: 0.0490 - val_loss: 0.0595\n",
      "Epoch 94/150\n",
      "756/756 [==============================] - 0s - loss: 0.0493 - val_loss: 0.0591\n",
      "Epoch 95/150\n",
      "756/756 [==============================] - 0s - loss: 0.0496 - val_loss: 0.0582\n",
      "Epoch 96/150\n",
      "756/756 [==============================] - 0s - loss: 0.0497 - val_loss: 0.0572\n",
      "Epoch 97/150\n",
      "756/756 [==============================] - 0s - loss: 0.0495 - val_loss: 0.0565\n",
      "Epoch 98/150\n",
      "756/756 [==============================] - 0s - loss: 0.0489 - val_loss: 0.0566\n",
      "Epoch 99/150\n",
      "756/756 [==============================] - 0s - loss: 0.0482 - val_loss: 0.0577\n",
      "Epoch 100/150\n",
      "756/756 [==============================] - 0s - loss: 0.0479 - val_loss: 0.0589\n",
      "Epoch 101/150\n",
      "756/756 [==============================] - 0s - loss: 0.0481 - val_loss: 0.0600\n",
      "Epoch 102/150\n",
      "756/756 [==============================] - 0s - loss: 0.0487 - val_loss: 0.0596\n",
      "Epoch 103/150\n",
      "756/756 [==============================] - 0s - loss: 0.0494 - val_loss: 0.0584\n",
      "Epoch 104/150\n",
      "756/756 [==============================] - 0s - loss: 0.0499 - val_loss: 0.0561\n",
      "Epoch 105/150\n",
      "756/756 [==============================] - 0s - loss: 0.0493 - val_loss: 0.0549\n",
      "Epoch 106/150\n",
      "756/756 [==============================] - 0s - loss: 0.0479 - val_loss: 0.0556\n",
      "Epoch 107/150\n",
      "756/756 [==============================] - 0s - loss: 0.0469 - val_loss: 0.0572\n",
      "Epoch 108/150\n",
      "756/756 [==============================] - 0s - loss: 0.0469 - val_loss: 0.0596\n",
      "Epoch 109/150\n",
      "756/756 [==============================] - 0s - loss: 0.0477 - val_loss: 0.0610\n",
      "Epoch 110/150\n",
      "756/756 [==============================] - 0s - loss: 0.0485 - val_loss: 0.0600\n",
      "Epoch 111/150\n",
      "756/756 [==============================] - 0s - loss: 0.0494 - val_loss: 0.0572\n",
      "Epoch 112/150\n",
      "756/756 [==============================] - 0s - loss: 0.0499 - val_loss: 0.0542\n",
      "Epoch 113/150\n",
      "756/756 [==============================] - 0s - loss: 0.0483 - val_loss: 0.0541\n",
      "Epoch 114/150\n",
      "756/756 [==============================] - 0s - loss: 0.0465 - val_loss: 0.0563\n",
      "Epoch 115/150\n",
      "756/756 [==============================] - 0s - loss: 0.0463 - val_loss: 0.0590\n",
      "Epoch 116/150\n",
      "756/756 [==============================] - 0s - loss: 0.0471 - val_loss: 0.0607\n",
      "Epoch 117/150\n",
      "756/756 [==============================] - 0s - loss: 0.0479 - val_loss: 0.0601\n",
      "Epoch 118/150\n",
      "756/756 [==============================] - 0s - loss: 0.0489 - val_loss: 0.0569\n",
      "Epoch 119/150\n",
      "756/756 [==============================] - 0s - loss: 0.0492 - val_loss: 0.0542\n",
      "Epoch 120/150\n",
      "756/756 [==============================] - 0s - loss: 0.0478 - val_loss: 0.0540\n",
      "Epoch 121/150\n",
      "756/756 [==============================] - 0s - loss: 0.0462 - val_loss: 0.0557\n",
      "Epoch 122/150\n",
      "756/756 [==============================] - 0s - loss: 0.0457 - val_loss: 0.0582\n",
      "Epoch 123/150\n",
      "756/756 [==============================] - 0s - loss: 0.0463 - val_loss: 0.0600\n",
      "Epoch 124/150\n",
      "756/756 [==============================] - 0s - loss: 0.0471 - val_loss: 0.0599\n",
      "Epoch 125/150\n",
      "756/756 [==============================] - 0s - loss: 0.0480 - val_loss: 0.0580\n",
      "Epoch 126/150\n",
      "756/756 [==============================] - 0s - loss: 0.0486 - val_loss: 0.0550\n",
      "Epoch 127/150\n",
      "756/756 [==============================] - 0s - loss: 0.0479 - val_loss: 0.0536\n",
      "Epoch 128/150\n",
      "756/756 [==============================] - 0s - loss: 0.0461 - val_loss: 0.0549\n",
      "Epoch 129/150\n",
      "756/756 [==============================] - 0s - loss: 0.0453 - val_loss: 0.0576\n",
      "Epoch 130/150\n",
      "756/756 [==============================] - 0s - loss: 0.0458 - val_loss: 0.0597\n",
      "Epoch 131/150\n",
      "756/756 [==============================] - 0s - loss: 0.0465 - val_loss: 0.0597\n",
      "Epoch 132/150\n",
      "756/756 [==============================] - 0s - loss: 0.0474 - val_loss: 0.0575\n",
      "Epoch 133/150\n",
      "756/756 [==============================] - 0s - loss: 0.0477 - val_loss: 0.0554\n",
      "Epoch 134/150\n",
      "756/756 [==============================] - 0s - loss: 0.0473 - val_loss: 0.0541\n",
      "Epoch 135/150\n",
      "756/756 [==============================] - 0s - loss: 0.0461 - val_loss: 0.0548\n",
      "Epoch 136/150\n",
      "756/756 [==============================] - 0s - loss: 0.0452 - val_loss: 0.0559\n",
      "Epoch 137/150\n",
      "756/756 [==============================] - 0s - loss: 0.0449 - val_loss: 0.0584\n",
      "Epoch 138/150\n",
      "756/756 [==============================] - 0s - loss: 0.0454 - val_loss: 0.0598\n",
      "Epoch 139/150\n",
      "756/756 [==============================] - 0s - loss: 0.0463 - val_loss: 0.0595\n",
      "Epoch 140/150\n",
      "756/756 [==============================] - 0s - loss: 0.0472 - val_loss: 0.0569\n",
      "Epoch 141/150\n",
      "756/756 [==============================] - 0s - loss: 0.0477 - val_loss: 0.0543\n",
      "Epoch 142/150\n",
      "756/756 [==============================] - 0s - loss: 0.0469 - val_loss: 0.0536\n",
      "Epoch 143/150\n",
      "756/756 [==============================] - 0s - loss: 0.0452 - val_loss: 0.0547\n",
      "Epoch 144/150\n",
      "756/756 [==============================] - 0s - loss: 0.0443 - val_loss: 0.0572\n",
      "Epoch 145/150\n",
      "756/756 [==============================] - 0s - loss: 0.0447 - val_loss: 0.0597\n",
      "Epoch 146/150\n",
      "756/756 [==============================] - 0s - loss: 0.0455 - val_loss: 0.0603\n",
      "Epoch 147/150\n",
      "756/756 [==============================] - 0s - loss: 0.0464 - val_loss: 0.0590\n",
      "Epoch 148/150\n",
      "756/756 [==============================] - 0s - loss: 0.0475 - val_loss: 0.0558\n",
      "Epoch 149/150\n",
      "756/756 [==============================] - 0s - loss: 0.0475 - val_loss: 0.0531\n",
      "Epoch 150/150\n",
      "756/756 [==============================] - 0s - loss: 0.0458 - val_loss: 0.0535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 5/22 [02:18<07:36, 26.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 756 samples, validate on 190 samples\n",
      "Epoch 1/150\n",
      "756/756 [==============================] - 5s - loss: 0.3430 - val_loss: 0.2011\n",
      "Epoch 2/150\n",
      "756/756 [==============================] - 0s - loss: 0.2503 - val_loss: 0.1932\n",
      "Epoch 3/150\n",
      "756/756 [==============================] - 0s - loss: 0.2016 - val_loss: 0.1860\n",
      "Epoch 4/150\n",
      "756/756 [==============================] - 0s - loss: 0.1712 - val_loss: 0.1869\n",
      "Epoch 5/150\n",
      "756/756 [==============================] - 0s - loss: 0.1573 - val_loss: 0.1867\n",
      "Epoch 6/150\n",
      "756/756 [==============================] - 0s - loss: 0.1511 - val_loss: 0.1818\n",
      "Epoch 7/150\n",
      "756/756 [==============================] - 0s - loss: 0.1465 - val_loss: 0.1728\n",
      "Epoch 8/150\n",
      "756/756 [==============================] - 0s - loss: 0.1399 - val_loss: 0.1650\n",
      "Epoch 9/150\n",
      "756/756 [==============================] - 0s - loss: 0.1329 - val_loss: 0.1591\n",
      "Epoch 10/150\n",
      "756/756 [==============================] - 0s - loss: 0.1268 - val_loss: 0.1529\n",
      "Epoch 11/150\n",
      "756/756 [==============================] - 0s - loss: 0.1207 - val_loss: 0.1466\n",
      "Epoch 12/150\n",
      "756/756 [==============================] - 0s - loss: 0.1151 - val_loss: 0.1404\n",
      "Epoch 13/150\n",
      "756/756 [==============================] - 0s - loss: 0.1100 - val_loss: 0.1344\n",
      "Epoch 14/150\n",
      "756/756 [==============================] - 0s - loss: 0.1054 - val_loss: 0.1287\n",
      "Epoch 15/150\n",
      "756/756 [==============================] - 0s - loss: 0.1014 - val_loss: 0.1235\n",
      "Epoch 16/150\n",
      "756/756 [==============================] - 0s - loss: 0.0978 - val_loss: 0.1189\n",
      "Epoch 17/150\n",
      "756/756 [==============================] - 0s - loss: 0.0946 - val_loss: 0.1152\n",
      "Epoch 18/150\n",
      "756/756 [==============================] - 0s - loss: 0.0921 - val_loss: 0.1122\n",
      "Epoch 19/150\n",
      "756/756 [==============================] - 0s - loss: 0.0898 - val_loss: 0.1098\n",
      "Epoch 20/150\n",
      "756/756 [==============================] - 0s - loss: 0.0879 - val_loss: 0.1079\n",
      "Epoch 21/150\n",
      "756/756 [==============================] - 0s - loss: 0.0863 - val_loss: 0.1065\n",
      "Epoch 22/150\n",
      "756/756 [==============================] - 0s - loss: 0.0849 - val_loss: 0.1055\n",
      "Epoch 23/150\n",
      "756/756 [==============================] - 0s - loss: 0.0837 - val_loss: 0.1045\n",
      "Epoch 24/150\n",
      "756/756 [==============================] - 0s - loss: 0.0828 - val_loss: 0.1037\n",
      "Epoch 25/150\n",
      "756/756 [==============================] - 0s - loss: 0.0818 - val_loss: 0.1033\n",
      "Epoch 26/150\n",
      "756/756 [==============================] - 0s - loss: 0.0809 - val_loss: 0.1028\n",
      "Epoch 27/150\n",
      "756/756 [==============================] - 0s - loss: 0.0803 - val_loss: 0.1023\n",
      "Epoch 28/150\n",
      "756/756 [==============================] - 0s - loss: 0.0795 - val_loss: 0.1021\n",
      "Epoch 29/150\n",
      "756/756 [==============================] - 0s - loss: 0.0789 - val_loss: 0.1018\n",
      "Epoch 30/150\n",
      "756/756 [==============================] - 0s - loss: 0.0787 - val_loss: 0.1016\n",
      "Epoch 31/150\n",
      "756/756 [==============================] - 0s - loss: 0.0779 - val_loss: 0.1015\n",
      "Epoch 32/150\n",
      "756/756 [==============================] - 0s - loss: 0.0772 - val_loss: 0.1014\n",
      "Epoch 33/150\n",
      "756/756 [==============================] - 0s - loss: 0.0772 - val_loss: 0.1011\n",
      "Epoch 34/150\n",
      "756/756 [==============================] - 0s - loss: 0.0767 - val_loss: 0.1011\n",
      "Epoch 35/150\n",
      "756/756 [==============================] - 0s - loss: 0.0760 - val_loss: 0.1012\n",
      "Epoch 36/150\n",
      "756/756 [==============================] - 0s - loss: 0.0756 - val_loss: 0.1011\n",
      "Epoch 37/150\n",
      "756/756 [==============================] - 0s - loss: 0.0756 - val_loss: 0.1009\n",
      "Epoch 38/150\n",
      "756/756 [==============================] - 0s - loss: 0.0753 - val_loss: 0.1009\n",
      "Epoch 39/150\n",
      "756/756 [==============================] - 0s - loss: 0.0745 - val_loss: 0.1009\n",
      "Epoch 40/150\n",
      "756/756 [==============================] - 0s - loss: 0.0741 - val_loss: 0.1009\n",
      "Epoch 41/150\n",
      "756/756 [==============================] - 0s - loss: 0.0745 - val_loss: 0.1006\n",
      "Epoch 42/150\n",
      "756/756 [==============================] - 0s - loss: 0.0741 - val_loss: 0.1006\n",
      "Epoch 43/150\n",
      "756/756 [==============================] - 0s - loss: 0.0732 - val_loss: 0.1007\n",
      "Epoch 44/150\n",
      "756/756 [==============================] - 0s - loss: 0.0730 - val_loss: 0.1007\n",
      "Epoch 45/150\n",
      "756/756 [==============================] - 0s - loss: 0.0736 - val_loss: 0.1004\n",
      "Epoch 46/150\n",
      "756/756 [==============================] - 0s - loss: 0.0729 - val_loss: 0.1004\n",
      "Epoch 47/150\n",
      "756/756 [==============================] - 0s - loss: 0.0720 - val_loss: 0.1006\n",
      "Epoch 48/150\n",
      "756/756 [==============================] - 0s - loss: 0.0722 - val_loss: 0.1005\n",
      "Epoch 49/150\n",
      "756/756 [==============================] - 0s - loss: 0.0725 - val_loss: 0.1003\n",
      "Epoch 50/150\n",
      "756/756 [==============================] - 0s - loss: 0.0720 - val_loss: 0.1003\n",
      "Epoch 51/150\n",
      "756/756 [==============================] - 0s - loss: 0.0712 - val_loss: 0.1005\n",
      "Epoch 52/150\n",
      "756/756 [==============================] - 0s - loss: 0.0713 - val_loss: 0.1004\n",
      "Epoch 53/150\n",
      "756/756 [==============================] - 0s - loss: 0.0713 - val_loss: 0.1003\n",
      "Epoch 54/150\n",
      "756/756 [==============================] - 0s - loss: 0.0710 - val_loss: 0.1003\n",
      "Epoch 55/150\n",
      "756/756 [==============================] - 0s - loss: 0.0705 - val_loss: 0.1003\n",
      "Epoch 56/150\n",
      "756/756 [==============================] - 0s - loss: 0.0703 - val_loss: 0.1003\n",
      "Epoch 57/150\n",
      "756/756 [==============================] - 0s - loss: 0.0702 - val_loss: 0.1003\n",
      "Epoch 58/150\n",
      "756/756 [==============================] - 0s - loss: 0.0703 - val_loss: 0.1000\n",
      "Epoch 59/150\n",
      "756/756 [==============================] - 0s - loss: 0.0697 - val_loss: 0.1002\n",
      "Epoch 60/150\n",
      "756/756 [==============================] - 0s - loss: 0.0692 - val_loss: 0.1004\n",
      "Epoch 61/150\n",
      "756/756 [==============================] - 0s - loss: 0.0696 - val_loss: 0.1001\n",
      "Epoch 62/150\n",
      "756/756 [==============================] - 0s - loss: 0.0696 - val_loss: 0.0998\n",
      "Epoch 63/150\n",
      "756/756 [==============================] - 0s - loss: 0.0688 - val_loss: 0.1002\n",
      "Epoch 64/150\n",
      "756/756 [==============================] - 0s - loss: 0.0682 - val_loss: 0.1005\n",
      "Epoch 65/150\n",
      "756/756 [==============================] - 0s - loss: 0.0685 - val_loss: 0.1002\n",
      "Epoch 66/150\n",
      "756/756 [==============================] - 0s - loss: 0.0692 - val_loss: 0.0995\n",
      "Epoch 67/150\n",
      "756/756 [==============================] - 0s - loss: 0.0686 - val_loss: 0.0999\n",
      "Epoch 68/150\n",
      "756/756 [==============================] - 0s - loss: 0.0672 - val_loss: 0.1006\n",
      "Epoch 69/150\n",
      "756/756 [==============================] - 0s - loss: 0.0673 - val_loss: 0.1006\n",
      "Epoch 70/150\n",
      "756/756 [==============================] - 0s - loss: 0.0690 - val_loss: 0.0992\n",
      "Epoch 71/150\n",
      "756/756 [==============================] - 0s - loss: 0.0683 - val_loss: 0.0995\n",
      "Epoch 72/150\n",
      "756/756 [==============================] - 0s - loss: 0.0660 - val_loss: 0.1012\n",
      "Epoch 73/150\n",
      "756/756 [==============================] - 0s - loss: 0.0669 - val_loss: 0.1004\n",
      "Epoch 74/150\n",
      "756/756 [==============================] - 0s - loss: 0.0689 - val_loss: 0.0990\n",
      "Epoch 75/150\n",
      "756/756 [==============================] - 0s - loss: 0.0670 - val_loss: 0.0999\n",
      "Epoch 76/150\n",
      "756/756 [==============================] - 0s - loss: 0.0652 - val_loss: 0.1013\n",
      "Epoch 77/150\n",
      "756/756 [==============================] - 0s - loss: 0.0666 - val_loss: 0.1000\n",
      "Epoch 78/150\n",
      "756/756 [==============================] - 0s - loss: 0.0681 - val_loss: 0.0989\n",
      "Epoch 79/150\n",
      "756/756 [==============================] - 0s - loss: 0.0659 - val_loss: 0.1002\n",
      "Epoch 80/150\n",
      "756/756 [==============================] - 0s - loss: 0.0645 - val_loss: 0.1013\n",
      "Epoch 81/150\n",
      "756/756 [==============================] - 0s - loss: 0.0663 - val_loss: 0.0996\n",
      "Epoch 82/150\n",
      "756/756 [==============================] - 0s - loss: 0.0669 - val_loss: 0.0991\n",
      "Epoch 83/150\n",
      "756/756 [==============================] - 0s - loss: 0.0645 - val_loss: 0.1007\n",
      "Epoch 84/150\n",
      "756/756 [==============================] - 0s - loss: 0.0641 - val_loss: 0.1008\n",
      "Epoch 85/150\n",
      "756/756 [==============================] - 0s - loss: 0.0663 - val_loss: 0.0990\n",
      "Epoch 86/150\n",
      "756/756 [==============================] - 0s - loss: 0.0659 - val_loss: 0.0994\n",
      "Epoch 87/150\n",
      "756/756 [==============================] - 0s - loss: 0.0630 - val_loss: 0.1015\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 0s - loss: 0.0642 - val_loss: 0.1003\n",
      "Epoch 89/150\n",
      "756/756 [==============================] - 0s - loss: 0.0667 - val_loss: 0.0985\n",
      "Epoch 90/150\n",
      "756/756 [==============================] - 0s - loss: 0.0641 - val_loss: 0.1003\n",
      "Epoch 91/150\n",
      "756/756 [==============================] - 0s - loss: 0.0624 - val_loss: 0.1017\n",
      "Epoch 92/150\n",
      "756/756 [==============================] - 0s - loss: 0.0646 - val_loss: 0.0994\n",
      "Epoch 93/150\n",
      "756/756 [==============================] - 0s - loss: 0.0662 - val_loss: 0.0988\n",
      "Epoch 94/150\n",
      "756/756 [==============================] - 0s - loss: 0.0622 - val_loss: 0.1015\n",
      "Epoch 95/150\n",
      "756/756 [==============================] - 0s - loss: 0.0626 - val_loss: 0.1009\n",
      "Epoch 96/150\n",
      "756/756 [==============================] - 0s - loss: 0.0656 - val_loss: 0.0984\n",
      "Epoch 97/150\n",
      "756/756 [==============================] - 0s - loss: 0.0636 - val_loss: 0.1003\n",
      "Epoch 98/150\n",
      "756/756 [==============================] - 0s - loss: 0.0612 - val_loss: 0.1023\n",
      "Epoch 99/150\n",
      "756/756 [==============================] - 0s - loss: 0.0634 - val_loss: 0.0996\n",
      "Epoch 100/150\n",
      "756/756 [==============================] - 0s - loss: 0.0653 - val_loss: 0.0986\n",
      "Epoch 101/150\n",
      "756/756 [==============================] - 0s - loss: 0.0613 - val_loss: 0.1019\n",
      "Epoch 102/150\n",
      "756/756 [==============================] - 0s - loss: 0.0616 - val_loss: 0.1011\n",
      "Epoch 103/150\n",
      "756/756 [==============================] - 0s - loss: 0.0645 - val_loss: 0.0986\n",
      "Epoch 104/150\n",
      "756/756 [==============================] - 0s - loss: 0.0626 - val_loss: 0.1007\n",
      "Epoch 105/150\n",
      "756/756 [==============================] - 0s - loss: 0.0603 - val_loss: 0.1024\n",
      "Epoch 106/150\n",
      "756/756 [==============================] - 0s - loss: 0.0623 - val_loss: 0.0997\n",
      "Epoch 107/150\n",
      "756/756 [==============================] - 0s - loss: 0.0645 - val_loss: 0.0990\n",
      "Epoch 108/150\n",
      "756/756 [==============================] - 0s - loss: 0.0601 - val_loss: 0.1025\n",
      "Epoch 109/150\n",
      "756/756 [==============================] - 0s - loss: 0.0609 - val_loss: 0.1014\n",
      "Epoch 110/150\n",
      "756/756 [==============================] - 0s - loss: 0.0641 - val_loss: 0.0985\n",
      "Epoch 111/150\n",
      "756/756 [==============================] - 0s - loss: 0.0615 - val_loss: 0.1013\n",
      "Epoch 112/150\n",
      "756/756 [==============================] - 0s - loss: 0.0595 - val_loss: 0.1030\n",
      "Epoch 113/150\n",
      "756/756 [==============================] - 0s - loss: 0.0620 - val_loss: 0.0994\n",
      "Epoch 114/150\n",
      "756/756 [==============================] - 0s - loss: 0.0637 - val_loss: 0.0994\n",
      "Epoch 115/150\n",
      "756/756 [==============================] - 0s - loss: 0.0589 - val_loss: 0.1038\n",
      "Epoch 116/150\n",
      "756/756 [==============================] - 0s - loss: 0.0606 - val_loss: 0.1010\n",
      "Epoch 117/150\n",
      "756/756 [==============================] - 0s - loss: 0.0645 - val_loss: 0.0984\n",
      "Epoch 118/150\n",
      "756/756 [==============================] - 0s - loss: 0.0595 - val_loss: 0.1039\n",
      "Epoch 119/150\n",
      "756/756 [==============================] - 0s - loss: 0.0600 - val_loss: 0.1022\n",
      "Epoch 120/150\n",
      "756/756 [==============================] - 0s - loss: 0.0631 - val_loss: 0.0983\n",
      "Epoch 121/150\n",
      "756/756 [==============================] - 0s - loss: 0.0608 - val_loss: 0.1021\n",
      "Epoch 122/150\n",
      "756/756 [==============================] - 0s - loss: 0.0586 - val_loss: 0.1043\n",
      "Epoch 123/150\n",
      "756/756 [==============================] - 0s - loss: 0.0613 - val_loss: 0.0990\n",
      "Epoch 124/150\n",
      "756/756 [==============================] - 0s - loss: 0.0628 - val_loss: 0.0996\n",
      "Epoch 125/150\n",
      "756/756 [==============================] - 0s - loss: 0.0579 - val_loss: 0.1051\n",
      "Epoch 126/150\n",
      "756/756 [==============================] - 0s - loss: 0.0602 - val_loss: 0.1007\n",
      "Epoch 127/150\n",
      "756/756 [==============================] - 0s - loss: 0.0636 - val_loss: 0.0984\n",
      "Epoch 128/150\n",
      "756/756 [==============================] - 0s - loss: 0.0583 - val_loss: 0.1053\n",
      "Epoch 129/150\n",
      "756/756 [==============================] - 0s - loss: 0.0596 - val_loss: 0.1022\n",
      "Epoch 130/150\n",
      "756/756 [==============================] - 0s - loss: 0.0622 - val_loss: 0.0980\n",
      "Epoch 131/150\n",
      "756/756 [==============================] - 0s - loss: 0.0595 - val_loss: 0.1029\n",
      "Epoch 132/150\n",
      "756/756 [==============================] - 0s - loss: 0.0579 - val_loss: 0.1043\n",
      "Epoch 133/150\n",
      "756/756 [==============================] - 0s - loss: 0.0605 - val_loss: 0.0989\n",
      "Epoch 134/150\n",
      "756/756 [==============================] - 0s - loss: 0.0613 - val_loss: 0.1003\n",
      "Epoch 135/150\n",
      "756/756 [==============================] - 0s - loss: 0.0570 - val_loss: 0.1054\n",
      "Epoch 136/150\n",
      "756/756 [==============================] - 0s - loss: 0.0592 - val_loss: 0.1009\n",
      "Epoch 137/150\n",
      "756/756 [==============================] - 0s - loss: 0.0622 - val_loss: 0.0991\n",
      "Epoch 138/150\n",
      "756/756 [==============================] - 0s - loss: 0.0573 - val_loss: 0.1052\n",
      "Epoch 139/150\n",
      "756/756 [==============================] - 0s - loss: 0.0583 - val_loss: 0.1024\n",
      "Epoch 140/150\n",
      "756/756 [==============================] - 0s - loss: 0.0612 - val_loss: 0.0985\n",
      "Epoch 141/150\n",
      "756/756 [==============================] - 0s - loss: 0.0584 - val_loss: 0.1036\n",
      "Epoch 142/150\n",
      "756/756 [==============================] - 0s - loss: 0.0571 - val_loss: 0.1043\n",
      "Epoch 143/150\n",
      "756/756 [==============================] - 0s - loss: 0.0597 - val_loss: 0.0991\n",
      "Epoch 144/150\n",
      "756/756 [==============================] - 0s - loss: 0.0601 - val_loss: 0.1011\n",
      "Epoch 145/150\n",
      "756/756 [==============================] - 0s - loss: 0.0562 - val_loss: 0.1056\n",
      "Epoch 146/150\n",
      "756/756 [==============================] - 0s - loss: 0.0586 - val_loss: 0.1008\n",
      "Epoch 147/150\n",
      "756/756 [==============================] - 0s - loss: 0.0613 - val_loss: 0.0998\n",
      "Epoch 148/150\n",
      "756/756 [==============================] - 0s - loss: 0.0562 - val_loss: 0.1056\n",
      "Epoch 149/150\n",
      "756/756 [==============================] - 0s - loss: 0.0577 - val_loss: 0.1023\n",
      "Epoch 150/150\n",
      "756/756 [==============================] - 0s - loss: 0.0610 - val_loss: 0.0989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 6/22 [02:41<06:48, 25.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 756 samples, validate on 190 samples\n",
      "Epoch 1/150\n",
      "756/756 [==============================] - 5s - loss: 0.3016 - val_loss: 0.0961\n",
      "Epoch 2/150\n",
      "756/756 [==============================] - 0s - loss: 0.2401 - val_loss: 0.1001\n",
      "Epoch 3/150\n",
      "756/756 [==============================] - 0s - loss: 0.2070 - val_loss: 0.1183\n",
      "Epoch 4/150\n",
      "756/756 [==============================] - 0s - loss: 0.1865 - val_loss: 0.1297\n",
      "Epoch 5/150\n",
      "756/756 [==============================] - 0s - loss: 0.1703 - val_loss: 0.1280\n",
      "Epoch 6/150\n",
      "756/756 [==============================] - 0s - loss: 0.1547 - val_loss: 0.1197\n",
      "Epoch 7/150\n",
      "756/756 [==============================] - 0s - loss: 0.1398 - val_loss: 0.1120\n",
      "Epoch 8/150\n",
      "756/756 [==============================] - 0s - loss: 0.1272 - val_loss: 0.1060\n",
      "Epoch 9/150\n",
      "756/756 [==============================] - 0s - loss: 0.1175 - val_loss: 0.1017\n",
      "Epoch 10/150\n",
      "756/756 [==============================] - 0s - loss: 0.1099 - val_loss: 0.0976\n",
      "Epoch 11/150\n",
      "756/756 [==============================] - 0s - loss: 0.1035 - val_loss: 0.0926\n",
      "Epoch 12/150\n",
      "756/756 [==============================] - 0s - loss: 0.0982 - val_loss: 0.0880\n",
      "Epoch 13/150\n",
      "756/756 [==============================] - 0s - loss: 0.0937 - val_loss: 0.0847\n",
      "Epoch 14/150\n",
      "756/756 [==============================] - 0s - loss: 0.0904 - val_loss: 0.0822\n",
      "Epoch 15/150\n",
      "756/756 [==============================] - 0s - loss: 0.0877 - val_loss: 0.0807\n",
      "Epoch 16/150\n",
      "756/756 [==============================] - 0s - loss: 0.0852 - val_loss: 0.0797\n",
      "Epoch 17/150\n",
      "756/756 [==============================] - 0s - loss: 0.0832 - val_loss: 0.0788\n",
      "Epoch 18/150\n",
      "756/756 [==============================] - 0s - loss: 0.0817 - val_loss: 0.0781\n",
      "Epoch 19/150\n",
      "756/756 [==============================] - 0s - loss: 0.0801 - val_loss: 0.0772\n",
      "Epoch 20/150\n",
      "756/756 [==============================] - 0s - loss: 0.0788 - val_loss: 0.0767\n",
      "Epoch 21/150\n",
      "756/756 [==============================] - 0s - loss: 0.0776 - val_loss: 0.0762\n",
      "Epoch 22/150\n",
      "756/756 [==============================] - 0s - loss: 0.0768 - val_loss: 0.0758\n",
      "Epoch 23/150\n",
      "756/756 [==============================] - 0s - loss: 0.0758 - val_loss: 0.0755\n",
      "Epoch 24/150\n",
      "756/756 [==============================] - 0s - loss: 0.0747 - val_loss: 0.0749\n",
      "Epoch 25/150\n",
      "756/756 [==============================] - 0s - loss: 0.0743 - val_loss: 0.0744\n",
      "Epoch 26/150\n",
      "756/756 [==============================] - 0s - loss: 0.0736 - val_loss: 0.0744\n",
      "Epoch 27/150\n",
      "756/756 [==============================] - 0s - loss: 0.0725 - val_loss: 0.0740\n",
      "Epoch 28/150\n",
      "756/756 [==============================] - 0s - loss: 0.0721 - val_loss: 0.0736\n",
      "Epoch 29/150\n",
      "756/756 [==============================] - 0s - loss: 0.0722 - val_loss: 0.0737\n",
      "Epoch 30/150\n",
      "756/756 [==============================] - 0s - loss: 0.0709 - val_loss: 0.0734\n",
      "Epoch 31/150\n",
      "756/756 [==============================] - 0s - loss: 0.0703 - val_loss: 0.0731\n",
      "Epoch 32/150\n",
      "756/756 [==============================] - 0s - loss: 0.0704 - val_loss: 0.0727\n",
      "Epoch 33/150\n",
      "756/756 [==============================] - 0s - loss: 0.0700 - val_loss: 0.0731\n",
      "Epoch 34/150\n",
      "756/756 [==============================] - 0s - loss: 0.0688 - val_loss: 0.0726\n",
      "Epoch 35/150\n",
      "756/756 [==============================] - 0s - loss: 0.0685 - val_loss: 0.0721\n",
      "Epoch 36/150\n",
      "756/756 [==============================] - 0s - loss: 0.0689 - val_loss: 0.0722\n",
      "Epoch 37/150\n",
      "756/756 [==============================] - 0s - loss: 0.0679 - val_loss: 0.0724\n",
      "Epoch 38/150\n",
      "756/756 [==============================] - 0s - loss: 0.0670 - val_loss: 0.0718\n",
      "Epoch 39/150\n",
      "756/756 [==============================] - 0s - loss: 0.0673 - val_loss: 0.0711\n",
      "Epoch 40/150\n",
      "756/756 [==============================] - 0s - loss: 0.0678 - val_loss: 0.0720\n",
      "Epoch 41/150\n",
      "756/756 [==============================] - 0s - loss: 0.0662 - val_loss: 0.0718\n",
      "Epoch 42/150\n",
      "756/756 [==============================] - 0s - loss: 0.0655 - val_loss: 0.0708\n",
      "Epoch 43/150\n",
      "756/756 [==============================] - 0s - loss: 0.0669 - val_loss: 0.0709\n",
      "Epoch 44/150\n",
      "756/756 [==============================] - 0s - loss: 0.0661 - val_loss: 0.0721\n",
      "Epoch 45/150\n",
      "756/756 [==============================] - 0s - loss: 0.0649 - val_loss: 0.0710\n",
      "Epoch 46/150\n",
      "756/756 [==============================] - 0s - loss: 0.0650 - val_loss: 0.0702\n",
      "Epoch 47/150\n",
      "756/756 [==============================] - 0s - loss: 0.0665 - val_loss: 0.0716\n",
      "Epoch 48/150\n",
      "756/756 [==============================] - 0s - loss: 0.0644 - val_loss: 0.0714\n",
      "Epoch 49/150\n",
      "756/756 [==============================] - 0s - loss: 0.0637 - val_loss: 0.0702\n",
      "Epoch 50/150\n",
      "756/756 [==============================] - 0s - loss: 0.0652 - val_loss: 0.0702\n",
      "Epoch 51/150\n",
      "756/756 [==============================] - 0s - loss: 0.0641 - val_loss: 0.0715\n",
      "Epoch 52/150\n",
      "756/756 [==============================] - 0s - loss: 0.0630 - val_loss: 0.0705\n",
      "Epoch 53/150\n",
      "756/756 [==============================] - 0s - loss: 0.0633 - val_loss: 0.0696\n",
      "Epoch 54/150\n",
      "756/756 [==============================] - 0s - loss: 0.0647 - val_loss: 0.0712\n",
      "Epoch 55/150\n",
      "756/756 [==============================] - 0s - loss: 0.0626 - val_loss: 0.0709\n",
      "Epoch 56/150\n",
      "756/756 [==============================] - 0s - loss: 0.0619 - val_loss: 0.0697\n",
      "Epoch 57/150\n",
      "756/756 [==============================] - 0s - loss: 0.0635 - val_loss: 0.0699\n",
      "Epoch 58/150\n",
      "756/756 [==============================] - 0s - loss: 0.0629 - val_loss: 0.0718\n",
      "Epoch 59/150\n",
      "756/756 [==============================] - 0s - loss: 0.0614 - val_loss: 0.0704\n",
      "Epoch 60/150\n",
      "756/756 [==============================] - 0s - loss: 0.0614 - val_loss: 0.0690\n",
      "Epoch 61/150\n",
      "756/756 [==============================] - 0s - loss: 0.0640 - val_loss: 0.0711\n",
      "Epoch 62/150\n",
      "756/756 [==============================] - 0s - loss: 0.0619 - val_loss: 0.0720\n",
      "Epoch 63/150\n",
      "756/756 [==============================] - 0s - loss: 0.0607 - val_loss: 0.0694\n",
      "Epoch 64/150\n",
      "756/756 [==============================] - 0s - loss: 0.0631 - val_loss: 0.0692\n",
      "Epoch 65/150\n",
      "756/756 [==============================] - 0s - loss: 0.0625 - val_loss: 0.0725\n",
      "Epoch 66/150\n",
      "756/756 [==============================] - 0s - loss: 0.0604 - val_loss: 0.0704\n",
      "Epoch 67/150\n",
      "756/756 [==============================] - 0s - loss: 0.0604 - val_loss: 0.0689\n",
      "Epoch 68/150\n",
      "756/756 [==============================] - 0s - loss: 0.0637 - val_loss: 0.0715\n",
      "Epoch 69/150\n",
      "756/756 [==============================] - 0s - loss: 0.0601 - val_loss: 0.0716\n",
      "Epoch 70/150\n",
      "756/756 [==============================] - 0s - loss: 0.0593 - val_loss: 0.0694\n",
      "Epoch 71/150\n",
      "756/756 [==============================] - 0s - loss: 0.0619 - val_loss: 0.0698\n",
      "Epoch 72/150\n",
      "756/756 [==============================] - 0s - loss: 0.0608 - val_loss: 0.0728\n",
      "Epoch 73/150\n",
      "756/756 [==============================] - 0s - loss: 0.0590 - val_loss: 0.0703\n",
      "Epoch 74/150\n",
      "756/756 [==============================] - 0s - loss: 0.0597 - val_loss: 0.0689\n",
      "Epoch 75/150\n",
      "756/756 [==============================] - 0s - loss: 0.0624 - val_loss: 0.0723\n",
      "Epoch 76/150\n",
      "756/756 [==============================] - 0s - loss: 0.0589 - val_loss: 0.0719\n",
      "Epoch 77/150\n",
      "756/756 [==============================] - 0s - loss: 0.0581 - val_loss: 0.0695\n",
      "Epoch 78/150\n",
      "756/756 [==============================] - 0s - loss: 0.0610 - val_loss: 0.0704\n",
      "Epoch 79/150\n",
      "756/756 [==============================] - 0s - loss: 0.0596 - val_loss: 0.0736\n",
      "Epoch 80/150\n",
      "756/756 [==============================] - 0s - loss: 0.0577 - val_loss: 0.0702\n",
      "Epoch 81/150\n",
      "756/756 [==============================] - 0s - loss: 0.0595 - val_loss: 0.0692\n",
      "Epoch 82/150\n",
      "756/756 [==============================] - 0s - loss: 0.0617 - val_loss: 0.0737\n",
      "Epoch 83/150\n",
      "756/756 [==============================] - 0s - loss: 0.0580 - val_loss: 0.0716\n",
      "Epoch 84/150\n",
      "756/756 [==============================] - 0s - loss: 0.0579 - val_loss: 0.0694\n",
      "Epoch 85/150\n",
      "756/756 [==============================] - 0s - loss: 0.0617 - val_loss: 0.0722\n",
      "Epoch 86/150\n",
      "756/756 [==============================] - 0s - loss: 0.0579 - val_loss: 0.0732\n",
      "Epoch 87/150\n",
      "756/756 [==============================] - 0s - loss: 0.0568 - val_loss: 0.0701\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 0s - loss: 0.0595 - val_loss: 0.0703\n",
      "Epoch 89/150\n",
      "756/756 [==============================] - 0s - loss: 0.0587 - val_loss: 0.0744\n",
      "Epoch 90/150\n",
      "756/756 [==============================] - 0s - loss: 0.0566 - val_loss: 0.0710\n",
      "Epoch 91/150\n",
      "756/756 [==============================] - 0s - loss: 0.0579 - val_loss: 0.0696\n",
      "Epoch 92/150\n",
      "756/756 [==============================] - 0s - loss: 0.0607 - val_loss: 0.0743\n",
      "Epoch 93/150\n",
      "756/756 [==============================] - 0s - loss: 0.0568 - val_loss: 0.0728\n",
      "Epoch 94/150\n",
      "756/756 [==============================] - 0s - loss: 0.0564 - val_loss: 0.0701\n",
      "Epoch 95/150\n",
      "756/756 [==============================] - 0s - loss: 0.0596 - val_loss: 0.0721\n",
      "Epoch 96/150\n",
      "756/756 [==============================] - 0s - loss: 0.0568 - val_loss: 0.0743\n",
      "Epoch 97/150\n",
      "756/756 [==============================] - 0s - loss: 0.0555 - val_loss: 0.0708\n",
      "Epoch 98/150\n",
      "756/756 [==============================] - 0s - loss: 0.0584 - val_loss: 0.0707\n",
      "Epoch 99/150\n",
      "756/756 [==============================] - 0s - loss: 0.0579 - val_loss: 0.0754\n",
      "Epoch 100/150\n",
      "756/756 [==============================] - 0s - loss: 0.0554 - val_loss: 0.0716\n",
      "Epoch 101/150\n",
      "756/756 [==============================] - 0s - loss: 0.0570 - val_loss: 0.0701\n",
      "Epoch 102/150\n",
      "756/756 [==============================] - 0s - loss: 0.0599 - val_loss: 0.0758\n",
      "Epoch 103/150\n",
      "756/756 [==============================] - 0s - loss: 0.0557 - val_loss: 0.0730\n",
      "Epoch 104/150\n",
      "756/756 [==============================] - 0s - loss: 0.0560 - val_loss: 0.0705\n",
      "Epoch 105/150\n",
      "756/756 [==============================] - 0s - loss: 0.0590 - val_loss: 0.0742\n",
      "Epoch 106/150\n",
      "756/756 [==============================] - 0s - loss: 0.0554 - val_loss: 0.0741\n",
      "Epoch 107/150\n",
      "756/756 [==============================] - 0s - loss: 0.0549 - val_loss: 0.0712\n",
      "Epoch 108/150\n",
      "756/756 [==============================] - 0s - loss: 0.0577 - val_loss: 0.0724\n",
      "Epoch 109/150\n",
      "756/756 [==============================] - 0s - loss: 0.0556 - val_loss: 0.0755\n",
      "Epoch 110/150\n",
      "756/756 [==============================] - 0s - loss: 0.0541 - val_loss: 0.0717\n",
      "Epoch 111/150\n",
      "756/756 [==============================] - 0s - loss: 0.0569 - val_loss: 0.0713\n",
      "Epoch 112/150\n",
      "756/756 [==============================] - 0s - loss: 0.0571 - val_loss: 0.0768\n",
      "Epoch 113/150\n",
      "756/756 [==============================] - 0s - loss: 0.0541 - val_loss: 0.0723\n",
      "Epoch 114/150\n",
      "756/756 [==============================] - 0s - loss: 0.0563 - val_loss: 0.0710\n",
      "Epoch 115/150\n",
      "756/756 [==============================] - 0s - loss: 0.0588 - val_loss: 0.0772\n",
      "Epoch 116/150\n",
      "756/756 [==============================] - 0s - loss: 0.0544 - val_loss: 0.0734\n",
      "Epoch 117/150\n",
      "756/756 [==============================] - 0s - loss: 0.0555 - val_loss: 0.0712\n",
      "Epoch 118/150\n",
      "756/756 [==============================] - 0s - loss: 0.0574 - val_loss: 0.0760\n",
      "Epoch 119/150\n",
      "756/756 [==============================] - 0s - loss: 0.0538 - val_loss: 0.0742\n",
      "Epoch 120/150\n",
      "756/756 [==============================] - 0s - loss: 0.0542 - val_loss: 0.0718\n",
      "Epoch 121/150\n",
      "756/756 [==============================] - 0s - loss: 0.0565 - val_loss: 0.0749\n",
      "Epoch 122/150\n",
      "756/756 [==============================] - 0s - loss: 0.0535 - val_loss: 0.0752\n",
      "Epoch 123/150\n",
      "756/756 [==============================] - 0s - loss: 0.0531 - val_loss: 0.0724\n",
      "Epoch 124/150\n",
      "756/756 [==============================] - 0s - loss: 0.0556 - val_loss: 0.0734\n",
      "Epoch 125/150\n",
      "756/756 [==============================] - 0s - loss: 0.0541 - val_loss: 0.0770\n",
      "Epoch 126/150\n",
      "756/756 [==============================] - 0s - loss: 0.0523 - val_loss: 0.0728\n",
      "Epoch 127/150\n",
      "756/756 [==============================] - 0s - loss: 0.0553 - val_loss: 0.0722\n",
      "Epoch 128/150\n",
      "756/756 [==============================] - 0s - loss: 0.0563 - val_loss: 0.0786\n",
      "Epoch 129/150\n",
      "756/756 [==============================] - 0s - loss: 0.0529 - val_loss: 0.0736\n",
      "Epoch 130/150\n",
      "756/756 [==============================] - 0s - loss: 0.0548 - val_loss: 0.0722\n",
      "Epoch 131/150\n",
      "756/756 [==============================] - 0s - loss: 0.0568 - val_loss: 0.0780\n",
      "Epoch 132/150\n",
      "756/756 [==============================] - 0s - loss: 0.0528 - val_loss: 0.0748\n",
      "Epoch 133/150\n",
      "756/756 [==============================] - 0s - loss: 0.0535 - val_loss: 0.0726\n",
      "Epoch 134/150\n",
      "756/756 [==============================] - 0s - loss: 0.0558 - val_loss: 0.0771\n",
      "Epoch 135/150\n",
      "756/756 [==============================] - 0s - loss: 0.0522 - val_loss: 0.0757\n",
      "Epoch 136/150\n",
      "756/756 [==============================] - 0s - loss: 0.0520 - val_loss: 0.0729\n",
      "Epoch 137/150\n",
      "756/756 [==============================] - 0s - loss: 0.0550 - val_loss: 0.0757\n",
      "Epoch 138/150\n",
      "756/756 [==============================] - 0s - loss: 0.0524 - val_loss: 0.0778\n",
      "Epoch 139/150\n",
      "756/756 [==============================] - 0s - loss: 0.0512 - val_loss: 0.0735\n",
      "Epoch 140/150\n",
      "756/756 [==============================] - 0s - loss: 0.0548 - val_loss: 0.0740\n",
      "Epoch 141/150\n",
      "756/756 [==============================] - 0s - loss: 0.0536 - val_loss: 0.0797\n",
      "Epoch 142/150\n",
      "756/756 [==============================] - 0s - loss: 0.0513 - val_loss: 0.0742\n",
      "Epoch 143/150\n",
      "756/756 [==============================] - 0s - loss: 0.0545 - val_loss: 0.0734\n",
      "Epoch 144/150\n",
      "756/756 [==============================] - 0s - loss: 0.0553 - val_loss: 0.0801\n",
      "Epoch 145/150\n",
      "756/756 [==============================] - 0s - loss: 0.0519 - val_loss: 0.0754\n",
      "Epoch 146/150\n",
      "756/756 [==============================] - 0s - loss: 0.0536 - val_loss: 0.0740\n",
      "Epoch 147/150\n",
      "756/756 [==============================] - 0s - loss: 0.0547 - val_loss: 0.0790\n",
      "Epoch 148/150\n",
      "756/756 [==============================] - 0s - loss: 0.0511 - val_loss: 0.0760\n",
      "Epoch 149/150\n",
      "756/756 [==============================] - 0s - loss: 0.0520 - val_loss: 0.0743\n",
      "Epoch 150/150\n",
      "756/756 [==============================] - 0s - loss: 0.0537 - val_loss: 0.0786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 7/22 [03:05<06:14, 25.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 756 samples, validate on 190 samples\n",
      "Epoch 1/150\n",
      "756/756 [==============================] - 5s - loss: 0.3082 - val_loss: 0.1829\n",
      "Epoch 2/150\n",
      "756/756 [==============================] - 0s - loss: 0.2225 - val_loss: 0.1732\n",
      "Epoch 3/150\n",
      "756/756 [==============================] - 0s - loss: 0.1936 - val_loss: 0.1617\n",
      "Epoch 4/150\n",
      "756/756 [==============================] - 0s - loss: 0.1723 - val_loss: 0.1407\n",
      "Epoch 5/150\n",
      "756/756 [==============================] - 0s - loss: 0.1536 - val_loss: 0.1270\n",
      "Epoch 6/150\n",
      "756/756 [==============================] - 0s - loss: 0.1397 - val_loss: 0.1262\n",
      "Epoch 7/150\n",
      "756/756 [==============================] - 0s - loss: 0.1328 - val_loss: 0.1287\n",
      "Epoch 8/150\n",
      "756/756 [==============================] - 0s - loss: 0.1301 - val_loss: 0.1283\n",
      "Epoch 9/150\n",
      "756/756 [==============================] - 0s - loss: 0.1276 - val_loss: 0.1255\n",
      "Epoch 10/150\n",
      "756/756 [==============================] - 0s - loss: 0.1242 - val_loss: 0.1226\n",
      "Epoch 11/150\n",
      "756/756 [==============================] - 0s - loss: 0.1208 - val_loss: 0.1208\n",
      "Epoch 12/150\n",
      "756/756 [==============================] - 0s - loss: 0.1178 - val_loss: 0.1197\n",
      "Epoch 13/150\n",
      "756/756 [==============================] - 0s - loss: 0.1153 - val_loss: 0.1183\n",
      "Epoch 14/150\n",
      "756/756 [==============================] - 0s - loss: 0.1127 - val_loss: 0.1167\n",
      "Epoch 15/150\n",
      "756/756 [==============================] - 0s - loss: 0.1102 - val_loss: 0.1152\n",
      "Epoch 16/150\n",
      "756/756 [==============================] - 0s - loss: 0.1078 - val_loss: 0.1137\n",
      "Epoch 17/150\n",
      "756/756 [==============================] - 0s - loss: 0.1055 - val_loss: 0.1122\n",
      "Epoch 18/150\n",
      "756/756 [==============================] - 0s - loss: 0.1034 - val_loss: 0.1104\n",
      "Epoch 19/150\n",
      "756/756 [==============================] - 0s - loss: 0.1012 - val_loss: 0.1088\n",
      "Epoch 20/150\n",
      "756/756 [==============================] - 0s - loss: 0.0993 - val_loss: 0.1074\n",
      "Epoch 21/150\n",
      "756/756 [==============================] - 0s - loss: 0.0974 - val_loss: 0.1059\n",
      "Epoch 22/150\n",
      "756/756 [==============================] - 0s - loss: 0.0955 - val_loss: 0.1044\n",
      "Epoch 23/150\n",
      "756/756 [==============================] - 0s - loss: 0.0939 - val_loss: 0.1032\n",
      "Epoch 24/150\n",
      "756/756 [==============================] - 0s - loss: 0.0922 - val_loss: 0.1017\n",
      "Epoch 25/150\n",
      "756/756 [==============================] - 0s - loss: 0.0908 - val_loss: 0.1007\n",
      "Epoch 26/150\n",
      "756/756 [==============================] - 0s - loss: 0.0893 - val_loss: 0.0993\n",
      "Epoch 27/150\n",
      "756/756 [==============================] - 0s - loss: 0.0880 - val_loss: 0.0982\n",
      "Epoch 28/150\n",
      "756/756 [==============================] - 0s - loss: 0.0867 - val_loss: 0.0972\n",
      "Epoch 29/150\n",
      "756/756 [==============================] - 0s - loss: 0.0855 - val_loss: 0.0959\n",
      "Epoch 30/150\n",
      "756/756 [==============================] - 0s - loss: 0.0845 - val_loss: 0.0951\n",
      "Epoch 31/150\n",
      "756/756 [==============================] - 0s - loss: 0.0835 - val_loss: 0.0941\n",
      "Epoch 32/150\n",
      "756/756 [==============================] - 0s - loss: 0.0824 - val_loss: 0.0931\n",
      "Epoch 33/150\n",
      "756/756 [==============================] - 0s - loss: 0.0816 - val_loss: 0.0924\n",
      "Epoch 34/150\n",
      "756/756 [==============================] - 0s - loss: 0.0809 - val_loss: 0.0920\n",
      "Epoch 35/150\n",
      "756/756 [==============================] - 0s - loss: 0.0801 - val_loss: 0.0916\n",
      "Epoch 36/150\n",
      "756/756 [==============================] - 0s - loss: 0.0793 - val_loss: 0.0908\n",
      "Epoch 37/150\n",
      "756/756 [==============================] - 0s - loss: 0.0788 - val_loss: 0.0903\n",
      "Epoch 38/150\n",
      "756/756 [==============================] - 0s - loss: 0.0783 - val_loss: 0.0902\n",
      "Epoch 39/150\n",
      "756/756 [==============================] - 0s - loss: 0.0776 - val_loss: 0.0901\n",
      "Epoch 40/150\n",
      "756/756 [==============================] - 0s - loss: 0.0770 - val_loss: 0.0893\n",
      "Epoch 41/150\n",
      "756/756 [==============================] - 0s - loss: 0.0768 - val_loss: 0.0892\n",
      "Epoch 42/150\n",
      "756/756 [==============================] - 0s - loss: 0.0764 - val_loss: 0.0892\n",
      "Epoch 43/150\n",
      "756/756 [==============================] - 0s - loss: 0.0757 - val_loss: 0.0887\n",
      "Epoch 44/150\n",
      "756/756 [==============================] - 0s - loss: 0.0753 - val_loss: 0.0882\n",
      "Epoch 45/150\n",
      "756/756 [==============================] - 0s - loss: 0.0752 - val_loss: 0.0883\n",
      "Epoch 46/150\n",
      "756/756 [==============================] - 0s - loss: 0.0748 - val_loss: 0.0885\n",
      "Epoch 47/150\n",
      "756/756 [==============================] - 0s - loss: 0.0741 - val_loss: 0.0880\n",
      "Epoch 48/150\n",
      "756/756 [==============================] - 0s - loss: 0.0739 - val_loss: 0.0875\n",
      "Epoch 49/150\n",
      "756/756 [==============================] - 0s - loss: 0.0738 - val_loss: 0.0876\n",
      "Epoch 50/150\n",
      "756/756 [==============================] - 0s - loss: 0.0734 - val_loss: 0.0875\n",
      "Epoch 51/150\n",
      "756/756 [==============================] - 0s - loss: 0.0732 - val_loss: 0.0874\n",
      "Epoch 52/150\n",
      "756/756 [==============================] - 0s - loss: 0.0728 - val_loss: 0.0872\n",
      "Epoch 53/150\n",
      "756/756 [==============================] - 0s - loss: 0.0726 - val_loss: 0.0871\n",
      "Epoch 54/150\n",
      "756/756 [==============================] - 0s - loss: 0.0722 - val_loss: 0.0866\n",
      "Epoch 55/150\n",
      "756/756 [==============================] - 0s - loss: 0.0722 - val_loss: 0.0862\n",
      "Epoch 56/150\n",
      "756/756 [==============================] - 0s - loss: 0.0724 - val_loss: 0.0871\n",
      "Epoch 57/150\n",
      "756/756 [==============================] - 0s - loss: 0.0715 - val_loss: 0.0869\n",
      "Epoch 58/150\n",
      "756/756 [==============================] - 0s - loss: 0.0709 - val_loss: 0.0856\n",
      "Epoch 59/150\n",
      "756/756 [==============================] - 0s - loss: 0.0713 - val_loss: 0.0853\n",
      "Epoch 60/150\n",
      "756/756 [==============================] - 0s - loss: 0.0719 - val_loss: 0.0869\n",
      "Epoch 61/150\n",
      "756/756 [==============================] - 0s - loss: 0.0708 - val_loss: 0.0867\n",
      "Epoch 62/150\n",
      "756/756 [==============================] - 0s - loss: 0.0699 - val_loss: 0.0856\n",
      "Epoch 63/150\n",
      "756/756 [==============================] - 0s - loss: 0.0702 - val_loss: 0.0851\n",
      "Epoch 64/150\n",
      "756/756 [==============================] - 0s - loss: 0.0706 - val_loss: 0.0855\n",
      "Epoch 65/150\n",
      "756/756 [==============================] - 0s - loss: 0.0707 - val_loss: 0.0865\n",
      "Epoch 66/150\n",
      "756/756 [==============================] - 0s - loss: 0.0696 - val_loss: 0.0863\n",
      "Epoch 67/150\n",
      "756/756 [==============================] - 0s - loss: 0.0689 - val_loss: 0.0850\n",
      "Epoch 68/150\n",
      "756/756 [==============================] - 0s - loss: 0.0692 - val_loss: 0.0844\n",
      "Epoch 69/150\n",
      "756/756 [==============================] - 0s - loss: 0.0700 - val_loss: 0.0851\n",
      "Epoch 70/150\n",
      "756/756 [==============================] - 0s - loss: 0.0702 - val_loss: 0.0865\n",
      "Epoch 71/150\n",
      "756/756 [==============================] - 0s - loss: 0.0688 - val_loss: 0.0862\n",
      "Epoch 72/150\n",
      "756/756 [==============================] - 0s - loss: 0.0681 - val_loss: 0.0851\n",
      "Epoch 73/150\n",
      "756/756 [==============================] - 0s - loss: 0.0685 - val_loss: 0.0839\n",
      "Epoch 74/150\n",
      "756/756 [==============================] - 0s - loss: 0.0696 - val_loss: 0.0848\n",
      "Epoch 75/150\n",
      "756/756 [==============================] - 0s - loss: 0.0694 - val_loss: 0.0864\n",
      "Epoch 76/150\n",
      "756/756 [==============================] - 0s - loss: 0.0677 - val_loss: 0.0859\n",
      "Epoch 77/150\n",
      "756/756 [==============================] - 0s - loss: 0.0672 - val_loss: 0.0845\n",
      "Epoch 78/150\n",
      "756/756 [==============================] - 0s - loss: 0.0677 - val_loss: 0.0836\n",
      "Epoch 79/150\n",
      "756/756 [==============================] - 0s - loss: 0.0690 - val_loss: 0.0848\n",
      "Epoch 80/150\n",
      "756/756 [==============================] - 0s - loss: 0.0688 - val_loss: 0.0866\n",
      "Epoch 81/150\n",
      "756/756 [==============================] - 0s - loss: 0.0669 - val_loss: 0.0858\n",
      "Epoch 82/150\n",
      "756/756 [==============================] - 0s - loss: 0.0664 - val_loss: 0.0841\n",
      "Epoch 83/150\n",
      "756/756 [==============================] - 0s - loss: 0.0671 - val_loss: 0.0834\n",
      "Epoch 84/150\n",
      "756/756 [==============================] - 0s - loss: 0.0684 - val_loss: 0.0846\n",
      "Epoch 85/150\n",
      "756/756 [==============================] - 0s - loss: 0.0683 - val_loss: 0.0868\n",
      "Epoch 86/150\n",
      "756/756 [==============================] - 0s - loss: 0.0662 - val_loss: 0.0860\n",
      "Epoch 87/150\n",
      "756/756 [==============================] - 0s - loss: 0.0657 - val_loss: 0.0838\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 0s - loss: 0.0665 - val_loss: 0.0830\n",
      "Epoch 89/150\n",
      "756/756 [==============================] - 0s - loss: 0.0681 - val_loss: 0.0845\n",
      "Epoch 90/150\n",
      "756/756 [==============================] - 0s - loss: 0.0680 - val_loss: 0.0869\n",
      "Epoch 91/150\n",
      "756/756 [==============================] - 0s - loss: 0.0655 - val_loss: 0.0858\n",
      "Epoch 92/150\n",
      "756/756 [==============================] - 0s - loss: 0.0651 - val_loss: 0.0839\n",
      "Epoch 93/150\n",
      "756/756 [==============================] - 0s - loss: 0.0661 - val_loss: 0.0835\n",
      "Epoch 94/150\n",
      "756/756 [==============================] - 0s - loss: 0.0674 - val_loss: 0.0845\n",
      "Epoch 95/150\n",
      "756/756 [==============================] - 0s - loss: 0.0670 - val_loss: 0.0865\n",
      "Epoch 96/150\n",
      "756/756 [==============================] - 0s - loss: 0.0649 - val_loss: 0.0855\n",
      "Epoch 97/150\n",
      "756/756 [==============================] - 0s - loss: 0.0647 - val_loss: 0.0841\n",
      "Epoch 98/150\n",
      "756/756 [==============================] - 0s - loss: 0.0654 - val_loss: 0.0835\n",
      "Epoch 99/150\n",
      "756/756 [==============================] - 0s - loss: 0.0667 - val_loss: 0.0846\n",
      "Epoch 100/150\n",
      "756/756 [==============================] - 0s - loss: 0.0663 - val_loss: 0.0864\n",
      "Epoch 101/150\n",
      "756/756 [==============================] - 0s - loss: 0.0644 - val_loss: 0.0859\n",
      "Epoch 102/150\n",
      "756/756 [==============================] - 0s - loss: 0.0640 - val_loss: 0.0844\n",
      "Epoch 103/150\n",
      "756/756 [==============================] - 0s - loss: 0.0644 - val_loss: 0.0828\n",
      "Epoch 104/150\n",
      "756/756 [==============================] - 0s - loss: 0.0661 - val_loss: 0.0837\n",
      "Epoch 105/150\n",
      "756/756 [==============================] - 0s - loss: 0.0670 - val_loss: 0.0873\n",
      "Epoch 106/150\n",
      "756/756 [==============================] - 0s - loss: 0.0641 - val_loss: 0.0870\n",
      "Epoch 107/150\n",
      "756/756 [==============================] - 0s - loss: 0.0634 - val_loss: 0.0841\n",
      "Epoch 108/150\n",
      "756/756 [==============================] - 0s - loss: 0.0642 - val_loss: 0.0822\n",
      "Epoch 109/150\n",
      "756/756 [==============================] - 0s - loss: 0.0662 - val_loss: 0.0838\n",
      "Epoch 110/150\n",
      "756/756 [==============================] - 0s - loss: 0.0666 - val_loss: 0.0876\n",
      "Epoch 111/150\n",
      "756/756 [==============================] - 0s - loss: 0.0632 - val_loss: 0.0862\n",
      "Epoch 112/150\n",
      "756/756 [==============================] - 0s - loss: 0.0630 - val_loss: 0.0844\n",
      "Epoch 113/150\n",
      "756/756 [==============================] - 0s - loss: 0.0638 - val_loss: 0.0831\n",
      "Epoch 114/150\n",
      "756/756 [==============================] - 0s - loss: 0.0656 - val_loss: 0.0844\n",
      "Epoch 115/150\n",
      "756/756 [==============================] - 0s - loss: 0.0656 - val_loss: 0.0873\n",
      "Epoch 116/150\n",
      "756/756 [==============================] - 0s - loss: 0.0627 - val_loss: 0.0861\n",
      "Epoch 117/150\n",
      "756/756 [==============================] - 0s - loss: 0.0624 - val_loss: 0.0843\n",
      "Epoch 118/150\n",
      "756/756 [==============================] - 0s - loss: 0.0631 - val_loss: 0.0831\n",
      "Epoch 119/150\n",
      "756/756 [==============================] - 0s - loss: 0.0650 - val_loss: 0.0844\n",
      "Epoch 120/150\n",
      "756/756 [==============================] - 0s - loss: 0.0653 - val_loss: 0.0880\n",
      "Epoch 121/150\n",
      "756/756 [==============================] - 0s - loss: 0.0622 - val_loss: 0.0863\n",
      "Epoch 122/150\n",
      "756/756 [==============================] - 0s - loss: 0.0621 - val_loss: 0.0847\n",
      "Epoch 123/150\n",
      "756/756 [==============================] - 0s - loss: 0.0628 - val_loss: 0.0827\n",
      "Epoch 124/150\n",
      "756/756 [==============================] - 0s - loss: 0.0652 - val_loss: 0.0842\n",
      "Epoch 125/150\n",
      "756/756 [==============================] - 0s - loss: 0.0657 - val_loss: 0.0886\n",
      "Epoch 126/150\n",
      "756/756 [==============================] - 0s - loss: 0.0616 - val_loss: 0.0867\n",
      "Epoch 127/150\n",
      "756/756 [==============================] - 0s - loss: 0.0616 - val_loss: 0.0843\n",
      "Epoch 128/150\n",
      "756/756 [==============================] - 0s - loss: 0.0625 - val_loss: 0.0825\n",
      "Epoch 129/150\n",
      "756/756 [==============================] - 0s - loss: 0.0654 - val_loss: 0.0854\n",
      "Epoch 130/150\n",
      "756/756 [==============================] - 0s - loss: 0.0641 - val_loss: 0.0882\n",
      "Epoch 131/150\n",
      "756/756 [==============================] - 0s - loss: 0.0608 - val_loss: 0.0861\n",
      "Epoch 132/150\n",
      "756/756 [==============================] - 0s - loss: 0.0614 - val_loss: 0.0839\n",
      "Epoch 133/150\n",
      "756/756 [==============================] - 0s - loss: 0.0626 - val_loss: 0.0829\n",
      "Epoch 134/150\n",
      "756/756 [==============================] - 0s - loss: 0.0662 - val_loss: 0.0877\n",
      "Epoch 135/150\n",
      "756/756 [==============================] - 0s - loss: 0.0623 - val_loss: 0.0879\n",
      "Epoch 136/150\n",
      "756/756 [==============================] - 0s - loss: 0.0603 - val_loss: 0.0852\n",
      "Epoch 137/150\n",
      "756/756 [==============================] - 0s - loss: 0.0612 - val_loss: 0.0834\n",
      "Epoch 138/150\n",
      "756/756 [==============================] - 0s - loss: 0.0628 - val_loss: 0.0839\n",
      "Epoch 139/150\n",
      "756/756 [==============================] - 0s - loss: 0.0652 - val_loss: 0.0885\n",
      "Epoch 140/150\n",
      "756/756 [==============================] - 0s - loss: 0.0609 - val_loss: 0.0873\n",
      "Epoch 141/150\n",
      "756/756 [==============================] - 0s - loss: 0.0600 - val_loss: 0.0852\n",
      "Epoch 142/150\n",
      "756/756 [==============================] - 0s - loss: 0.0609 - val_loss: 0.0836\n",
      "Epoch 143/150\n",
      "756/756 [==============================] - 0s - loss: 0.0628 - val_loss: 0.0848\n",
      "Epoch 144/150\n",
      "756/756 [==============================] - 0s - loss: 0.0641 - val_loss: 0.0891\n",
      "Epoch 145/150\n",
      "756/756 [==============================] - 0s - loss: 0.0603 - val_loss: 0.0874\n",
      "Epoch 146/150\n",
      "756/756 [==============================] - 0s - loss: 0.0597 - val_loss: 0.0850\n",
      "Epoch 147/150\n",
      "756/756 [==============================] - 0s - loss: 0.0608 - val_loss: 0.0832\n",
      "Epoch 148/150\n",
      "756/756 [==============================] - 0s - loss: 0.0635 - val_loss: 0.0852\n",
      "Epoch 149/150\n",
      "756/756 [==============================] - 0s - loss: 0.0638 - val_loss: 0.0897\n",
      "Epoch 150/150\n",
      "756/756 [==============================] - 0s - loss: 0.0594 - val_loss: 0.0871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▋      | 8/22 [03:30<05:53, 25.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 756 samples, validate on 190 samples\n",
      "Epoch 1/150\n",
      "756/756 [==============================] - 8s - loss: 0.3156 - val_loss: 0.1219\n",
      "Epoch 2/150\n",
      "756/756 [==============================] - 0s - loss: 0.2368 - val_loss: 0.1320\n",
      "Epoch 3/150\n",
      "756/756 [==============================] - 0s - loss: 0.2062 - val_loss: 0.1459\n",
      "Epoch 4/150\n",
      "756/756 [==============================] - 0s - loss: 0.1905 - val_loss: 0.1531\n",
      "Epoch 5/150\n",
      "756/756 [==============================] - 0s - loss: 0.1813 - val_loss: 0.1510\n",
      "Epoch 6/150\n",
      "756/756 [==============================] - 0s - loss: 0.1741 - val_loss: 0.1426\n",
      "Epoch 7/150\n",
      "756/756 [==============================] - 0s - loss: 0.1666 - val_loss: 0.1322\n",
      "Epoch 8/150\n",
      "756/756 [==============================] - 0s - loss: 0.1588 - val_loss: 0.1232\n",
      "Epoch 9/150\n",
      "756/756 [==============================] - 0s - loss: 0.1513 - val_loss: 0.1166\n",
      "Epoch 10/150\n",
      "756/756 [==============================] - 0s - loss: 0.1446 - val_loss: 0.1123\n",
      "Epoch 11/150\n",
      "756/756 [==============================] - 0s - loss: 0.1383 - val_loss: 0.1095\n",
      "Epoch 12/150\n",
      "756/756 [==============================] - 0s - loss: 0.1327 - val_loss: 0.1065\n",
      "Epoch 13/150\n",
      "756/756 [==============================] - 0s - loss: 0.1274 - val_loss: 0.1026\n",
      "Epoch 14/150\n",
      "756/756 [==============================] - 0s - loss: 0.1222 - val_loss: 0.0984\n",
      "Epoch 15/150\n",
      "756/756 [==============================] - 0s - loss: 0.1172 - val_loss: 0.0939\n",
      "Epoch 16/150\n",
      "756/756 [==============================] - 0s - loss: 0.1124 - val_loss: 0.0897\n",
      "Epoch 17/150\n",
      "756/756 [==============================] - 0s - loss: 0.1078 - val_loss: 0.0861\n",
      "Epoch 18/150\n",
      "756/756 [==============================] - 0s - loss: 0.1037 - val_loss: 0.0833\n",
      "Epoch 19/150\n",
      "756/756 [==============================] - 0s - loss: 0.0999 - val_loss: 0.0808\n",
      "Epoch 20/150\n",
      "756/756 [==============================] - 0s - loss: 0.0967 - val_loss: 0.0788\n",
      "Epoch 21/150\n",
      "756/756 [==============================] - 0s - loss: 0.0937 - val_loss: 0.0770\n",
      "Epoch 22/150\n",
      "756/756 [==============================] - 0s - loss: 0.0911 - val_loss: 0.0756\n",
      "Epoch 23/150\n",
      "756/756 [==============================] - 0s - loss: 0.0889 - val_loss: 0.0745\n",
      "Epoch 24/150\n",
      "756/756 [==============================] - 0s - loss: 0.0869 - val_loss: 0.0735\n",
      "Epoch 25/150\n",
      "756/756 [==============================] - 0s - loss: 0.0851 - val_loss: 0.0727\n",
      "Epoch 26/150\n",
      "756/756 [==============================] - 0s - loss: 0.0837 - val_loss: 0.0720\n",
      "Epoch 27/150\n",
      "756/756 [==============================] - 0s - loss: 0.0823 - val_loss: 0.0713\n",
      "Epoch 28/150\n",
      "756/756 [==============================] - 0s - loss: 0.0811 - val_loss: 0.0707\n",
      "Epoch 29/150\n",
      "756/756 [==============================] - 0s - loss: 0.0800 - val_loss: 0.0701\n",
      "Epoch 30/150\n",
      "756/756 [==============================] - 0s - loss: 0.0791 - val_loss: 0.0696\n",
      "Epoch 31/150\n",
      "756/756 [==============================] - 0s - loss: 0.0783 - val_loss: 0.0692\n",
      "Epoch 32/150\n",
      "756/756 [==============================] - 0s - loss: 0.0774 - val_loss: 0.0688\n",
      "Epoch 33/150\n",
      "756/756 [==============================] - 0s - loss: 0.0767 - val_loss: 0.0686\n",
      "Epoch 34/150\n",
      "756/756 [==============================] - 0s - loss: 0.0759 - val_loss: 0.0682\n",
      "Epoch 35/150\n",
      "756/756 [==============================] - 0s - loss: 0.0753 - val_loss: 0.0679\n",
      "Epoch 36/150\n",
      "756/756 [==============================] - 0s - loss: 0.0748 - val_loss: 0.0676\n",
      "Epoch 37/150\n",
      "756/756 [==============================] - 0s - loss: 0.0741 - val_loss: 0.0674\n",
      "Epoch 38/150\n",
      "756/756 [==============================] - 0s - loss: 0.0737 - val_loss: 0.0672\n",
      "Epoch 39/150\n",
      "756/756 [==============================] - 0s - loss: 0.0732 - val_loss: 0.0669\n",
      "Epoch 40/150\n",
      "756/756 [==============================] - 0s - loss: 0.0726 - val_loss: 0.0668\n",
      "Epoch 41/150\n",
      "756/756 [==============================] - 0s - loss: 0.0722 - val_loss: 0.0665\n",
      "Epoch 42/150\n",
      "756/756 [==============================] - 0s - loss: 0.0717 - val_loss: 0.0663\n",
      "Epoch 43/150\n",
      "756/756 [==============================] - 0s - loss: 0.0713 - val_loss: 0.0662\n",
      "Epoch 44/150\n",
      "756/756 [==============================] - 0s - loss: 0.0709 - val_loss: 0.0659\n",
      "Epoch 45/150\n",
      "756/756 [==============================] - 0s - loss: 0.0705 - val_loss: 0.0657\n",
      "Epoch 46/150\n",
      "756/756 [==============================] - 0s - loss: 0.0701 - val_loss: 0.0656\n",
      "Epoch 47/150\n",
      "756/756 [==============================] - 0s - loss: 0.0697 - val_loss: 0.0654\n",
      "Epoch 48/150\n",
      "756/756 [==============================] - 0s - loss: 0.0695 - val_loss: 0.0651\n",
      "Epoch 49/150\n",
      "756/756 [==============================] - 0s - loss: 0.0690 - val_loss: 0.0650\n",
      "Epoch 50/150\n",
      "756/756 [==============================] - 0s - loss: 0.0688 - val_loss: 0.0649\n",
      "Epoch 51/150\n",
      "756/756 [==============================] - 0s - loss: 0.0685 - val_loss: 0.0644\n",
      "Epoch 52/150\n",
      "756/756 [==============================] - 0s - loss: 0.0679 - val_loss: 0.0645\n",
      "Epoch 53/150\n",
      "756/756 [==============================] - 0s - loss: 0.0679 - val_loss: 0.0643\n",
      "Epoch 54/150\n",
      "756/756 [==============================] - 0s - loss: 0.0677 - val_loss: 0.0640\n",
      "Epoch 55/150\n",
      "756/756 [==============================] - 0s - loss: 0.0671 - val_loss: 0.0639\n",
      "Epoch 56/150\n",
      "756/756 [==============================] - 0s - loss: 0.0668 - val_loss: 0.0639\n",
      "Epoch 57/150\n",
      "756/756 [==============================] - 0s - loss: 0.0669 - val_loss: 0.0635\n",
      "Epoch 58/150\n",
      "756/756 [==============================] - 0s - loss: 0.0665 - val_loss: 0.0633\n",
      "Epoch 59/150\n",
      "756/756 [==============================] - 0s - loss: 0.0655 - val_loss: 0.0634\n",
      "Epoch 60/150\n",
      "756/756 [==============================] - 0s - loss: 0.0660 - val_loss: 0.0631\n",
      "Epoch 61/150\n",
      "756/756 [==============================] - 0s - loss: 0.0662 - val_loss: 0.0625\n",
      "Epoch 62/150\n",
      "756/756 [==============================] - 0s - loss: 0.0648 - val_loss: 0.0630\n",
      "Epoch 63/150\n",
      "756/756 [==============================] - 0s - loss: 0.0649 - val_loss: 0.0629\n",
      "Epoch 64/150\n",
      "756/756 [==============================] - 0s - loss: 0.0655 - val_loss: 0.0622\n",
      "Epoch 65/150\n",
      "756/756 [==============================] - 0s - loss: 0.0648 - val_loss: 0.0622\n",
      "Epoch 66/150\n",
      "756/756 [==============================] - 0s - loss: 0.0636 - val_loss: 0.0626\n",
      "Epoch 67/150\n",
      "756/756 [==============================] - ETA: 0s - loss: 0.066 - 0s - loss: 0.0646 - val_loss: 0.0620\n",
      "Epoch 68/150\n",
      "756/756 [==============================] - 0s - loss: 0.0647 - val_loss: 0.0613\n",
      "Epoch 69/150\n",
      "756/756 [==============================] - 0s - loss: 0.0629 - val_loss: 0.0620\n",
      "Epoch 70/150\n",
      "756/756 [==============================] - 0s - loss: 0.0631 - val_loss: 0.0620\n",
      "Epoch 71/150\n",
      "756/756 [==============================] - 0s - loss: 0.0642 - val_loss: 0.0610\n",
      "Epoch 72/150\n",
      "756/756 [==============================] - 0s - loss: 0.0630 - val_loss: 0.0613\n",
      "Epoch 73/150\n",
      "756/756 [==============================] - 0s - loss: 0.0619 - val_loss: 0.0616\n",
      "Epoch 74/150\n",
      "756/756 [==============================] - 0s - loss: 0.0629 - val_loss: 0.0612\n",
      "Epoch 75/150\n",
      "756/756 [==============================] - 0s - loss: 0.0635 - val_loss: 0.0603\n",
      "Epoch 76/150\n",
      "756/756 [==============================] - 0s - loss: 0.0612 - val_loss: 0.0611\n",
      "Epoch 77/150\n",
      "756/756 [==============================] - 0s - loss: 0.0616 - val_loss: 0.0614\n",
      "Epoch 78/150\n",
      "756/756 [==============================] - 0s - loss: 0.0633 - val_loss: 0.0600\n",
      "Epoch 79/150\n",
      "756/756 [==============================] - 0s - loss: 0.0617 - val_loss: 0.0606\n",
      "Epoch 80/150\n",
      "756/756 [==============================] - 0s - loss: 0.0604 - val_loss: 0.0610\n",
      "Epoch 81/150\n",
      "756/756 [==============================] - 0s - loss: 0.0618 - val_loss: 0.0608\n",
      "Epoch 82/150\n",
      "756/756 [==============================] - 0s - loss: 0.0626 - val_loss: 0.0595\n",
      "Epoch 83/150\n",
      "756/756 [==============================] - 0s - loss: 0.0596 - val_loss: 0.0606\n",
      "Epoch 84/150\n",
      "756/756 [==============================] - 0s - loss: 0.0604 - val_loss: 0.0608\n",
      "Epoch 85/150\n",
      "756/756 [==============================] - 0s - loss: 0.0617 - val_loss: 0.0592\n",
      "Epoch 86/150\n",
      "756/756 [==============================] - 0s - loss: 0.0600 - val_loss: 0.0599\n",
      "Epoch 87/150\n",
      "756/756 [==============================] - 0s - loss: 0.0591 - val_loss: 0.0604\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 0s - loss: 0.0604 - val_loss: 0.0600\n",
      "Epoch 89/150\n",
      "756/756 [==============================] - 0s - loss: 0.0609 - val_loss: 0.0591\n",
      "Epoch 90/150\n",
      "756/756 [==============================] - 0s - loss: 0.0585 - val_loss: 0.0600\n",
      "Epoch 91/150\n",
      "756/756 [==============================] - 0s - loss: 0.0591 - val_loss: 0.0604\n",
      "Epoch 92/150\n",
      "756/756 [==============================] - 0s - loss: 0.0605 - val_loss: 0.0587\n",
      "Epoch 93/150\n",
      "756/756 [==============================] - 0s - loss: 0.0589 - val_loss: 0.0593\n",
      "Epoch 94/150\n",
      "756/756 [==============================] - 0s - loss: 0.0578 - val_loss: 0.0598\n",
      "Epoch 95/150\n",
      "756/756 [==============================] - 0s - loss: 0.0590 - val_loss: 0.0601\n",
      "Epoch 96/150\n",
      "756/756 [==============================] - 0s - loss: 0.0606 - val_loss: 0.0584\n",
      "Epoch 97/150\n",
      "756/756 [==============================] - 0s - loss: 0.0571 - val_loss: 0.0595\n",
      "Epoch 98/150\n",
      "756/756 [==============================] - 0s - loss: 0.0579 - val_loss: 0.0604\n",
      "Epoch 99/150\n",
      "756/756 [==============================] - 0s - loss: 0.0598 - val_loss: 0.0585\n",
      "Epoch 100/150\n",
      "756/756 [==============================] - 0s - loss: 0.0585 - val_loss: 0.0589\n",
      "Epoch 101/150\n",
      "756/756 [==============================] - 0s - loss: 0.0565 - val_loss: 0.0595\n",
      "Epoch 102/150\n",
      "756/756 [==============================] - 0s - loss: 0.0579 - val_loss: 0.0601\n",
      "Epoch 103/150\n",
      "756/756 [==============================] - 0s - loss: 0.0598 - val_loss: 0.0576\n",
      "Epoch 104/150\n",
      "756/756 [==============================] - 0s - loss: 0.0561 - val_loss: 0.0590\n",
      "Epoch 105/150\n",
      "756/756 [==============================] - 0s - loss: 0.0565 - val_loss: 0.0600\n",
      "Epoch 106/150\n",
      "756/756 [==============================] - 0s - loss: 0.0585 - val_loss: 0.0586\n",
      "Epoch 107/150\n",
      "756/756 [==============================] - 0s - loss: 0.0580 - val_loss: 0.0582\n",
      "Epoch 108/150\n",
      "756/756 [==============================] - 0s - loss: 0.0554 - val_loss: 0.0591\n",
      "Epoch 109/150\n",
      "756/756 [==============================] - 0s - loss: 0.0568 - val_loss: 0.0601\n",
      "Epoch 110/150\n",
      "756/756 [==============================] - 0s - loss: 0.0589 - val_loss: 0.0573\n",
      "Epoch 111/150\n",
      "756/756 [==============================] - 0s - loss: 0.0554 - val_loss: 0.0587\n",
      "Epoch 112/150\n",
      "756/756 [==============================] - 0s - loss: 0.0554 - val_loss: 0.0598\n",
      "Epoch 113/150\n",
      "756/756 [==============================] - 0s - loss: 0.0574 - val_loss: 0.0588\n",
      "Epoch 114/150\n",
      "756/756 [==============================] - 0s - loss: 0.0577 - val_loss: 0.0576\n",
      "Epoch 115/150\n",
      "756/756 [==============================] - 0s - loss: 0.0541 - val_loss: 0.0587\n",
      "Epoch 116/150\n",
      "756/756 [==============================] - 0s - loss: 0.0555 - val_loss: 0.0603\n",
      "Epoch 117/150\n",
      "756/756 [==============================] - 0s - loss: 0.0584 - val_loss: 0.0573\n",
      "Epoch 118/150\n",
      "756/756 [==============================] - 0s - loss: 0.0554 - val_loss: 0.0587\n",
      "Epoch 119/150\n",
      "756/756 [==============================] - 0s - loss: 0.0547 - val_loss: 0.0599\n",
      "Epoch 120/150\n",
      "756/756 [==============================] - 0s - loss: 0.0568 - val_loss: 0.0590\n",
      "Epoch 121/150\n",
      "756/756 [==============================] - 0s - loss: 0.0568 - val_loss: 0.0571\n",
      "Epoch 122/150\n",
      "756/756 [==============================] - 0s - loss: 0.0532 - val_loss: 0.0586\n",
      "Epoch 123/150\n",
      "756/756 [==============================] - 0s - loss: 0.0545 - val_loss: 0.0600\n",
      "Epoch 124/150\n",
      "756/756 [==============================] - 0s - loss: 0.0566 - val_loss: 0.0575\n",
      "Epoch 125/150\n",
      "756/756 [==============================] - 0s - loss: 0.0546 - val_loss: 0.0583\n",
      "Epoch 126/150\n",
      "756/756 [==============================] - 0s - loss: 0.0533 - val_loss: 0.0594\n",
      "Epoch 127/150\n",
      "756/756 [==============================] - 0s - loss: 0.0552 - val_loss: 0.0593\n",
      "Epoch 128/150\n",
      "756/756 [==============================] - 0s - loss: 0.0559 - val_loss: 0.0570\n",
      "Epoch 129/150\n",
      "756/756 [==============================] - 0s - loss: 0.0522 - val_loss: 0.0585\n",
      "Epoch 130/150\n",
      "756/756 [==============================] - 0s - loss: 0.0534 - val_loss: 0.0604\n",
      "Epoch 131/150\n",
      "756/756 [==============================] - 0s - loss: 0.0562 - val_loss: 0.0580\n",
      "Epoch 132/150\n",
      "756/756 [==============================] - 0s - loss: 0.0548 - val_loss: 0.0584\n",
      "Epoch 133/150\n",
      "756/756 [==============================] - 0s - loss: 0.0524 - val_loss: 0.0594\n",
      "Epoch 134/150\n",
      "756/756 [==============================] - 0s - loss: 0.0544 - val_loss: 0.0601\n",
      "Epoch 135/150\n",
      "756/756 [==============================] - 0s - loss: 0.0558 - val_loss: 0.0570\n",
      "Epoch 136/150\n",
      "756/756 [==============================] - 0s - loss: 0.0515 - val_loss: 0.0588\n",
      "Epoch 137/150\n",
      "756/756 [==============================] - 0s - loss: 0.0528 - val_loss: 0.0608\n",
      "Epoch 138/150\n",
      "756/756 [==============================] - 0s - loss: 0.0553 - val_loss: 0.0583\n",
      "Epoch 139/150\n",
      "756/756 [==============================] - 0s - loss: 0.0540 - val_loss: 0.0583\n",
      "Epoch 140/150\n",
      "756/756 [==============================] - 0s - loss: 0.0515 - val_loss: 0.0594\n",
      "Epoch 141/150\n",
      "756/756 [==============================] - 0s - loss: 0.0533 - val_loss: 0.0602\n",
      "Epoch 142/150\n",
      "756/756 [==============================] - 0s - loss: 0.0545 - val_loss: 0.0573\n",
      "Epoch 143/150\n",
      "756/756 [==============================] - 0s - loss: 0.0512 - val_loss: 0.0590\n",
      "Epoch 144/150\n",
      "756/756 [==============================] - 0s - loss: 0.0516 - val_loss: 0.0604\n",
      "Epoch 145/150\n",
      "756/756 [==============================] - 0s - loss: 0.0537 - val_loss: 0.0593\n",
      "Epoch 146/150\n",
      "756/756 [==============================] - 0s - loss: 0.0537 - val_loss: 0.0580\n",
      "Epoch 147/150\n",
      "756/756 [==============================] - 0s - loss: 0.0502 - val_loss: 0.0592\n",
      "Epoch 148/150\n",
      "756/756 [==============================] - 0s - loss: 0.0522 - val_loss: 0.0613\n",
      "Epoch 149/150\n",
      "756/756 [==============================] - 0s - loss: 0.0549 - val_loss: 0.0577\n",
      "Epoch 150/150\n",
      "756/756 [==============================] - 0s - loss: 0.0512 - val_loss: 0.0594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████      | 9/22 [03:57<05:34, 25.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 756 samples, validate on 190 samples\n",
      "Epoch 1/150\n",
      "756/756 [==============================] - 5s - loss: 0.3998 - val_loss: 0.1799\n",
      "Epoch 2/150\n",
      "756/756 [==============================] - 0s - loss: 0.2638 - val_loss: 0.1502\n",
      "Epoch 3/150\n",
      "756/756 [==============================] - 0s - loss: 0.1965 - val_loss: 0.1515\n",
      "Epoch 4/150\n",
      "756/756 [==============================] - 0s - loss: 0.1629 - val_loss: 0.1475\n",
      "Epoch 5/150\n",
      "756/756 [==============================] - 0s - loss: 0.1476 - val_loss: 0.1374\n",
      "Epoch 6/150\n",
      "756/756 [==============================] - 0s - loss: 0.1397 - val_loss: 0.1287\n",
      "Epoch 7/150\n",
      "756/756 [==============================] - 0s - loss: 0.1334 - val_loss: 0.1177\n",
      "Epoch 8/150\n",
      "756/756 [==============================] - 0s - loss: 0.1254 - val_loss: 0.1092\n",
      "Epoch 9/150\n",
      "756/756 [==============================] - 0s - loss: 0.1184 - val_loss: 0.1043\n",
      "Epoch 10/150\n",
      "756/756 [==============================] - 0s - loss: 0.1122 - val_loss: 0.0983\n",
      "Epoch 11/150\n",
      "756/756 [==============================] - 0s - loss: 0.1069 - val_loss: 0.0936\n",
      "Epoch 12/150\n",
      "756/756 [==============================] - 0s - loss: 0.1019 - val_loss: 0.0920\n",
      "Epoch 13/150\n",
      "756/756 [==============================] - 0s - loss: 0.0982 - val_loss: 0.0888\n",
      "Epoch 14/150\n",
      "756/756 [==============================] - 0s - loss: 0.0949 - val_loss: 0.0860\n",
      "Epoch 15/150\n",
      "756/756 [==============================] - 0s - loss: 0.0922 - val_loss: 0.0840\n",
      "Epoch 16/150\n",
      "756/756 [==============================] - 0s - loss: 0.0899 - val_loss: 0.0823\n",
      "Epoch 17/150\n",
      "756/756 [==============================] - 0s - loss: 0.0880 - val_loss: 0.0805\n",
      "Epoch 18/150\n",
      "756/756 [==============================] - 0s - loss: 0.0862 - val_loss: 0.0792\n",
      "Epoch 19/150\n",
      "756/756 [==============================] - 0s - loss: 0.0847 - val_loss: 0.0779\n",
      "Epoch 20/150\n",
      "756/756 [==============================] - 0s - loss: 0.0833 - val_loss: 0.0766\n",
      "Epoch 21/150\n",
      "756/756 [==============================] - 0s - loss: 0.0821 - val_loss: 0.0752\n",
      "Epoch 22/150\n",
      "756/756 [==============================] - 0s - loss: 0.0809 - val_loss: 0.0742\n",
      "Epoch 23/150\n",
      "756/756 [==============================] - 0s - loss: 0.0799 - val_loss: 0.0730\n",
      "Epoch 24/150\n",
      "756/756 [==============================] - 0s - loss: 0.0788 - val_loss: 0.0719\n",
      "Epoch 25/150\n",
      "756/756 [==============================] - 0s - loss: 0.0779 - val_loss: 0.0708\n",
      "Epoch 26/150\n",
      "756/756 [==============================] - 0s - loss: 0.0769 - val_loss: 0.0697\n",
      "Epoch 27/150\n",
      "756/756 [==============================] - 0s - loss: 0.0761 - val_loss: 0.0689\n",
      "Epoch 28/150\n",
      "756/756 [==============================] - 0s - loss: 0.0752 - val_loss: 0.0676\n",
      "Epoch 29/150\n",
      "756/756 [==============================] - 0s - loss: 0.0744 - val_loss: 0.0671\n",
      "Epoch 30/150\n",
      "756/756 [==============================] - 0s - loss: 0.0736 - val_loss: 0.0658\n",
      "Epoch 31/150\n",
      "756/756 [==============================] - 0s - loss: 0.0730 - val_loss: 0.0658\n",
      "Epoch 32/150\n",
      "756/756 [==============================] - 0s - loss: 0.0721 - val_loss: 0.0643\n",
      "Epoch 33/150\n",
      "756/756 [==============================] - 0s - loss: 0.0715 - val_loss: 0.0644\n",
      "Epoch 34/150\n",
      "756/756 [==============================] - 0s - loss: 0.0709 - val_loss: 0.0629\n",
      "Epoch 35/150\n",
      "756/756 [==============================] - 0s - loss: 0.0702 - val_loss: 0.0632\n",
      "Epoch 36/150\n",
      "756/756 [==============================] - 0s - loss: 0.0696 - val_loss: 0.0621\n",
      "Epoch 37/150\n",
      "756/756 [==============================] - 0s - loss: 0.0691 - val_loss: 0.0622\n",
      "Epoch 38/150\n",
      "756/756 [==============================] - 0s - loss: 0.0684 - val_loss: 0.0612\n",
      "Epoch 39/150\n",
      "756/756 [==============================] - 0s - loss: 0.0681 - val_loss: 0.0612\n",
      "Epoch 40/150\n",
      "756/756 [==============================] - 0s - loss: 0.0673 - val_loss: 0.0606\n",
      "Epoch 41/150\n",
      "756/756 [==============================] - 0s - loss: 0.0670 - val_loss: 0.0607\n",
      "Epoch 42/150\n",
      "756/756 [==============================] - 0s - loss: 0.0664 - val_loss: 0.0601\n",
      "Epoch 43/150\n",
      "756/756 [==============================] - 0s - loss: 0.0662 - val_loss: 0.0602\n",
      "Epoch 44/150\n",
      "756/756 [==============================] - 0s - loss: 0.0656 - val_loss: 0.0598\n",
      "Epoch 45/150\n",
      "756/756 [==============================] - 0s - loss: 0.0654 - val_loss: 0.0593\n",
      "Epoch 46/150\n",
      "756/756 [==============================] - 0s - loss: 0.0647 - val_loss: 0.0591\n",
      "Epoch 47/150\n",
      "756/756 [==============================] - 0s - loss: 0.0645 - val_loss: 0.0595\n",
      "Epoch 48/150\n",
      "756/756 [==============================] - 0s - loss: 0.0642 - val_loss: 0.0589\n",
      "Epoch 49/150\n",
      "756/756 [==============================] - 0s - loss: 0.0638 - val_loss: 0.0588\n",
      "Epoch 50/150\n",
      "756/756 [==============================] - 0s - loss: 0.0633 - val_loss: 0.0591\n",
      "Epoch 51/150\n",
      "756/756 [==============================] - 0s - loss: 0.0633 - val_loss: 0.0592\n",
      "Epoch 52/150\n",
      "756/756 [==============================] - 0s - loss: 0.0631 - val_loss: 0.0585\n",
      "Epoch 53/150\n",
      "756/756 [==============================] - 0s - loss: 0.0627 - val_loss: 0.0588\n",
      "Epoch 54/150\n",
      "756/756 [==============================] - 0s - loss: 0.0625 - val_loss: 0.0585\n",
      "Epoch 55/150\n",
      "756/756 [==============================] - 0s - loss: 0.0622 - val_loss: 0.0585\n",
      "Epoch 56/150\n",
      "756/756 [==============================] - 0s - loss: 0.0618 - val_loss: 0.0580\n",
      "Epoch 57/150\n",
      "756/756 [==============================] - 0s - loss: 0.0613 - val_loss: 0.0585\n",
      "Epoch 58/150\n",
      "756/756 [==============================] - 0s - loss: 0.0612 - val_loss: 0.0588\n",
      "Epoch 59/150\n",
      "756/756 [==============================] - 0s - loss: 0.0612 - val_loss: 0.0590\n",
      "Epoch 60/150\n",
      "756/756 [==============================] - 0s - loss: 0.0612 - val_loss: 0.0587\n",
      "Epoch 61/150\n",
      "756/756 [==============================] - 0s - loss: 0.0611 - val_loss: 0.0585\n",
      "Epoch 62/150\n",
      "756/756 [==============================] - 0s - loss: 0.0609 - val_loss: 0.0581\n",
      "Epoch 63/150\n",
      "756/756 [==============================] - 0s - loss: 0.0605 - val_loss: 0.0580\n",
      "Epoch 64/150\n",
      "756/756 [==============================] - 0s - loss: 0.0601 - val_loss: 0.0576\n",
      "Epoch 65/150\n",
      "756/756 [==============================] - 0s - loss: 0.0595 - val_loss: 0.0577\n",
      "Epoch 66/150\n",
      "756/756 [==============================] - 0s - loss: 0.0592 - val_loss: 0.0583\n",
      "Epoch 67/150\n",
      "756/756 [==============================] - 0s - loss: 0.0592 - val_loss: 0.0592\n",
      "Epoch 68/150\n",
      "756/756 [==============================] - 0s - loss: 0.0596 - val_loss: 0.0595\n",
      "Epoch 69/150\n",
      "756/756 [==============================] - 0s - loss: 0.0600 - val_loss: 0.0591\n",
      "Epoch 70/150\n",
      "756/756 [==============================] - 0s - loss: 0.0600 - val_loss: 0.0582\n",
      "Epoch 71/150\n",
      "756/756 [==============================] - 0s - loss: 0.0594 - val_loss: 0.0574\n",
      "Epoch 72/150\n",
      "756/756 [==============================] - 0s - loss: 0.0586 - val_loss: 0.0574\n",
      "Epoch 73/150\n",
      "756/756 [==============================] - 0s - loss: 0.0582 - val_loss: 0.0577\n",
      "Epoch 74/150\n",
      "756/756 [==============================] - 0s - loss: 0.0580 - val_loss: 0.0583\n",
      "Epoch 75/150\n",
      "756/756 [==============================] - 0s - loss: 0.0579 - val_loss: 0.0590\n",
      "Epoch 76/150\n",
      "756/756 [==============================] - 0s - loss: 0.0582 - val_loss: 0.0595\n",
      "Epoch 77/150\n",
      "756/756 [==============================] - 0s - loss: 0.0587 - val_loss: 0.0588\n",
      "Epoch 78/150\n",
      "756/756 [==============================] - 0s - loss: 0.0584 - val_loss: 0.0584\n",
      "Epoch 79/150\n",
      "756/756 [==============================] - 0s - loss: 0.0581 - val_loss: 0.0582\n",
      "Epoch 80/150\n",
      "756/756 [==============================] - 0s - loss: 0.0580 - val_loss: 0.0578\n",
      "Epoch 81/150\n",
      "756/756 [==============================] - 0s - loss: 0.0574 - val_loss: 0.0577\n",
      "Epoch 82/150\n",
      "756/756 [==============================] - 0s - loss: 0.0568 - val_loss: 0.0581\n",
      "Epoch 83/150\n",
      "756/756 [==============================] - 0s - loss: 0.0567 - val_loss: 0.0584\n",
      "Epoch 84/150\n",
      "756/756 [==============================] - 0s - loss: 0.0568 - val_loss: 0.0588\n",
      "Epoch 85/150\n",
      "756/756 [==============================] - 0s - loss: 0.0569 - val_loss: 0.0593\n",
      "Epoch 86/150\n",
      "756/756 [==============================] - 0s - loss: 0.0572 - val_loss: 0.0595\n",
      "Epoch 87/150\n",
      "756/756 [==============================] - 0s - loss: 0.0575 - val_loss: 0.0590\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 0s - loss: 0.0575 - val_loss: 0.0578\n",
      "Epoch 89/150\n",
      "756/756 [==============================] - 0s - loss: 0.0567 - val_loss: 0.0572\n",
      "Epoch 90/150\n",
      "756/756 [==============================] - 0s - loss: 0.0559 - val_loss: 0.0573\n",
      "Epoch 91/150\n",
      "756/756 [==============================] - 0s - loss: 0.0555 - val_loss: 0.0574\n",
      "Epoch 92/150\n",
      "756/756 [==============================] - 0s - loss: 0.0551 - val_loss: 0.0579\n",
      "Epoch 93/150\n",
      "756/756 [==============================] - 0s - loss: 0.0552 - val_loss: 0.0592\n",
      "Epoch 94/150\n",
      "756/756 [==============================] - 0s - loss: 0.0556 - val_loss: 0.0604\n",
      "Epoch 95/150\n",
      "756/756 [==============================] - 0s - loss: 0.0564 - val_loss: 0.0608\n",
      "Epoch 96/150\n",
      "756/756 [==============================] - 0s - loss: 0.0572 - val_loss: 0.0597\n",
      "Epoch 97/150\n",
      "756/756 [==============================] - 0s - loss: 0.0572 - val_loss: 0.0578\n",
      "Epoch 98/150\n",
      "756/756 [==============================] - 0s - loss: 0.0558 - val_loss: 0.0569\n",
      "Epoch 99/150\n",
      "756/756 [==============================] - 0s - loss: 0.0545 - val_loss: 0.0573\n",
      "Epoch 100/150\n",
      "756/756 [==============================] - 0s - loss: 0.0543 - val_loss: 0.0579\n",
      "Epoch 101/150\n",
      "756/756 [==============================] - 0s - loss: 0.0543 - val_loss: 0.0586\n",
      "Epoch 102/150\n",
      "756/756 [==============================] - 0s - loss: 0.0544 - val_loss: 0.0597\n",
      "Epoch 103/150\n",
      "756/756 [==============================] - 0s - loss: 0.0549 - val_loss: 0.0605\n",
      "Epoch 104/150\n",
      "756/756 [==============================] - 0s - loss: 0.0556 - val_loss: 0.0602\n",
      "Epoch 105/150\n",
      "756/756 [==============================] - 0s - loss: 0.0558 - val_loss: 0.0596\n",
      "Epoch 106/150\n",
      "756/756 [==============================] - 0s - loss: 0.0559 - val_loss: 0.0583\n",
      "Epoch 107/150\n",
      "756/756 [==============================] - 0s - loss: 0.0550 - val_loss: 0.0574\n",
      "Epoch 108/150\n",
      "756/756 [==============================] - 0s - loss: 0.0539 - val_loss: 0.0574\n",
      "Epoch 109/150\n",
      "756/756 [==============================] - 0s - loss: 0.0534 - val_loss: 0.0579\n",
      "Epoch 110/150\n",
      "756/756 [==============================] - 0s - loss: 0.0532 - val_loss: 0.0586\n",
      "Epoch 111/150\n",
      "756/756 [==============================] - 0s - loss: 0.0533 - val_loss: 0.0594\n",
      "Epoch 112/150\n",
      "756/756 [==============================] - 0s - loss: 0.0537 - val_loss: 0.0603\n",
      "Epoch 113/150\n",
      "756/756 [==============================] - 0s - loss: 0.0543 - val_loss: 0.0607\n",
      "Epoch 114/150\n",
      "756/756 [==============================] - 0s - loss: 0.0546 - val_loss: 0.0605\n",
      "Epoch 115/150\n",
      "756/756 [==============================] - 0s - loss: 0.0549 - val_loss: 0.0597\n",
      "Epoch 116/150\n",
      "756/756 [==============================] - 0s - loss: 0.0547 - val_loss: 0.0584\n",
      "Epoch 117/150\n",
      "756/756 [==============================] - 0s - loss: 0.0537 - val_loss: 0.0577\n",
      "Epoch 118/150\n",
      "756/756 [==============================] - 0s - loss: 0.0527 - val_loss: 0.0579\n",
      "Epoch 119/150\n",
      "756/756 [==============================] - 0s - loss: 0.0522 - val_loss: 0.0585\n",
      "Epoch 120/150\n",
      "756/756 [==============================] - 0s - loss: 0.0523 - val_loss: 0.0593\n",
      "Epoch 121/150\n",
      "756/756 [==============================] - 0s - loss: 0.0525 - val_loss: 0.0601\n",
      "Epoch 122/150\n",
      "756/756 [==============================] - 0s - loss: 0.0527 - val_loss: 0.0609\n",
      "Epoch 123/150\n",
      "756/756 [==============================] - 0s - loss: 0.0531 - val_loss: 0.0611\n",
      "Epoch 124/150\n",
      "756/756 [==============================] - 0s - loss: 0.0537 - val_loss: 0.0606\n",
      "Epoch 125/150\n",
      "756/756 [==============================] - 0s - loss: 0.0538 - val_loss: 0.0597\n",
      "Epoch 126/150\n",
      "756/756 [==============================] - 0s - loss: 0.0535 - val_loss: 0.0594\n",
      "Epoch 127/150\n",
      "756/756 [==============================] - 0s - loss: 0.0532 - val_loss: 0.0589\n",
      "Epoch 128/150\n",
      "756/756 [==============================] - 0s - loss: 0.0528 - val_loss: 0.0584\n",
      "Epoch 129/150\n",
      "756/756 [==============================] - 0s - loss: 0.0520 - val_loss: 0.0583\n",
      "Epoch 130/150\n",
      "756/756 [==============================] - 0s - loss: 0.0514 - val_loss: 0.0588\n",
      "Epoch 131/150\n",
      "756/756 [==============================] - 0s - loss: 0.0514 - val_loss: 0.0590\n",
      "Epoch 132/150\n",
      "756/756 [==============================] - 0s - loss: 0.0515 - val_loss: 0.0594\n",
      "Epoch 133/150\n",
      "756/756 [==============================] - 0s - loss: 0.0517 - val_loss: 0.0598\n",
      "Epoch 134/150\n",
      "756/756 [==============================] - 0s - loss: 0.0516 - val_loss: 0.0602\n",
      "Epoch 135/150\n",
      "756/756 [==============================] - 0s - loss: 0.0515 - val_loss: 0.0608\n",
      "Epoch 136/150\n",
      "756/756 [==============================] - 0s - loss: 0.0518 - val_loss: 0.0616\n",
      "Epoch 137/150\n",
      "756/756 [==============================] - 0s - loss: 0.0524 - val_loss: 0.0615\n",
      "Epoch 138/150\n",
      "756/756 [==============================] - 0s - loss: 0.0528 - val_loss: 0.0610\n",
      "Epoch 139/150\n",
      "756/756 [==============================] - 0s - loss: 0.0528 - val_loss: 0.0601\n",
      "Epoch 140/150\n",
      "756/756 [==============================] - 0s - loss: 0.0523 - val_loss: 0.0593\n",
      "Epoch 141/150\n",
      "756/756 [==============================] - 0s - loss: 0.0516 - val_loss: 0.0586\n",
      "Epoch 142/150\n",
      "756/756 [==============================] - 0s - loss: 0.0505 - val_loss: 0.0588\n",
      "Epoch 143/150\n",
      "756/756 [==============================] - 0s - loss: 0.0502 - val_loss: 0.0590\n",
      "Epoch 144/150\n",
      "756/756 [==============================] - 0s - loss: 0.0500 - val_loss: 0.0592\n",
      "Epoch 145/150\n",
      "756/756 [==============================] - 0s - loss: 0.0498 - val_loss: 0.0598\n",
      "Epoch 146/150\n",
      "756/756 [==============================] - 0s - loss: 0.0500 - val_loss: 0.0605\n",
      "Epoch 147/150\n",
      "756/756 [==============================] - 0s - loss: 0.0503 - val_loss: 0.0613\n",
      "Epoch 148/150\n",
      "756/756 [==============================] - 0s - loss: 0.0506 - val_loss: 0.0625\n",
      "Epoch 149/150\n",
      "756/756 [==============================] - 0s - loss: 0.0514 - val_loss: 0.0630\n",
      "Epoch 150/150\n",
      "756/756 [==============================] - 0s - loss: 0.0521 - val_loss: 0.0629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 10/22 [04:20<04:58, 24.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 756 samples, validate on 189 samples\n",
      "Epoch 1/150\n",
      "756/756 [==============================] - 5s - loss: 0.3759 - val_loss: 0.1552\n",
      "Epoch 2/150\n",
      "756/756 [==============================] - 0s - loss: 0.2480 - val_loss: 0.1415\n",
      "Epoch 3/150\n",
      "756/756 [==============================] - 0s - loss: 0.1851 - val_loss: 0.1659\n",
      "Epoch 4/150\n",
      "756/756 [==============================] - 0s - loss: 0.1636 - val_loss: 0.1596\n",
      "Epoch 5/150\n",
      "756/756 [==============================] - 0s - loss: 0.1473 - val_loss: 0.1355\n",
      "Epoch 6/150\n",
      "756/756 [==============================] - 0s - loss: 0.1329 - val_loss: 0.1204\n",
      "Epoch 7/150\n",
      "756/756 [==============================] - 0s - loss: 0.1217 - val_loss: 0.1125\n",
      "Epoch 8/150\n",
      "756/756 [==============================] - 0s - loss: 0.1119 - val_loss: 0.1066\n",
      "Epoch 9/150\n",
      "756/756 [==============================] - 0s - loss: 0.1034 - val_loss: 0.0991\n",
      "Epoch 10/150\n",
      "756/756 [==============================] - 0s - loss: 0.0965 - val_loss: 0.0915\n",
      "Epoch 11/150\n",
      "756/756 [==============================] - 0s - loss: 0.0909 - val_loss: 0.0867\n",
      "Epoch 12/150\n",
      "756/756 [==============================] - 0s - loss: 0.0866 - val_loss: 0.0843\n",
      "Epoch 13/150\n",
      "756/756 [==============================] - 0s - loss: 0.0838 - val_loss: 0.0816\n",
      "Epoch 14/150\n",
      "756/756 [==============================] - 0s - loss: 0.0814 - val_loss: 0.0794\n",
      "Epoch 15/150\n",
      "756/756 [==============================] - 0s - loss: 0.0794 - val_loss: 0.0781\n",
      "Epoch 16/150\n",
      "756/756 [==============================] - 0s - loss: 0.0779 - val_loss: 0.0757\n",
      "Epoch 17/150\n",
      "756/756 [==============================] - 0s - loss: 0.0763 - val_loss: 0.0747\n",
      "Epoch 18/150\n",
      "756/756 [==============================] - 0s - loss: 0.0749 - val_loss: 0.0729\n",
      "Epoch 19/150\n",
      "756/756 [==============================] - 0s - loss: 0.0735 - val_loss: 0.0719\n",
      "Epoch 20/150\n",
      "756/756 [==============================] - 0s - loss: 0.0725 - val_loss: 0.0706\n",
      "Epoch 21/150\n",
      "756/756 [==============================] - 0s - loss: 0.0714 - val_loss: 0.0690\n",
      "Epoch 22/150\n",
      "756/756 [==============================] - 0s - loss: 0.0702 - val_loss: 0.0680\n",
      "Epoch 23/150\n",
      "756/756 [==============================] - 0s - loss: 0.0693 - val_loss: 0.0671\n",
      "Epoch 24/150\n",
      "756/756 [==============================] - 0s - loss: 0.0685 - val_loss: 0.0655\n",
      "Epoch 25/150\n",
      "756/756 [==============================] - 0s - loss: 0.0674 - val_loss: 0.0645\n",
      "Epoch 26/150\n",
      "756/756 [==============================] - 0s - loss: 0.0665 - val_loss: 0.0636\n",
      "Epoch 27/150\n",
      "756/756 [==============================] - 0s - loss: 0.0659 - val_loss: 0.0626\n",
      "Epoch 28/150\n",
      "756/756 [==============================] - 0s - loss: 0.0651 - val_loss: 0.0612\n",
      "Epoch 29/150\n",
      "756/756 [==============================] - 0s - loss: 0.0641 - val_loss: 0.0603\n",
      "Epoch 30/150\n",
      "756/756 [==============================] - 0s - loss: 0.0634 - val_loss: 0.0597\n",
      "Epoch 31/150\n",
      "756/756 [==============================] - 0s - loss: 0.0629 - val_loss: 0.0588\n",
      "Epoch 32/150\n",
      "756/756 [==============================] - 0s - loss: 0.0624 - val_loss: 0.0579\n",
      "Epoch 33/150\n",
      "756/756 [==============================] - 0s - loss: 0.0618 - val_loss: 0.0568\n",
      "Epoch 34/150\n",
      "756/756 [==============================] - 0s - loss: 0.0608 - val_loss: 0.0563\n",
      "Epoch 35/150\n",
      "756/756 [==============================] - 0s - loss: 0.0604 - val_loss: 0.0556\n",
      "Epoch 36/150\n",
      "756/756 [==============================] - 0s - loss: 0.0600 - val_loss: 0.0551\n",
      "Epoch 37/150\n",
      "756/756 [==============================] - 0s - loss: 0.0597 - val_loss: 0.0540\n",
      "Epoch 38/150\n",
      "756/756 [==============================] - 0s - loss: 0.0588 - val_loss: 0.0535\n",
      "Epoch 39/150\n",
      "756/756 [==============================] - 0s - loss: 0.0582 - val_loss: 0.0530\n",
      "Epoch 40/150\n",
      "756/756 [==============================] - 0s - loss: 0.0578 - val_loss: 0.0528\n",
      "Epoch 41/150\n",
      "756/756 [==============================] - 0s - loss: 0.0580 - val_loss: 0.0520\n",
      "Epoch 42/150\n",
      "756/756 [==============================] - 0s - loss: 0.0575 - val_loss: 0.0512\n",
      "Epoch 43/150\n",
      "756/756 [==============================] - 0s - loss: 0.0566 - val_loss: 0.0509\n",
      "Epoch 44/150\n",
      "756/756 [==============================] - 0s - loss: 0.0562 - val_loss: 0.0505\n",
      "Epoch 45/150\n",
      "756/756 [==============================] - 0s - loss: 0.0560 - val_loss: 0.0504\n",
      "Epoch 46/150\n",
      "756/756 [==============================] - 0s - loss: 0.0562 - val_loss: 0.0498\n",
      "Epoch 47/150\n",
      "756/756 [==============================] - 0s - loss: 0.0559 - val_loss: 0.0492\n",
      "Epoch 48/150\n",
      "756/756 [==============================] - 0s - loss: 0.0551 - val_loss: 0.0489\n",
      "Epoch 49/150\n",
      "756/756 [==============================] - 0s - loss: 0.0546 - val_loss: 0.0488\n",
      "Epoch 50/150\n",
      "756/756 [==============================] - 0s - loss: 0.0546 - val_loss: 0.0486\n",
      "Epoch 51/150\n",
      "756/756 [==============================] - 0s - loss: 0.0546 - val_loss: 0.0481\n",
      "Epoch 52/150\n",
      "756/756 [==============================] - 0s - loss: 0.0544 - val_loss: 0.0477\n",
      "Epoch 53/150\n",
      "756/756 [==============================] - 0s - loss: 0.0539 - val_loss: 0.0476\n",
      "Epoch 54/150\n",
      "756/756 [==============================] - 0s - loss: 0.0534 - val_loss: 0.0472\n",
      "Epoch 55/150\n",
      "756/756 [==============================] - 0s - loss: 0.0532 - val_loss: 0.0471\n",
      "Epoch 56/150\n",
      "756/756 [==============================] - 0s - loss: 0.0534 - val_loss: 0.0469\n",
      "Epoch 57/150\n",
      "756/756 [==============================] - 0s - loss: 0.0534 - val_loss: 0.0466\n",
      "Epoch 58/150\n",
      "756/756 [==============================] - 0s - loss: 0.0531 - val_loss: 0.0463\n",
      "Epoch 59/150\n",
      "756/756 [==============================] - 0s - loss: 0.0523 - val_loss: 0.0463\n",
      "Epoch 60/150\n",
      "756/756 [==============================] - 0s - loss: 0.0520 - val_loss: 0.0464\n",
      "Epoch 61/150\n",
      "756/756 [==============================] - 0s - loss: 0.0524 - val_loss: 0.0460\n",
      "Epoch 62/150\n",
      "756/756 [==============================] - 0s - loss: 0.0524 - val_loss: 0.0458\n",
      "Epoch 63/150\n",
      "756/756 [==============================] - 0s - loss: 0.0522 - val_loss: 0.0456\n",
      "Epoch 64/150\n",
      "756/756 [==============================] - 0s - loss: 0.0516 - val_loss: 0.0455\n",
      "Epoch 65/150\n",
      "756/756 [==============================] - 0s - loss: 0.0511 - val_loss: 0.0454\n",
      "Epoch 66/150\n",
      "756/756 [==============================] - 0s - loss: 0.0511 - val_loss: 0.0454\n",
      "Epoch 67/150\n",
      "756/756 [==============================] - 0s - loss: 0.0512 - val_loss: 0.0453\n",
      "Epoch 68/150\n",
      "756/756 [==============================] - 0s - loss: 0.0516 - val_loss: 0.0450\n",
      "Epoch 69/150\n",
      "756/756 [==============================] - 0s - loss: 0.0511 - val_loss: 0.0449\n",
      "Epoch 70/150\n",
      "756/756 [==============================] - 0s - loss: 0.0509 - val_loss: 0.0448\n",
      "Epoch 71/150\n",
      "756/756 [==============================] - 0s - loss: 0.0506 - val_loss: 0.0449\n",
      "Epoch 72/150\n",
      "756/756 [==============================] - 0s - loss: 0.0502 - val_loss: 0.0447\n",
      "Epoch 73/150\n",
      "756/756 [==============================] - 0s - loss: 0.0500 - val_loss: 0.0446\n",
      "Epoch 74/150\n",
      "756/756 [==============================] - 0s - loss: 0.0498 - val_loss: 0.0446\n",
      "Epoch 75/150\n",
      "756/756 [==============================] - 0s - loss: 0.0501 - val_loss: 0.0446\n",
      "Epoch 76/150\n",
      "756/756 [==============================] - 0s - loss: 0.0507 - val_loss: 0.0443\n",
      "Epoch 77/150\n",
      "756/756 [==============================] - 0s - loss: 0.0505 - val_loss: 0.0441\n",
      "Epoch 78/150\n",
      "756/756 [==============================] - 0s - loss: 0.0493 - val_loss: 0.0442\n",
      "Epoch 79/150\n",
      "756/756 [==============================] - 0s - loss: 0.0489 - val_loss: 0.0443\n",
      "Epoch 80/150\n",
      "756/756 [==============================] - 0s - loss: 0.0491 - val_loss: 0.0444\n",
      "Epoch 81/150\n",
      "756/756 [==============================] - 0s - loss: 0.0492 - val_loss: 0.0443\n",
      "Epoch 82/150\n",
      "756/756 [==============================] - 0s - loss: 0.0495 - val_loss: 0.0442\n",
      "Epoch 83/150\n",
      "756/756 [==============================] - 0s - loss: 0.0500 - val_loss: 0.0439\n",
      "Epoch 84/150\n",
      "756/756 [==============================] - 0s - loss: 0.0494 - val_loss: 0.0439\n",
      "Epoch 85/150\n",
      "756/756 [==============================] - 0s - loss: 0.0485 - val_loss: 0.0439\n",
      "Epoch 86/150\n",
      "756/756 [==============================] - 0s - loss: 0.0481 - val_loss: 0.0439\n",
      "Epoch 87/150\n",
      "756/756 [==============================] - 0s - loss: 0.0480 - val_loss: 0.0440\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 0s - loss: 0.0484 - val_loss: 0.0440\n",
      "Epoch 89/150\n",
      "756/756 [==============================] - 0s - loss: 0.0490 - val_loss: 0.0438\n",
      "Epoch 90/150\n",
      "756/756 [==============================] - 0s - loss: 0.0493 - val_loss: 0.0435\n",
      "Epoch 91/150\n",
      "756/756 [==============================] - 0s - loss: 0.0480 - val_loss: 0.0437\n",
      "Epoch 92/150\n",
      "756/756 [==============================] - 0s - loss: 0.0472 - val_loss: 0.0437\n",
      "Epoch 93/150\n",
      "756/756 [==============================] - 0s - loss: 0.0474 - val_loss: 0.0439\n",
      "Epoch 94/150\n",
      "756/756 [==============================] - 0s - loss: 0.0478 - val_loss: 0.0439\n",
      "Epoch 95/150\n",
      "756/756 [==============================] - 0s - loss: 0.0485 - val_loss: 0.0436\n",
      "Epoch 96/150\n",
      "756/756 [==============================] - 0s - loss: 0.0488 - val_loss: 0.0433\n",
      "Epoch 97/150\n",
      "756/756 [==============================] - 0s - loss: 0.0476 - val_loss: 0.0435\n",
      "Epoch 98/150\n",
      "756/756 [==============================] - 0s - loss: 0.0470 - val_loss: 0.0437\n",
      "Epoch 99/150\n",
      "756/756 [==============================] - 0s - loss: 0.0470 - val_loss: 0.0436\n",
      "Epoch 100/150\n",
      "756/756 [==============================] - 0s - loss: 0.0474 - val_loss: 0.0436\n",
      "Epoch 101/150\n",
      "756/756 [==============================] - 0s - loss: 0.0478 - val_loss: 0.0434\n",
      "Epoch 102/150\n",
      "756/756 [==============================] - 0s - loss: 0.0475 - val_loss: 0.0433\n",
      "Epoch 103/150\n",
      "756/756 [==============================] - 0s - loss: 0.0468 - val_loss: 0.0435\n",
      "Epoch 104/150\n",
      "756/756 [==============================] - 0s - loss: 0.0466 - val_loss: 0.0436\n",
      "Epoch 105/150\n",
      "756/756 [==============================] - 0s - loss: 0.0467 - val_loss: 0.0435\n",
      "Epoch 106/150\n",
      "756/756 [==============================] - 0s - loss: 0.0470 - val_loss: 0.0433\n",
      "Epoch 107/150\n",
      "756/756 [==============================] - 0s - loss: 0.0472 - val_loss: 0.0432\n",
      "Epoch 108/150\n",
      "756/756 [==============================] - 0s - loss: 0.0467 - val_loss: 0.0433\n",
      "Epoch 109/150\n",
      "756/756 [==============================] - 0s - loss: 0.0460 - val_loss: 0.0434\n",
      "Epoch 110/150\n",
      "756/756 [==============================] - 0s - loss: 0.0460 - val_loss: 0.0436\n",
      "Epoch 111/150\n",
      "756/756 [==============================] - 0s - loss: 0.0462 - val_loss: 0.0435\n",
      "Epoch 112/150\n",
      "756/756 [==============================] - 0s - loss: 0.0464 - val_loss: 0.0433\n",
      "Epoch 113/150\n",
      "756/756 [==============================] - 0s - loss: 0.0467 - val_loss: 0.0433\n",
      "Epoch 114/150\n",
      "756/756 [==============================] - 0s - loss: 0.0463 - val_loss: 0.0433\n",
      "Epoch 115/150\n",
      "756/756 [==============================] - 0s - loss: 0.0459 - val_loss: 0.0433\n",
      "Epoch 116/150\n",
      "756/756 [==============================] - 0s - loss: 0.0458 - val_loss: 0.0434\n",
      "Epoch 117/150\n",
      "756/756 [==============================] - 0s - loss: 0.0453 - val_loss: 0.0434\n",
      "Epoch 118/150\n",
      "756/756 [==============================] - 0s - loss: 0.0454 - val_loss: 0.0433\n",
      "Epoch 119/150\n",
      "756/756 [==============================] - 0s - loss: 0.0454 - val_loss: 0.0433\n",
      "Epoch 120/150\n",
      "756/756 [==============================] - 0s - loss: 0.0450 - val_loss: 0.0435\n",
      "Epoch 121/150\n",
      "756/756 [==============================] - 0s - loss: 0.0452 - val_loss: 0.0435\n",
      "Epoch 122/150\n",
      "756/756 [==============================] - 0s - loss: 0.0459 - val_loss: 0.0434\n",
      "Epoch 123/150\n",
      "756/756 [==============================] - 0s - loss: 0.0462 - val_loss: 0.0433\n",
      "Epoch 124/150\n",
      "756/756 [==============================] - 0s - loss: 0.0463 - val_loss: 0.0432\n",
      "Epoch 125/150\n",
      "756/756 [==============================] - 0s - loss: 0.0450 - val_loss: 0.0432\n",
      "Epoch 126/150\n",
      "756/756 [==============================] - 0s - loss: 0.0438 - val_loss: 0.0434\n",
      "Epoch 127/150\n",
      "756/756 [==============================] - 0s - loss: 0.0440 - val_loss: 0.0440\n",
      "Epoch 128/150\n",
      "756/756 [==============================] - 0s - loss: 0.0448 - val_loss: 0.0440\n",
      "Epoch 129/150\n",
      "756/756 [==============================] - 0s - loss: 0.0452 - val_loss: 0.0437\n",
      "Epoch 130/150\n",
      "756/756 [==============================] - 0s - loss: 0.0461 - val_loss: 0.0432\n",
      "Epoch 131/150\n",
      "756/756 [==============================] - 0s - loss: 0.0459 - val_loss: 0.0432\n",
      "Epoch 132/150\n",
      "756/756 [==============================] - 0s - loss: 0.0443 - val_loss: 0.0434\n",
      "Epoch 133/150\n",
      "756/756 [==============================] - 0s - loss: 0.0434 - val_loss: 0.0435\n",
      "Epoch 134/150\n",
      "756/756 [==============================] - 0s - loss: 0.0436 - val_loss: 0.0438\n",
      "Epoch 135/150\n",
      "756/756 [==============================] - 0s - loss: 0.0439 - val_loss: 0.0438\n",
      "Epoch 136/150\n",
      "756/756 [==============================] - 0s - loss: 0.0441 - val_loss: 0.0438\n",
      "Epoch 137/150\n",
      "756/756 [==============================] - 0s - loss: 0.0444 - val_loss: 0.0437\n",
      "Epoch 138/150\n",
      "756/756 [==============================] - 0s - loss: 0.0453 - val_loss: 0.0435\n",
      "Epoch 139/150\n",
      "756/756 [==============================] - 0s - loss: 0.0448 - val_loss: 0.0435\n",
      "Epoch 140/150\n",
      "756/756 [==============================] - 0s - loss: 0.0435 - val_loss: 0.0435\n",
      "Epoch 141/150\n",
      "756/756 [==============================] - 0s - loss: 0.0427 - val_loss: 0.0437\n",
      "Epoch 142/150\n",
      "756/756 [==============================] - 0s - loss: 0.0426 - val_loss: 0.0440\n",
      "Epoch 143/150\n",
      "756/756 [==============================] - 0s - loss: 0.0430 - val_loss: 0.0445\n",
      "Epoch 144/150\n",
      "756/756 [==============================] - 0s - loss: 0.0436 - val_loss: 0.0445\n",
      "Epoch 145/150\n",
      "756/756 [==============================] - 0s - loss: 0.0443 - val_loss: 0.0441\n",
      "Epoch 146/150\n",
      "756/756 [==============================] - 0s - loss: 0.0459 - val_loss: 0.0435\n",
      "Epoch 147/150\n",
      "756/756 [==============================] - 0s - loss: 0.0443 - val_loss: 0.0436\n",
      "Epoch 148/150\n",
      "756/756 [==============================] - 0s - loss: 0.0421 - val_loss: 0.0438\n",
      "Epoch 149/150\n",
      "756/756 [==============================] - 0s - loss: 0.0419 - val_loss: 0.0442\n",
      "Epoch 150/150\n",
      "756/756 [==============================] - 0s - loss: 0.0424 - val_loss: 0.0446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 11/22 [04:43<04:25, 24.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 595 samples, validate on 149 samples\n",
      "Epoch 1/150\n",
      "595/595 [==============================] - 5s - loss: 0.3209 - val_loss: 0.4798\n",
      "Epoch 2/150\n",
      "595/595 [==============================] - 0s - loss: 0.2324 - val_loss: 0.3582\n",
      "Epoch 3/150\n",
      "595/595 [==============================] - 0s - loss: 0.1963 - val_loss: 0.2862\n",
      "Epoch 4/150\n",
      "595/595 [==============================] - 0s - loss: 0.1795 - val_loss: 0.2547\n",
      "Epoch 5/150\n",
      "595/595 [==============================] - 0s - loss: 0.1671 - val_loss: 0.2435\n",
      "Epoch 6/150\n",
      "595/595 [==============================] - 0s - loss: 0.1565 - val_loss: 0.2351\n",
      "Epoch 7/150\n",
      "595/595 [==============================] - 0s - loss: 0.1468 - val_loss: 0.2202\n",
      "Epoch 8/150\n",
      "595/595 [==============================] - 0s - loss: 0.1372 - val_loss: 0.1989\n",
      "Epoch 9/150\n",
      "595/595 [==============================] - 0s - loss: 0.1276 - val_loss: 0.1779\n",
      "Epoch 10/150\n",
      "595/595 [==============================] - 0s - loss: 0.1190 - val_loss: 0.1621\n",
      "Epoch 11/150\n",
      "595/595 [==============================] - 0s - loss: 0.1112 - val_loss: 0.1493\n",
      "Epoch 12/150\n",
      "595/595 [==============================] - 0s - loss: 0.1037 - val_loss: 0.1373\n",
      "Epoch 13/150\n",
      "595/595 [==============================] - 0s - loss: 0.0967 - val_loss: 0.1253\n",
      "Epoch 14/150\n",
      "595/595 [==============================] - 0s - loss: 0.0904 - val_loss: 0.1147\n",
      "Epoch 15/150\n",
      "595/595 [==============================] - 0s - loss: 0.0848 - val_loss: 0.1065\n",
      "Epoch 16/150\n",
      "595/595 [==============================] - 0s - loss: 0.0800 - val_loss: 0.1000\n",
      "Epoch 17/150\n",
      "595/595 [==============================] - 0s - loss: 0.0758 - val_loss: 0.0945\n",
      "Epoch 18/150\n",
      "595/595 [==============================] - 0s - loss: 0.0723 - val_loss: 0.0899\n",
      "Epoch 19/150\n",
      "595/595 [==============================] - 0s - loss: 0.0693 - val_loss: 0.0863\n",
      "Epoch 20/150\n",
      "595/595 [==============================] - 0s - loss: 0.0668 - val_loss: 0.0833\n",
      "Epoch 21/150\n",
      "595/595 [==============================] - 0s - loss: 0.0646 - val_loss: 0.0809\n",
      "Epoch 22/150\n",
      "595/595 [==============================] - 0s - loss: 0.0628 - val_loss: 0.0790\n",
      "Epoch 23/150\n",
      "595/595 [==============================] - 0s - loss: 0.0612 - val_loss: 0.0775\n",
      "Epoch 24/150\n",
      "595/595 [==============================] - 0s - loss: 0.0598 - val_loss: 0.0762\n",
      "Epoch 25/150\n",
      "595/595 [==============================] - 0s - loss: 0.0586 - val_loss: 0.0749\n",
      "Epoch 26/150\n",
      "595/595 [==============================] - 0s - loss: 0.0575 - val_loss: 0.0738\n",
      "Epoch 27/150\n",
      "595/595 [==============================] - 0s - loss: 0.0567 - val_loss: 0.0729\n",
      "Epoch 28/150\n",
      "595/595 [==============================] - 0s - loss: 0.0558 - val_loss: 0.0720\n",
      "Epoch 29/150\n",
      "595/595 [==============================] - 0s - loss: 0.0551 - val_loss: 0.0713\n",
      "Epoch 30/150\n",
      "595/595 [==============================] - 0s - loss: 0.0544 - val_loss: 0.0704\n",
      "Epoch 31/150\n",
      "595/595 [==============================] - 0s - loss: 0.0539 - val_loss: 0.0699\n",
      "Epoch 32/150\n",
      "595/595 [==============================] - 0s - loss: 0.0533 - val_loss: 0.0692\n",
      "Epoch 33/150\n",
      "595/595 [==============================] - 0s - loss: 0.0528 - val_loss: 0.0686\n",
      "Epoch 34/150\n",
      "595/595 [==============================] - 0s - loss: 0.0523 - val_loss: 0.0681\n",
      "Epoch 35/150\n",
      "595/595 [==============================] - 0s - loss: 0.0520 - val_loss: 0.0675\n",
      "Epoch 36/150\n",
      "595/595 [==============================] - 0s - loss: 0.0514 - val_loss: 0.0669\n",
      "Epoch 37/150\n",
      "595/595 [==============================] - 0s - loss: 0.0510 - val_loss: 0.0666\n",
      "Epoch 38/150\n",
      "595/595 [==============================] - 0s - loss: 0.0508 - val_loss: 0.0662\n",
      "Epoch 39/150\n",
      "595/595 [==============================] - 0s - loss: 0.0503 - val_loss: 0.0655\n",
      "Epoch 40/150\n",
      "595/595 [==============================] - 0s - loss: 0.0498 - val_loss: 0.0653\n",
      "Epoch 41/150\n",
      "595/595 [==============================] - 0s - loss: 0.0499 - val_loss: 0.0652\n",
      "Epoch 42/150\n",
      "595/595 [==============================] - 0s - loss: 0.0493 - val_loss: 0.0644\n",
      "Epoch 43/150\n",
      "595/595 [==============================] - 0s - loss: 0.0488 - val_loss: 0.0643\n",
      "Epoch 44/150\n",
      "595/595 [==============================] - 0s - loss: 0.0489 - val_loss: 0.0644\n",
      "Epoch 45/150\n",
      "595/595 [==============================] - 0s - loss: 0.0487 - val_loss: 0.0638\n",
      "Epoch 46/150\n",
      "595/595 [==============================] - 0s - loss: 0.0479 - val_loss: 0.0634\n",
      "Epoch 47/150\n",
      "595/595 [==============================] - 0s - loss: 0.0479 - val_loss: 0.0635\n",
      "Epoch 48/150\n",
      "595/595 [==============================] - 0s - loss: 0.0479 - val_loss: 0.0636\n",
      "Epoch 49/150\n",
      "595/595 [==============================] - 0s - loss: 0.0473 - val_loss: 0.0626\n",
      "Epoch 50/150\n",
      "595/595 [==============================] - 0s - loss: 0.0472 - val_loss: 0.0627\n",
      "Epoch 51/150\n",
      "595/595 [==============================] - 0s - loss: 0.0470 - val_loss: 0.0632\n",
      "Epoch 52/150\n",
      "595/595 [==============================] - 0s - loss: 0.0469 - val_loss: 0.0624\n",
      "Epoch 53/150\n",
      "595/595 [==============================] - 0s - loss: 0.0461 - val_loss: 0.0624\n",
      "Epoch 54/150\n",
      "595/595 [==============================] - 0s - loss: 0.0466 - val_loss: 0.0622\n",
      "Epoch 55/150\n",
      "595/595 [==============================] - 0s - loss: 0.0464 - val_loss: 0.0638\n",
      "Epoch 56/150\n",
      "595/595 [==============================] - 0s - loss: 0.0463 - val_loss: 0.0614\n",
      "Epoch 57/150\n",
      "595/595 [==============================] - 0s - loss: 0.0464 - val_loss: 0.0622\n",
      "Epoch 58/150\n",
      "595/595 [==============================] - 0s - loss: 0.0466 - val_loss: 0.0637\n",
      "Epoch 59/150\n",
      "595/595 [==============================] - 0s - loss: 0.0464 - val_loss: 0.0607\n",
      "Epoch 60/150\n",
      "595/595 [==============================] - 0s - loss: 0.0460 - val_loss: 0.0617\n",
      "Epoch 61/150\n",
      "595/595 [==============================] - 0s - loss: 0.0459 - val_loss: 0.0624\n",
      "Epoch 62/150\n",
      "595/595 [==============================] - 0s - loss: 0.0452 - val_loss: 0.0605\n",
      "Epoch 63/150\n",
      "595/595 [==============================] - 0s - loss: 0.0450 - val_loss: 0.0612\n",
      "Epoch 64/150\n",
      "595/595 [==============================] - 0s - loss: 0.0447 - val_loss: 0.0617\n",
      "Epoch 65/150\n",
      "595/595 [==============================] - 0s - loss: 0.0443 - val_loss: 0.0606\n",
      "Epoch 66/150\n",
      "595/595 [==============================] - 0s - loss: 0.0440 - val_loss: 0.0611\n",
      "Epoch 67/150\n",
      "595/595 [==============================] - 0s - loss: 0.0442 - val_loss: 0.0611\n",
      "Epoch 68/150\n",
      "595/595 [==============================] - 0s - loss: 0.0439 - val_loss: 0.0613\n",
      "Epoch 69/150\n",
      "595/595 [==============================] - 0s - loss: 0.0434 - val_loss: 0.0609\n",
      "Epoch 70/150\n",
      "595/595 [==============================] - 0s - loss: 0.0442 - val_loss: 0.0609\n",
      "Epoch 71/150\n",
      "595/595 [==============================] - 0s - loss: 0.0438 - val_loss: 0.0626\n",
      "Epoch 72/150\n",
      "595/595 [==============================] - 0s - loss: 0.0437 - val_loss: 0.0605\n",
      "Epoch 73/150\n",
      "595/595 [==============================] - 0s - loss: 0.0445 - val_loss: 0.0610\n",
      "Epoch 74/150\n",
      "595/595 [==============================] - 0s - loss: 0.0441 - val_loss: 0.0627\n",
      "Epoch 75/150\n",
      "595/595 [==============================] - 0s - loss: 0.0436 - val_loss: 0.0599\n",
      "Epoch 76/150\n",
      "595/595 [==============================] - 0s - loss: 0.0440 - val_loss: 0.0612\n",
      "Epoch 77/150\n",
      "595/595 [==============================] - 0s - loss: 0.0440 - val_loss: 0.0620\n",
      "Epoch 78/150\n",
      "595/595 [==============================] - 0s - loss: 0.0429 - val_loss: 0.0598\n",
      "Epoch 79/150\n",
      "595/595 [==============================] - 0s - loss: 0.0433 - val_loss: 0.0608\n",
      "Epoch 80/150\n",
      "595/595 [==============================] - 0s - loss: 0.0429 - val_loss: 0.0612\n",
      "Epoch 81/150\n",
      "595/595 [==============================] - 0s - loss: 0.0420 - val_loss: 0.0601\n",
      "Epoch 82/150\n",
      "595/595 [==============================] - 0s - loss: 0.0421 - val_loss: 0.0605\n",
      "Epoch 83/150\n",
      "595/595 [==============================] - 0s - loss: 0.0418 - val_loss: 0.0608\n",
      "Epoch 84/150\n",
      "595/595 [==============================] - 0s - loss: 0.0415 - val_loss: 0.0603\n",
      "Epoch 85/150\n",
      "595/595 [==============================] - 0s - loss: 0.0413 - val_loss: 0.0605\n",
      "Epoch 86/150\n",
      "595/595 [==============================] - 0s - loss: 0.0417 - val_loss: 0.0605\n",
      "Epoch 87/150\n",
      "595/595 [==============================] - 0s - loss: 0.0413 - val_loss: 0.0612\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595/595 [==============================] - 0s - loss: 0.0410 - val_loss: 0.0604\n",
      "Epoch 89/150\n",
      "595/595 [==============================] - 0s - loss: 0.0422 - val_loss: 0.0606\n",
      "Epoch 90/150\n",
      "595/595 [==============================] - 0s - loss: 0.0418 - val_loss: 0.0629\n",
      "Epoch 91/150\n",
      "595/595 [==============================] - 0s - loss: 0.0418 - val_loss: 0.0603\n",
      "Epoch 92/150\n",
      "595/595 [==============================] - 0s - loss: 0.0430 - val_loss: 0.0607\n",
      "Epoch 93/150\n",
      "595/595 [==============================] - 0s - loss: 0.0427 - val_loss: 0.0626\n",
      "Epoch 94/150\n",
      "595/595 [==============================] - 0s - loss: 0.0417 - val_loss: 0.0598\n",
      "Epoch 95/150\n",
      "595/595 [==============================] - 0s - loss: 0.0423 - val_loss: 0.0606\n",
      "Epoch 96/150\n",
      "595/595 [==============================] - 0s - loss: 0.0421 - val_loss: 0.0616\n",
      "Epoch 97/150\n",
      "595/595 [==============================] - 0s - loss: 0.0408 - val_loss: 0.0600\n",
      "Epoch 98/150\n",
      "595/595 [==============================] - 0s - loss: 0.0413 - val_loss: 0.0604\n",
      "Epoch 99/150\n",
      "595/595 [==============================] - 0s - loss: 0.0410 - val_loss: 0.0614\n",
      "Epoch 100/150\n",
      "595/595 [==============================] - 0s - loss: 0.0403 - val_loss: 0.0600\n",
      "Epoch 101/150\n",
      "595/595 [==============================] - 0s - loss: 0.0406 - val_loss: 0.0604\n",
      "Epoch 102/150\n",
      "595/595 [==============================] - 0s - loss: 0.0404 - val_loss: 0.0613\n",
      "Epoch 103/150\n",
      "595/595 [==============================] - 0s - loss: 0.0399 - val_loss: 0.0600\n",
      "Epoch 104/150\n",
      "595/595 [==============================] - 0s - loss: 0.0402 - val_loss: 0.0603\n",
      "Epoch 105/150\n",
      "595/595 [==============================] - 0s - loss: 0.0399 - val_loss: 0.0615\n",
      "Epoch 106/150\n",
      "595/595 [==============================] - 0s - loss: 0.0395 - val_loss: 0.0601\n",
      "Epoch 107/150\n",
      "595/595 [==============================] - 0s - loss: 0.0399 - val_loss: 0.0603\n",
      "Epoch 108/150\n",
      "595/595 [==============================] - 0s - loss: 0.0395 - val_loss: 0.0618\n",
      "Epoch 109/150\n",
      "595/595 [==============================] - 0s - loss: 0.0392 - val_loss: 0.0602\n",
      "Epoch 110/150\n",
      "595/595 [==============================] - 0s - loss: 0.0398 - val_loss: 0.0604\n",
      "Epoch 111/150\n",
      "595/595 [==============================] - 0s - loss: 0.0395 - val_loss: 0.0621\n",
      "Epoch 112/150\n",
      "595/595 [==============================] - 0s - loss: 0.0391 - val_loss: 0.0602\n",
      "Epoch 113/150\n",
      "595/595 [==============================] - 0s - loss: 0.0396 - val_loss: 0.0605\n",
      "Epoch 114/150\n",
      "595/595 [==============================] - 0s - loss: 0.0394 - val_loss: 0.0623\n",
      "Epoch 115/150\n",
      "595/595 [==============================] - 0s - loss: 0.0391 - val_loss: 0.0602\n",
      "Epoch 116/150\n",
      "595/595 [==============================] - 0s - loss: 0.0393 - val_loss: 0.0607\n",
      "Epoch 117/150\n",
      "595/595 [==============================] - 0s - loss: 0.0392 - val_loss: 0.0622\n",
      "Epoch 118/150\n",
      "595/595 [==============================] - 0s - loss: 0.0387 - val_loss: 0.0604\n",
      "Epoch 119/150\n",
      "595/595 [==============================] - 0s - loss: 0.0388 - val_loss: 0.0606\n",
      "Epoch 120/150\n",
      "595/595 [==============================] - 0s - loss: 0.0388 - val_loss: 0.0621\n",
      "Epoch 121/150\n",
      "595/595 [==============================] - 0s - loss: 0.0383 - val_loss: 0.0607\n",
      "Epoch 122/150\n",
      "595/595 [==============================] - 0s - loss: 0.0382 - val_loss: 0.0607\n",
      "Epoch 123/150\n",
      "595/595 [==============================] - 0s - loss: 0.0385 - val_loss: 0.0616\n",
      "Epoch 124/150\n",
      "595/595 [==============================] - 0s - loss: 0.0381 - val_loss: 0.0617\n",
      "Epoch 125/150\n",
      "595/595 [==============================] - 0s - loss: 0.0381 - val_loss: 0.0609\n",
      "Epoch 126/150\n",
      "595/595 [==============================] - 0s - loss: 0.0388 - val_loss: 0.0613\n",
      "Epoch 127/150\n",
      "595/595 [==============================] - 0s - loss: 0.0385 - val_loss: 0.0632\n",
      "Epoch 128/150\n",
      "595/595 [==============================] - 0s - loss: 0.0384 - val_loss: 0.0609\n",
      "Epoch 129/150\n",
      "595/595 [==============================] - 0s - loss: 0.0388 - val_loss: 0.0611\n",
      "Epoch 130/150\n",
      "595/595 [==============================] - 0s - loss: 0.0389 - val_loss: 0.0640\n",
      "Epoch 131/150\n",
      "595/595 [==============================] - 0s - loss: 0.0387 - val_loss: 0.0608\n",
      "Epoch 132/150\n",
      "595/595 [==============================] - 0s - loss: 0.0387 - val_loss: 0.0610\n",
      "Epoch 133/150\n",
      "595/595 [==============================] - 0s - loss: 0.0389 - val_loss: 0.0637\n",
      "Epoch 134/150\n",
      "595/595 [==============================] - 0s - loss: 0.0383 - val_loss: 0.0609\n",
      "Epoch 135/150\n",
      "595/595 [==============================] - 0s - loss: 0.0382 - val_loss: 0.0609\n",
      "Epoch 136/150\n",
      "595/595 [==============================] - 0s - loss: 0.0385 - val_loss: 0.0634\n",
      "Epoch 137/150\n",
      "595/595 [==============================] - 0s - loss: 0.0376 - val_loss: 0.0609\n",
      "Epoch 138/150\n",
      "595/595 [==============================] - 0s - loss: 0.0377 - val_loss: 0.0611\n",
      "Epoch 139/150\n",
      "595/595 [==============================] - 0s - loss: 0.0378 - val_loss: 0.0633\n",
      "Epoch 140/150\n",
      "595/595 [==============================] - 0s - loss: 0.0372 - val_loss: 0.0611\n",
      "Epoch 141/150\n",
      "595/595 [==============================] - 0s - loss: 0.0371 - val_loss: 0.0612\n",
      "Epoch 142/150\n",
      "595/595 [==============================] - 0s - loss: 0.0373 - val_loss: 0.0636\n",
      "Epoch 143/150\n",
      "595/595 [==============================] - 0s - loss: 0.0368 - val_loss: 0.0611\n",
      "Epoch 144/150\n",
      "595/595 [==============================] - 0s - loss: 0.0371 - val_loss: 0.0614\n",
      "Epoch 145/150\n",
      "595/595 [==============================] - 0s - loss: 0.0372 - val_loss: 0.0642\n",
      "Epoch 146/150\n",
      "595/595 [==============================] - 0s - loss: 0.0368 - val_loss: 0.0611\n",
      "Epoch 147/150\n",
      "595/595 [==============================] - 0s - loss: 0.0371 - val_loss: 0.0617\n",
      "Epoch 148/150\n",
      "595/595 [==============================] - 0s - loss: 0.0373 - val_loss: 0.0645\n",
      "Epoch 149/150\n",
      "595/595 [==============================] - 0s - loss: 0.0369 - val_loss: 0.0612\n",
      "Epoch 150/150\n",
      "595/595 [==============================] - 0s - loss: 0.0373 - val_loss: 0.0619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▍    | 12/22 [05:04<03:52, 23.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 595 samples, validate on 149 samples\n",
      "Epoch 1/150\n",
      "595/595 [==============================] - 5s - loss: 0.3580 - val_loss: 0.6004\n",
      "Epoch 2/150\n",
      "595/595 [==============================] - 0s - loss: 0.2532 - val_loss: 0.4733\n",
      "Epoch 3/150\n",
      "595/595 [==============================] - 0s - loss: 0.2085 - val_loss: 0.3814\n",
      "Epoch 4/150\n",
      "595/595 [==============================] - 0s - loss: 0.1925 - val_loss: 0.3283\n",
      "Epoch 5/150\n",
      "595/595 [==============================] - 0s - loss: 0.1816 - val_loss: 0.3018\n",
      "Epoch 6/150\n",
      "595/595 [==============================] - 0s - loss: 0.1696 - val_loss: 0.2870\n",
      "Epoch 7/150\n",
      "595/595 [==============================] - 0s - loss: 0.1592 - val_loss: 0.2742\n",
      "Epoch 8/150\n",
      "595/595 [==============================] - 0s - loss: 0.1496 - val_loss: 0.2585\n",
      "Epoch 9/150\n",
      "595/595 [==============================] - 0s - loss: 0.1406 - val_loss: 0.2386\n",
      "Epoch 10/150\n",
      "595/595 [==============================] - 0s - loss: 0.1320 - val_loss: 0.2152\n",
      "Epoch 11/150\n",
      "595/595 [==============================] - 0s - loss: 0.1238 - val_loss: 0.1917\n",
      "Epoch 12/150\n",
      "595/595 [==============================] - 0s - loss: 0.1160 - val_loss: 0.1715\n",
      "Epoch 13/150\n",
      "595/595 [==============================] - 0s - loss: 0.1089 - val_loss: 0.1553\n",
      "Epoch 14/150\n",
      "595/595 [==============================] - 0s - loss: 0.1023 - val_loss: 0.1412\n",
      "Epoch 15/150\n",
      "595/595 [==============================] - 0s - loss: 0.0961 - val_loss: 0.1292\n",
      "Epoch 16/150\n",
      "595/595 [==============================] - 0s - loss: 0.0907 - val_loss: 0.1192\n",
      "Epoch 17/150\n",
      "595/595 [==============================] - 0s - loss: 0.0860 - val_loss: 0.1112\n",
      "Epoch 18/150\n",
      "595/595 [==============================] - 0s - loss: 0.0818 - val_loss: 0.1046\n",
      "Epoch 19/150\n",
      "595/595 [==============================] - 0s - loss: 0.0781 - val_loss: 0.0993\n",
      "Epoch 20/150\n",
      "595/595 [==============================] - 0s - loss: 0.0750 - val_loss: 0.0950\n",
      "Epoch 21/150\n",
      "595/595 [==============================] - 0s - loss: 0.0724 - val_loss: 0.0915\n",
      "Epoch 22/150\n",
      "595/595 [==============================] - 0s - loss: 0.0701 - val_loss: 0.0883\n",
      "Epoch 23/150\n",
      "595/595 [==============================] - 0s - loss: 0.0682 - val_loss: 0.0853\n",
      "Epoch 24/150\n",
      "595/595 [==============================] - 0s - loss: 0.0665 - val_loss: 0.0825\n",
      "Epoch 25/150\n",
      "595/595 [==============================] - 0s - loss: 0.0652 - val_loss: 0.0802\n",
      "Epoch 26/150\n",
      "595/595 [==============================] - 0s - loss: 0.0641 - val_loss: 0.0783\n",
      "Epoch 27/150\n",
      "595/595 [==============================] - 0s - loss: 0.0632 - val_loss: 0.0765\n",
      "Epoch 28/150\n",
      "595/595 [==============================] - 0s - loss: 0.0623 - val_loss: 0.0749\n",
      "Epoch 29/150\n",
      "595/595 [==============================] - 0s - loss: 0.0615 - val_loss: 0.0734\n",
      "Epoch 30/150\n",
      "595/595 [==============================] - 0s - loss: 0.0608 - val_loss: 0.0721\n",
      "Epoch 31/150\n",
      "595/595 [==============================] - 0s - loss: 0.0601 - val_loss: 0.0709\n",
      "Epoch 32/150\n",
      "595/595 [==============================] - 0s - loss: 0.0595 - val_loss: 0.0698\n",
      "Epoch 33/150\n",
      "595/595 [==============================] - 0s - loss: 0.0590 - val_loss: 0.0688\n",
      "Epoch 34/150\n",
      "595/595 [==============================] - 0s - loss: 0.0585 - val_loss: 0.0679\n",
      "Epoch 35/150\n",
      "595/595 [==============================] - 0s - loss: 0.0581 - val_loss: 0.0670\n",
      "Epoch 36/150\n",
      "595/595 [==============================] - 0s - loss: 0.0577 - val_loss: 0.0662\n",
      "Epoch 37/150\n",
      "595/595 [==============================] - 0s - loss: 0.0574 - val_loss: 0.0654\n",
      "Epoch 38/150\n",
      "595/595 [==============================] - 0s - loss: 0.0569 - val_loss: 0.0648\n",
      "Epoch 39/150\n",
      "595/595 [==============================] - 0s - loss: 0.0566 - val_loss: 0.0641\n",
      "Epoch 40/150\n",
      "595/595 [==============================] - 0s - loss: 0.0563 - val_loss: 0.0635\n",
      "Epoch 41/150\n",
      "595/595 [==============================] - 0s - loss: 0.0560 - val_loss: 0.0630\n",
      "Epoch 42/150\n",
      "595/595 [==============================] - 0s - loss: 0.0557 - val_loss: 0.0626\n",
      "Epoch 43/150\n",
      "595/595 [==============================] - 0s - loss: 0.0554 - val_loss: 0.0621\n",
      "Epoch 44/150\n",
      "595/595 [==============================] - 0s - loss: 0.0551 - val_loss: 0.0617\n",
      "Epoch 45/150\n",
      "595/595 [==============================] - 0s - loss: 0.0548 - val_loss: 0.0613\n",
      "Epoch 46/150\n",
      "595/595 [==============================] - 0s - loss: 0.0546 - val_loss: 0.0610\n",
      "Epoch 47/150\n",
      "595/595 [==============================] - 0s - loss: 0.0543 - val_loss: 0.0607\n",
      "Epoch 48/150\n",
      "595/595 [==============================] - 0s - loss: 0.0541 - val_loss: 0.0603\n",
      "Epoch 49/150\n",
      "595/595 [==============================] - 0s - loss: 0.0538 - val_loss: 0.0601\n",
      "Epoch 50/150\n",
      "595/595 [==============================] - 0s - loss: 0.0536 - val_loss: 0.0599\n",
      "Epoch 51/150\n",
      "595/595 [==============================] - 0s - loss: 0.0533 - val_loss: 0.0595\n",
      "Epoch 52/150\n",
      "595/595 [==============================] - 0s - loss: 0.0531 - val_loss: 0.0594\n",
      "Epoch 53/150\n",
      "595/595 [==============================] - 0s - loss: 0.0529 - val_loss: 0.0591\n",
      "Epoch 54/150\n",
      "595/595 [==============================] - 0s - loss: 0.0526 - val_loss: 0.0589\n",
      "Epoch 55/150\n",
      "595/595 [==============================] - 0s - loss: 0.0524 - val_loss: 0.0588\n",
      "Epoch 56/150\n",
      "595/595 [==============================] - 0s - loss: 0.0522 - val_loss: 0.0586\n",
      "Epoch 57/150\n",
      "595/595 [==============================] - 0s - loss: 0.0519 - val_loss: 0.0584\n",
      "Epoch 58/150\n",
      "595/595 [==============================] - 0s - loss: 0.0517 - val_loss: 0.0583\n",
      "Epoch 59/150\n",
      "595/595 [==============================] - 0s - loss: 0.0515 - val_loss: 0.0580\n",
      "Epoch 60/150\n",
      "595/595 [==============================] - 0s - loss: 0.0512 - val_loss: 0.0580\n",
      "Epoch 61/150\n",
      "595/595 [==============================] - 0s - loss: 0.0512 - val_loss: 0.0578\n",
      "Epoch 62/150\n",
      "595/595 [==============================] - 0s - loss: 0.0507 - val_loss: 0.0576\n",
      "Epoch 63/150\n",
      "595/595 [==============================] - 0s - loss: 0.0507 - val_loss: 0.0576\n",
      "Epoch 64/150\n",
      "595/595 [==============================] - 0s - loss: 0.0504 - val_loss: 0.0573\n",
      "Epoch 65/150\n",
      "595/595 [==============================] - 0s - loss: 0.0501 - val_loss: 0.0573\n",
      "Epoch 66/150\n",
      "595/595 [==============================] - 0s - loss: 0.0500 - val_loss: 0.0572\n",
      "Epoch 67/150\n",
      "595/595 [==============================] - 0s - loss: 0.0496 - val_loss: 0.0570\n",
      "Epoch 68/150\n",
      "595/595 [==============================] - 0s - loss: 0.0495 - val_loss: 0.0569\n",
      "Epoch 69/150\n",
      "595/595 [==============================] - 0s - loss: 0.0492 - val_loss: 0.0567\n",
      "Epoch 70/150\n",
      "595/595 [==============================] - 0s - loss: 0.0489 - val_loss: 0.0567\n",
      "Epoch 71/150\n",
      "595/595 [==============================] - 0s - loss: 0.0491 - val_loss: 0.0565\n",
      "Epoch 72/150\n",
      "595/595 [==============================] - 0s - loss: 0.0483 - val_loss: 0.0564\n",
      "Epoch 73/150\n",
      "595/595 [==============================] - 0s - loss: 0.0487 - val_loss: 0.0565\n",
      "Epoch 74/150\n",
      "595/595 [==============================] - 0s - loss: 0.0480 - val_loss: 0.0561\n",
      "Epoch 75/150\n",
      "595/595 [==============================] - 0s - loss: 0.0480 - val_loss: 0.0562\n",
      "Epoch 76/150\n",
      "595/595 [==============================] - 0s - loss: 0.0478 - val_loss: 0.0560\n",
      "Epoch 77/150\n",
      "595/595 [==============================] - 0s - loss: 0.0473 - val_loss: 0.0559\n",
      "Epoch 78/150\n",
      "595/595 [==============================] - 0s - loss: 0.0475 - val_loss: 0.0559\n",
      "Epoch 79/150\n",
      "595/595 [==============================] - 0s - loss: 0.0470 - val_loss: 0.0557\n",
      "Epoch 80/150\n",
      "595/595 [==============================] - 0s - loss: 0.0469 - val_loss: 0.0558\n",
      "Epoch 81/150\n",
      "595/595 [==============================] - 0s - loss: 0.0469 - val_loss: 0.0555\n",
      "Epoch 82/150\n",
      "595/595 [==============================] - 0s - loss: 0.0461 - val_loss: 0.0556\n",
      "Epoch 83/150\n",
      "595/595 [==============================] - 0s - loss: 0.0469 - val_loss: 0.0556\n",
      "Epoch 84/150\n",
      "595/595 [==============================] - 0s - loss: 0.0456 - val_loss: 0.0552\n",
      "Epoch 85/150\n",
      "595/595 [==============================] - 0s - loss: 0.0463 - val_loss: 0.0554\n",
      "Epoch 86/150\n",
      "595/595 [==============================] - 0s - loss: 0.0458 - val_loss: 0.0552\n",
      "Epoch 87/150\n",
      "595/595 [==============================] - 0s - loss: 0.0452 - val_loss: 0.0551\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595/595 [==============================] - 0s - loss: 0.0458 - val_loss: 0.0552\n",
      "Epoch 89/150\n",
      "595/595 [==============================] - 0s - loss: 0.0447 - val_loss: 0.0551\n",
      "Epoch 90/150\n",
      "595/595 [==============================] - 0s - loss: 0.0450 - val_loss: 0.0552\n",
      "Epoch 91/150\n",
      "595/595 [==============================] - 0s - loss: 0.0451 - val_loss: 0.0552\n",
      "Epoch 92/150\n",
      "595/595 [==============================] - 0s - loss: 0.0439 - val_loss: 0.0550\n",
      "Epoch 93/150\n",
      "595/595 [==============================] - 0s - loss: 0.0450 - val_loss: 0.0553\n",
      "Epoch 94/150\n",
      "595/595 [==============================] - 0s - loss: 0.0437 - val_loss: 0.0553\n",
      "Epoch 95/150\n",
      "595/595 [==============================] - 0s - loss: 0.0441 - val_loss: 0.0552\n",
      "Epoch 96/150\n",
      "595/595 [==============================] - 0s - loss: 0.0443 - val_loss: 0.0554\n",
      "Epoch 97/150\n",
      "595/595 [==============================] - 0s - loss: 0.0428 - val_loss: 0.0550\n",
      "Epoch 98/150\n",
      "595/595 [==============================] - 0s - loss: 0.0441 - val_loss: 0.0553\n",
      "Epoch 99/150\n",
      "595/595 [==============================] - 0s - loss: 0.0428 - val_loss: 0.0555\n",
      "Epoch 100/150\n",
      "595/595 [==============================] - 0s - loss: 0.0430 - val_loss: 0.0553\n",
      "Epoch 101/150\n",
      "595/595 [==============================] - 0s - loss: 0.0435 - val_loss: 0.0555\n",
      "Epoch 102/150\n",
      "595/595 [==============================] - 0s - loss: 0.0417 - val_loss: 0.0552\n",
      "Epoch 103/150\n",
      "595/595 [==============================] - 0s - loss: 0.0433 - val_loss: 0.0555\n",
      "Epoch 104/150\n",
      "595/595 [==============================] - 0s - loss: 0.0418 - val_loss: 0.0558\n",
      "Epoch 105/150\n",
      "595/595 [==============================] - 0s - loss: 0.0420 - val_loss: 0.0555\n",
      "Epoch 106/150\n",
      "595/595 [==============================] - 0s - loss: 0.0427 - val_loss: 0.0560\n",
      "Epoch 107/150\n",
      "595/595 [==============================] - 0s - loss: 0.0408 - val_loss: 0.0556\n",
      "Epoch 108/150\n",
      "595/595 [==============================] - 0s - loss: 0.0424 - val_loss: 0.0558\n",
      "Epoch 109/150\n",
      "595/595 [==============================] - 0s - loss: 0.0411 - val_loss: 0.0563\n",
      "Epoch 110/150\n",
      "595/595 [==============================] - 0s - loss: 0.0410 - val_loss: 0.0559\n",
      "Epoch 111/150\n",
      "595/595 [==============================] - 0s - loss: 0.0420 - val_loss: 0.0565\n",
      "Epoch 112/150\n",
      "595/595 [==============================] - 0s - loss: 0.0397 - val_loss: 0.0561\n",
      "Epoch 113/150\n",
      "595/595 [==============================] - 0s - loss: 0.0416 - val_loss: 0.0563\n",
      "Epoch 114/150\n",
      "595/595 [==============================] - 0s - loss: 0.0402 - val_loss: 0.0571\n",
      "Epoch 115/150\n",
      "595/595 [==============================] - 0s - loss: 0.0401 - val_loss: 0.0565\n",
      "Epoch 116/150\n",
      "595/595 [==============================] - 0s - loss: 0.0415 - val_loss: 0.0571\n",
      "Epoch 117/150\n",
      "595/595 [==============================] - 0s - loss: 0.0387 - val_loss: 0.0567\n",
      "Epoch 118/150\n",
      "595/595 [==============================] - 0s - loss: 0.0411 - val_loss: 0.0569\n",
      "Epoch 119/150\n",
      "595/595 [==============================] - 0s - loss: 0.0394 - val_loss: 0.0577\n",
      "Epoch 120/150\n",
      "595/595 [==============================] - 0s - loss: 0.0393 - val_loss: 0.0570\n",
      "Epoch 121/150\n",
      "595/595 [==============================] - 0s - loss: 0.0407 - val_loss: 0.0578\n",
      "Epoch 122/150\n",
      "595/595 [==============================] - 0s - loss: 0.0379 - val_loss: 0.0573\n",
      "Epoch 123/150\n",
      "595/595 [==============================] - 0s - loss: 0.0403 - val_loss: 0.0576\n",
      "Epoch 124/150\n",
      "595/595 [==============================] - 0s - loss: 0.0385 - val_loss: 0.0583\n",
      "Epoch 125/150\n",
      "595/595 [==============================] - 0s - loss: 0.0386 - val_loss: 0.0576\n",
      "Epoch 126/150\n",
      "595/595 [==============================] - 0s - loss: 0.0401 - val_loss: 0.0585\n",
      "Epoch 127/150\n",
      "595/595 [==============================] - 0s - loss: 0.0371 - val_loss: 0.0579\n",
      "Epoch 128/150\n",
      "595/595 [==============================] - 0s - loss: 0.0399 - val_loss: 0.0582\n",
      "Epoch 129/150\n",
      "595/595 [==============================] - 0s - loss: 0.0378 - val_loss: 0.0590\n",
      "Epoch 130/150\n",
      "595/595 [==============================] - 0s - loss: 0.0380 - val_loss: 0.0581\n",
      "Epoch 131/150\n",
      "595/595 [==============================] - 0s - loss: 0.0396 - val_loss: 0.0591\n",
      "Epoch 132/150\n",
      "595/595 [==============================] - 0s - loss: 0.0363 - val_loss: 0.0584\n",
      "Epoch 133/150\n",
      "595/595 [==============================] - 0s - loss: 0.0396 - val_loss: 0.0588\n",
      "Epoch 134/150\n",
      "595/595 [==============================] - 0s - loss: 0.0369 - val_loss: 0.0594\n",
      "Epoch 135/150\n",
      "595/595 [==============================] - 0s - loss: 0.0376 - val_loss: 0.0587\n",
      "Epoch 136/150\n",
      "595/595 [==============================] - 0s - loss: 0.0384 - val_loss: 0.0598\n",
      "Epoch 137/150\n",
      "595/595 [==============================] - 0s - loss: 0.0359 - val_loss: 0.0590\n",
      "Epoch 138/150\n",
      "595/595 [==============================] - 0s - loss: 0.0391 - val_loss: 0.0595\n",
      "Epoch 139/150\n",
      "595/595 [==============================] - 0s - loss: 0.0360 - val_loss: 0.0600\n",
      "Epoch 140/150\n",
      "595/595 [==============================] - 0s - loss: 0.0376 - val_loss: 0.0592\n",
      "Epoch 141/150\n",
      "595/595 [==============================] - 0s - loss: 0.0380 - val_loss: 0.0607\n",
      "Epoch 142/150\n",
      "595/595 [==============================] - 0s - loss: 0.0356 - val_loss: 0.0593\n",
      "Epoch 143/150\n",
      "595/595 [==============================] - 0s - loss: 0.0394 - val_loss: 0.0606\n",
      "Epoch 144/150\n",
      "595/595 [==============================] - 0s - loss: 0.0351 - val_loss: 0.0601\n",
      "Epoch 145/150\n",
      "595/595 [==============================] - 0s - loss: 0.0382 - val_loss: 0.0599\n",
      "Epoch 146/150\n",
      "595/595 [==============================] - 0s - loss: 0.0363 - val_loss: 0.0610\n",
      "Epoch 147/150\n",
      "595/595 [==============================] - 0s - loss: 0.0360 - val_loss: 0.0598\n",
      "Epoch 148/150\n",
      "595/595 [==============================] - 0s - loss: 0.0381 - val_loss: 0.0611\n",
      "Epoch 149/150\n",
      "595/595 [==============================] - 0s - loss: 0.0346 - val_loss: 0.0603\n",
      "Epoch 150/150\n",
      "595/595 [==============================] - 0s - loss: 0.0379 - val_loss: 0.0604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▉    | 13/22 [05:25<03:23, 22.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 595 samples, validate on 149 samples\n",
      "Epoch 1/150\n",
      "595/595 [==============================] - 5s - loss: 0.4497 - val_loss: 0.7337\n",
      "Epoch 2/150\n",
      "595/595 [==============================] - 0s - loss: 0.3468 - val_loss: 0.5974\n",
      "Epoch 3/150\n",
      "595/595 [==============================] - 0s - loss: 0.2795 - val_loss: 0.4749\n",
      "Epoch 4/150\n",
      "595/595 [==============================] - 0s - loss: 0.2360 - val_loss: 0.3813\n",
      "Epoch 5/150\n",
      "595/595 [==============================] - 0s - loss: 0.2117 - val_loss: 0.3219\n",
      "Epoch 6/150\n",
      "595/595 [==============================] - 0s - loss: 0.1987 - val_loss: 0.2907\n",
      "Epoch 7/150\n",
      "595/595 [==============================] - 0s - loss: 0.1885 - val_loss: 0.2742\n",
      "Epoch 8/150\n",
      "595/595 [==============================] - 0s - loss: 0.1791 - val_loss: 0.2626\n",
      "Epoch 9/150\n",
      "595/595 [==============================] - 0s - loss: 0.1704 - val_loss: 0.2520\n",
      "Epoch 10/150\n",
      "595/595 [==============================] - 0s - loss: 0.1626 - val_loss: 0.2416\n",
      "Epoch 11/150\n",
      "595/595 [==============================] - 0s - loss: 0.1553 - val_loss: 0.2299\n",
      "Epoch 12/150\n",
      "595/595 [==============================] - 0s - loss: 0.1483 - val_loss: 0.2157\n",
      "Epoch 13/150\n",
      "595/595 [==============================] - 0s - loss: 0.1420 - val_loss: 0.2002\n",
      "Epoch 14/150\n",
      "595/595 [==============================] - 0s - loss: 0.1364 - val_loss: 0.1860\n",
      "Epoch 15/150\n",
      "595/595 [==============================] - 0s - loss: 0.1313 - val_loss: 0.1742\n",
      "Epoch 16/150\n",
      "595/595 [==============================] - 0s - loss: 0.1261 - val_loss: 0.1641\n",
      "Epoch 17/150\n",
      "595/595 [==============================] - 0s - loss: 0.1209 - val_loss: 0.1546\n",
      "Epoch 18/150\n",
      "595/595 [==============================] - 0s - loss: 0.1157 - val_loss: 0.1453\n",
      "Epoch 19/150\n",
      "595/595 [==============================] - 0s - loss: 0.1105 - val_loss: 0.1363\n",
      "Epoch 20/150\n",
      "595/595 [==============================] - 0s - loss: 0.1053 - val_loss: 0.1276\n",
      "Epoch 21/150\n",
      "595/595 [==============================] - 0s - loss: 0.1002 - val_loss: 0.1193\n",
      "Epoch 22/150\n",
      "595/595 [==============================] - 0s - loss: 0.0953 - val_loss: 0.1115\n",
      "Epoch 23/150\n",
      "595/595 [==============================] - 0s - loss: 0.0908 - val_loss: 0.1046\n",
      "Epoch 24/150\n",
      "595/595 [==============================] - 0s - loss: 0.0866 - val_loss: 0.0988\n",
      "Epoch 25/150\n",
      "595/595 [==============================] - 0s - loss: 0.0831 - val_loss: 0.0940\n",
      "Epoch 26/150\n",
      "595/595 [==============================] - 0s - loss: 0.0796 - val_loss: 0.0897\n",
      "Epoch 27/150\n",
      "595/595 [==============================] - 0s - loss: 0.0769 - val_loss: 0.0859\n",
      "Epoch 28/150\n",
      "595/595 [==============================] - 0s - loss: 0.0742 - val_loss: 0.0820\n",
      "Epoch 29/150\n",
      "595/595 [==============================] - 0s - loss: 0.0719 - val_loss: 0.0787\n",
      "Epoch 30/150\n",
      "595/595 [==============================] - 0s - loss: 0.0698 - val_loss: 0.0759\n",
      "Epoch 31/150\n",
      "595/595 [==============================] - 0s - loss: 0.0678 - val_loss: 0.0734\n",
      "Epoch 32/150\n",
      "595/595 [==============================] - 0s - loss: 0.0660 - val_loss: 0.0712\n",
      "Epoch 33/150\n",
      "595/595 [==============================] - 0s - loss: 0.0645 - val_loss: 0.0699\n",
      "Epoch 34/150\n",
      "595/595 [==============================] - 0s - loss: 0.0628 - val_loss: 0.0685\n",
      "Epoch 35/150\n",
      "595/595 [==============================] - 0s - loss: 0.0617 - val_loss: 0.0680\n",
      "Epoch 36/150\n",
      "595/595 [==============================] - 0s - loss: 0.0603 - val_loss: 0.0673\n",
      "Epoch 37/150\n",
      "595/595 [==============================] - 0s - loss: 0.0595 - val_loss: 0.0669\n",
      "Epoch 38/150\n",
      "595/595 [==============================] - 0s - loss: 0.0585 - val_loss: 0.0666\n",
      "Epoch 39/150\n",
      "595/595 [==============================] - 0s - loss: 0.0577 - val_loss: 0.0663\n",
      "Epoch 40/150\n",
      "595/595 [==============================] - 0s - loss: 0.0569 - val_loss: 0.0661\n",
      "Epoch 41/150\n",
      "595/595 [==============================] - 0s - loss: 0.0564 - val_loss: 0.0661\n",
      "Epoch 42/150\n",
      "595/595 [==============================] - 0s - loss: 0.0556 - val_loss: 0.0661\n",
      "Epoch 43/150\n",
      "595/595 [==============================] - 0s - loss: 0.0552 - val_loss: 0.0662\n",
      "Epoch 44/150\n",
      "595/595 [==============================] - 0s - loss: 0.0544 - val_loss: 0.0661\n",
      "Epoch 45/150\n",
      "595/595 [==============================] - 0s - loss: 0.0543 - val_loss: 0.0663\n",
      "Epoch 46/150\n",
      "595/595 [==============================] - 0s - loss: 0.0536 - val_loss: 0.0662\n",
      "Epoch 47/150\n",
      "595/595 [==============================] - 0s - loss: 0.0533 - val_loss: 0.0663\n",
      "Epoch 48/150\n",
      "595/595 [==============================] - 0s - loss: 0.0528 - val_loss: 0.0662\n",
      "Epoch 49/150\n",
      "595/595 [==============================] - 0s - loss: 0.0525 - val_loss: 0.0664\n",
      "Epoch 50/150\n",
      "595/595 [==============================] - 0s - loss: 0.0521 - val_loss: 0.0665\n",
      "Epoch 51/150\n",
      "595/595 [==============================] - 0s - loss: 0.0517 - val_loss: 0.0664\n",
      "Epoch 52/150\n",
      "595/595 [==============================] - 0s - loss: 0.0516 - val_loss: 0.0669\n",
      "Epoch 53/150\n",
      "595/595 [==============================] - 0s - loss: 0.0510 - val_loss: 0.0666\n",
      "Epoch 54/150\n",
      "595/595 [==============================] - 0s - loss: 0.0511 - val_loss: 0.0672\n",
      "Epoch 55/150\n",
      "595/595 [==============================] - 0s - loss: 0.0504 - val_loss: 0.0668\n",
      "Epoch 56/150\n",
      "595/595 [==============================] - 0s - loss: 0.0506 - val_loss: 0.0675\n",
      "Epoch 57/150\n",
      "595/595 [==============================] - 0s - loss: 0.0498 - val_loss: 0.0670\n",
      "Epoch 58/150\n",
      "595/595 [==============================] - 0s - loss: 0.0501 - val_loss: 0.0677\n",
      "Epoch 59/150\n",
      "595/595 [==============================] - 0s - loss: 0.0494 - val_loss: 0.0673\n",
      "Epoch 60/150\n",
      "595/595 [==============================] - 0s - loss: 0.0496 - val_loss: 0.0680\n",
      "Epoch 61/150\n",
      "595/595 [==============================] - 0s - loss: 0.0489 - val_loss: 0.0676\n",
      "Epoch 62/150\n",
      "595/595 [==============================] - 0s - loss: 0.0491 - val_loss: 0.0681\n",
      "Epoch 63/150\n",
      "595/595 [==============================] - 0s - loss: 0.0485 - val_loss: 0.0680\n",
      "Epoch 64/150\n",
      "595/595 [==============================] - 0s - loss: 0.0485 - val_loss: 0.0684\n",
      "Epoch 65/150\n",
      "595/595 [==============================] - 0s - loss: 0.0482 - val_loss: 0.0683\n",
      "Epoch 66/150\n",
      "595/595 [==============================] - 0s - loss: 0.0482 - val_loss: 0.0688\n",
      "Epoch 67/150\n",
      "595/595 [==============================] - 0s - loss: 0.0476 - val_loss: 0.0685\n",
      "Epoch 68/150\n",
      "595/595 [==============================] - 0s - loss: 0.0479 - val_loss: 0.0690\n",
      "Epoch 69/150\n",
      "595/595 [==============================] - 0s - loss: 0.0473 - val_loss: 0.0689\n",
      "Epoch 70/150\n",
      "595/595 [==============================] - 0s - loss: 0.0473 - val_loss: 0.0692\n",
      "Epoch 71/150\n",
      "595/595 [==============================] - 0s - loss: 0.0471 - val_loss: 0.0692\n",
      "Epoch 72/150\n",
      "595/595 [==============================] - 0s - loss: 0.0470 - val_loss: 0.0696\n",
      "Epoch 73/150\n",
      "595/595 [==============================] - 0s - loss: 0.0466 - val_loss: 0.0694\n",
      "Epoch 74/150\n",
      "595/595 [==============================] - 0s - loss: 0.0466 - val_loss: 0.0697\n",
      "Epoch 75/150\n",
      "595/595 [==============================] - 0s - loss: 0.0463 - val_loss: 0.0698\n",
      "Epoch 76/150\n",
      "595/595 [==============================] - 0s - loss: 0.0462 - val_loss: 0.0699\n",
      "Epoch 77/150\n",
      "595/595 [==============================] - 0s - loss: 0.0461 - val_loss: 0.0701\n",
      "Epoch 78/150\n",
      "595/595 [==============================] - 0s - loss: 0.0458 - val_loss: 0.0703\n",
      "Epoch 79/150\n",
      "595/595 [==============================] - 0s - loss: 0.0458 - val_loss: 0.0700\n",
      "Epoch 80/150\n",
      "595/595 [==============================] - 0s - loss: 0.0458 - val_loss: 0.0708\n",
      "Epoch 81/150\n",
      "595/595 [==============================] - 0s - loss: 0.0451 - val_loss: 0.0703\n",
      "Epoch 82/150\n",
      "595/595 [==============================] - 0s - loss: 0.0458 - val_loss: 0.0706\n",
      "Epoch 83/150\n",
      "595/595 [==============================] - 0s - loss: 0.0451 - val_loss: 0.0711\n",
      "Epoch 84/150\n",
      "595/595 [==============================] - 0s - loss: 0.0448 - val_loss: 0.0706\n",
      "Epoch 85/150\n",
      "595/595 [==============================] - 0s - loss: 0.0453 - val_loss: 0.0708\n",
      "Epoch 86/150\n",
      "595/595 [==============================] - 0s - loss: 0.0446 - val_loss: 0.0716\n",
      "Epoch 87/150\n",
      "595/595 [==============================] - 0s - loss: 0.0443 - val_loss: 0.0704\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595/595 [==============================] - 0s - loss: 0.0455 - val_loss: 0.0715\n",
      "Epoch 89/150\n",
      "595/595 [==============================] - 0s - loss: 0.0438 - val_loss: 0.0717\n",
      "Epoch 90/150\n",
      "595/595 [==============================] - 0s - loss: 0.0444 - val_loss: 0.0707\n",
      "Epoch 91/150\n",
      "595/595 [==============================] - 0s - loss: 0.0449 - val_loss: 0.0718\n",
      "Epoch 92/150\n",
      "595/595 [==============================] - 0s - loss: 0.0433 - val_loss: 0.0717\n",
      "Epoch 93/150\n",
      "595/595 [==============================] - 0s - loss: 0.0441 - val_loss: 0.0709\n",
      "Epoch 94/150\n",
      "595/595 [==============================] - 0s - loss: 0.0446 - val_loss: 0.0725\n",
      "Epoch 95/150\n",
      "595/595 [==============================] - 0s - loss: 0.0428 - val_loss: 0.0714\n",
      "Epoch 96/150\n",
      "595/595 [==============================] - 0s - loss: 0.0442 - val_loss: 0.0714\n",
      "Epoch 97/150\n",
      "595/595 [==============================] - 0s - loss: 0.0438 - val_loss: 0.0728\n",
      "Epoch 98/150\n",
      "595/595 [==============================] - 0s - loss: 0.0426 - val_loss: 0.0715\n",
      "Epoch 99/150\n",
      "595/595 [==============================] - 0s - loss: 0.0440 - val_loss: 0.0718\n",
      "Epoch 100/150\n",
      "595/595 [==============================] - 0s - loss: 0.0431 - val_loss: 0.0734\n",
      "Epoch 101/150\n",
      "595/595 [==============================] - 0s - loss: 0.0424 - val_loss: 0.0715\n",
      "Epoch 102/150\n",
      "595/595 [==============================] - 0s - loss: 0.0442 - val_loss: 0.0725\n",
      "Epoch 103/150\n",
      "595/595 [==============================] - 0s - loss: 0.0425 - val_loss: 0.0734\n",
      "Epoch 104/150\n",
      "595/595 [==============================] - 0s - loss: 0.0422 - val_loss: 0.0716\n",
      "Epoch 105/150\n",
      "595/595 [==============================] - 0s - loss: 0.0439 - val_loss: 0.0725\n",
      "Epoch 106/150\n",
      "595/595 [==============================] - 0s - loss: 0.0420 - val_loss: 0.0739\n",
      "Epoch 107/150\n",
      "595/595 [==============================] - 0s - loss: 0.0421 - val_loss: 0.0716\n",
      "Epoch 108/150\n",
      "595/595 [==============================] - 0s - loss: 0.0440 - val_loss: 0.0732\n",
      "Epoch 109/150\n",
      "595/595 [==============================] - 0s - loss: 0.0414 - val_loss: 0.0738\n",
      "Epoch 110/150\n",
      "595/595 [==============================] - 0s - loss: 0.0423 - val_loss: 0.0718\n",
      "Epoch 111/150\n",
      "595/595 [==============================] - 0s - loss: 0.0435 - val_loss: 0.0733\n",
      "Epoch 112/150\n",
      "595/595 [==============================] - 0s - loss: 0.0410 - val_loss: 0.0743\n",
      "Epoch 113/150\n",
      "595/595 [==============================] - 0s - loss: 0.0421 - val_loss: 0.0718\n",
      "Epoch 114/150\n",
      "595/595 [==============================] - 0s - loss: 0.0436 - val_loss: 0.0742\n",
      "Epoch 115/150\n",
      "595/595 [==============================] - 0s - loss: 0.0403 - val_loss: 0.0734\n",
      "Epoch 116/150\n",
      "595/595 [==============================] - 0s - loss: 0.0424 - val_loss: 0.0721\n",
      "Epoch 117/150\n",
      "595/595 [==============================] - 0s - loss: 0.0430 - val_loss: 0.0743\n",
      "Epoch 118/150\n",
      "595/595 [==============================] - 0s - loss: 0.0400 - val_loss: 0.0737\n",
      "Epoch 119/150\n",
      "595/595 [==============================] - 0s - loss: 0.0421 - val_loss: 0.0722\n",
      "Epoch 120/150\n",
      "595/595 [==============================] - 0s - loss: 0.0430 - val_loss: 0.0750\n",
      "Epoch 121/150\n",
      "595/595 [==============================] - 0s - loss: 0.0395 - val_loss: 0.0735\n",
      "Epoch 122/150\n",
      "595/595 [==============================] - 0s - loss: 0.0426 - val_loss: 0.0724\n",
      "Epoch 123/150\n",
      "595/595 [==============================] - 0s - loss: 0.0427 - val_loss: 0.0753\n",
      "Epoch 124/150\n",
      "595/595 [==============================] - 0s - loss: 0.0392 - val_loss: 0.0735\n",
      "Epoch 125/150\n",
      "595/595 [==============================] - 0s - loss: 0.0425 - val_loss: 0.0726\n",
      "Epoch 126/150\n",
      "595/595 [==============================] - 0s - loss: 0.0425 - val_loss: 0.0757\n",
      "Epoch 127/150\n",
      "595/595 [==============================] - 0s - loss: 0.0390 - val_loss: 0.0732\n",
      "Epoch 128/150\n",
      "595/595 [==============================] - 0s - loss: 0.0427 - val_loss: 0.0728\n",
      "Epoch 129/150\n",
      "595/595 [==============================] - 0s - loss: 0.0418 - val_loss: 0.0756\n",
      "Epoch 130/150\n",
      "595/595 [==============================] - 0s - loss: 0.0389 - val_loss: 0.0730\n",
      "Epoch 131/150\n",
      "595/595 [==============================] - 0s - loss: 0.0423 - val_loss: 0.0731\n",
      "Epoch 132/150\n",
      "595/595 [==============================] - 0s - loss: 0.0413 - val_loss: 0.0760\n",
      "Epoch 133/150\n",
      "595/595 [==============================] - 0s - loss: 0.0388 - val_loss: 0.0729\n",
      "Epoch 134/150\n",
      "595/595 [==============================] - 0s - loss: 0.0424 - val_loss: 0.0736\n",
      "Epoch 135/150\n",
      "595/595 [==============================] - 0s - loss: 0.0407 - val_loss: 0.0761\n",
      "Epoch 136/150\n",
      "595/595 [==============================] - 0s - loss: 0.0389 - val_loss: 0.0726\n",
      "Epoch 137/150\n",
      "595/595 [==============================] - 0s - loss: 0.0425 - val_loss: 0.0740\n",
      "Epoch 138/150\n",
      "595/595 [==============================] - 0s - loss: 0.0399 - val_loss: 0.0762\n",
      "Epoch 139/150\n",
      "595/595 [==============================] - 0s - loss: 0.0393 - val_loss: 0.0727\n",
      "Epoch 140/150\n",
      "595/595 [==============================] - 0s - loss: 0.0427 - val_loss: 0.0744\n",
      "Epoch 141/150\n",
      "595/595 [==============================] - 0s - loss: 0.0392 - val_loss: 0.0762\n",
      "Epoch 142/150\n",
      "595/595 [==============================] - 0s - loss: 0.0399 - val_loss: 0.0725\n",
      "Epoch 143/150\n",
      "595/595 [==============================] - 0s - loss: 0.0426 - val_loss: 0.0749\n",
      "Epoch 144/150\n",
      "595/595 [==============================] - 0s - loss: 0.0385 - val_loss: 0.0757\n",
      "Epoch 145/150\n",
      "595/595 [==============================] - 0s - loss: 0.0399 - val_loss: 0.0726\n",
      "Epoch 146/150\n",
      "595/595 [==============================] - 0s - loss: 0.0421 - val_loss: 0.0755\n",
      "Epoch 147/150\n",
      "595/595 [==============================] - 0s - loss: 0.0379 - val_loss: 0.0750\n",
      "Epoch 148/150\n",
      "595/595 [==============================] - 0s - loss: 0.0400 - val_loss: 0.0728\n",
      "Epoch 149/150\n",
      "595/595 [==============================] - 0s - loss: 0.0415 - val_loss: 0.0756\n",
      "Epoch 150/150\n",
      "595/595 [==============================] - 0s - loss: 0.0376 - val_loss: 0.0752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▎   | 14/22 [05:46<02:56, 22.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 595 samples, validate on 149 samples\n",
      "Epoch 1/150\n",
      "595/595 [==============================] - 5s - loss: 0.3362 - val_loss: 0.5775\n",
      "Epoch 2/150\n",
      "595/595 [==============================] - 0s - loss: 0.2481 - val_loss: 0.4547\n",
      "Epoch 3/150\n",
      "595/595 [==============================] - 0s - loss: 0.1987 - val_loss: 0.3636\n",
      "Epoch 4/150\n",
      "595/595 [==============================] - 0s - loss: 0.1760 - val_loss: 0.3129\n",
      "Epoch 5/150\n",
      "595/595 [==============================] - 0s - loss: 0.1679 - val_loss: 0.2941\n",
      "Epoch 6/150\n",
      "595/595 [==============================] - 0s - loss: 0.1621 - val_loss: 0.2926\n",
      "Epoch 7/150\n",
      "595/595 [==============================] - 0s - loss: 0.1564 - val_loss: 0.2941\n",
      "Epoch 8/150\n",
      "595/595 [==============================] - 0s - loss: 0.1512 - val_loss: 0.2911\n",
      "Epoch 9/150\n",
      "595/595 [==============================] - 0s - loss: 0.1468 - val_loss: 0.2822\n",
      "Epoch 10/150\n",
      "595/595 [==============================] - 0s - loss: 0.1429 - val_loss: 0.2695\n",
      "Epoch 11/150\n",
      "595/595 [==============================] - 0s - loss: 0.1393 - val_loss: 0.2550\n",
      "Epoch 12/150\n",
      "595/595 [==============================] - 0s - loss: 0.1358 - val_loss: 0.2405\n",
      "Epoch 13/150\n",
      "595/595 [==============================] - 0s - loss: 0.1326 - val_loss: 0.2279\n",
      "Epoch 14/150\n",
      "595/595 [==============================] - 0s - loss: 0.1296 - val_loss: 0.2171\n",
      "Epoch 15/150\n",
      "595/595 [==============================] - 0s - loss: 0.1267 - val_loss: 0.2072\n",
      "Epoch 16/150\n",
      "595/595 [==============================] - 0s - loss: 0.1237 - val_loss: 0.1977\n",
      "Epoch 17/150\n",
      "595/595 [==============================] - 0s - loss: 0.1210 - val_loss: 0.1891\n",
      "Epoch 18/150\n",
      "595/595 [==============================] - 0s - loss: 0.1184 - val_loss: 0.1811\n",
      "Epoch 19/150\n",
      "595/595 [==============================] - 0s - loss: 0.1158 - val_loss: 0.1728\n",
      "Epoch 20/150\n",
      "595/595 [==============================] - 0s - loss: 0.1135 - val_loss: 0.1651\n",
      "Epoch 21/150\n",
      "595/595 [==============================] - 0s - loss: 0.1112 - val_loss: 0.1573\n",
      "Epoch 22/150\n",
      "595/595 [==============================] - 0s - loss: 0.1092 - val_loss: 0.1510\n",
      "Epoch 23/150\n",
      "595/595 [==============================] - 0s - loss: 0.1072 - val_loss: 0.1446\n",
      "Epoch 24/150\n",
      "595/595 [==============================] - 0s - loss: 0.1054 - val_loss: 0.1390\n",
      "Epoch 25/150\n",
      "595/595 [==============================] - 0s - loss: 0.1036 - val_loss: 0.1334\n",
      "Epoch 26/150\n",
      "595/595 [==============================] - 0s - loss: 0.1021 - val_loss: 0.1290\n",
      "Epoch 27/150\n",
      "595/595 [==============================] - 0s - loss: 0.1005 - val_loss: 0.1246\n",
      "Epoch 28/150\n",
      "595/595 [==============================] - 0s - loss: 0.0991 - val_loss: 0.1206\n",
      "Epoch 29/150\n",
      "595/595 [==============================] - 0s - loss: 0.0977 - val_loss: 0.1168\n",
      "Epoch 30/150\n",
      "595/595 [==============================] - 0s - loss: 0.0964 - val_loss: 0.1132\n",
      "Epoch 31/150\n",
      "595/595 [==============================] - 0s - loss: 0.0952 - val_loss: 0.1100\n",
      "Epoch 32/150\n",
      "595/595 [==============================] - 0s - loss: 0.0940 - val_loss: 0.1070\n",
      "Epoch 33/150\n",
      "595/595 [==============================] - 0s - loss: 0.0930 - val_loss: 0.1043\n",
      "Epoch 34/150\n",
      "595/595 [==============================] - 0s - loss: 0.0918 - val_loss: 0.1014\n",
      "Epoch 35/150\n",
      "595/595 [==============================] - 0s - loss: 0.0909 - val_loss: 0.0990\n",
      "Epoch 36/150\n",
      "595/595 [==============================] - 0s - loss: 0.0899 - val_loss: 0.0969\n",
      "Epoch 37/150\n",
      "595/595 [==============================] - 0s - loss: 0.0891 - val_loss: 0.0949\n",
      "Epoch 38/150\n",
      "595/595 [==============================] - 0s - loss: 0.0882 - val_loss: 0.0931\n",
      "Epoch 39/150\n",
      "595/595 [==============================] - 0s - loss: 0.0874 - val_loss: 0.0917\n",
      "Epoch 40/150\n",
      "595/595 [==============================] - 0s - loss: 0.0865 - val_loss: 0.0902\n",
      "Epoch 41/150\n",
      "595/595 [==============================] - 0s - loss: 0.0859 - val_loss: 0.0886\n",
      "Epoch 42/150\n",
      "595/595 [==============================] - 0s - loss: 0.0850 - val_loss: 0.0875\n",
      "Epoch 43/150\n",
      "595/595 [==============================] - 0s - loss: 0.0844 - val_loss: 0.0863\n",
      "Epoch 44/150\n",
      "595/595 [==============================] - 0s - loss: 0.0839 - val_loss: 0.0855\n",
      "Epoch 45/150\n",
      "595/595 [==============================] - 0s - loss: 0.0829 - val_loss: 0.0846\n",
      "Epoch 46/150\n",
      "595/595 [==============================] - 0s - loss: 0.0825 - val_loss: 0.0841\n",
      "Epoch 47/150\n",
      "595/595 [==============================] - 0s - loss: 0.0819 - val_loss: 0.0834\n",
      "Epoch 48/150\n",
      "595/595 [==============================] - 0s - loss: 0.0813 - val_loss: 0.0830\n",
      "Epoch 49/150\n",
      "595/595 [==============================] - 0s - loss: 0.0809 - val_loss: 0.0828\n",
      "Epoch 50/150\n",
      "595/595 [==============================] - 0s - loss: 0.0803 - val_loss: 0.0824\n",
      "Epoch 51/150\n",
      "595/595 [==============================] - 0s - loss: 0.0797 - val_loss: 0.0823\n",
      "Epoch 52/150\n",
      "595/595 [==============================] - 0s - loss: 0.0795 - val_loss: 0.0819\n",
      "Epoch 53/150\n",
      "595/595 [==============================] - 0s - loss: 0.0789 - val_loss: 0.0817\n",
      "Epoch 54/150\n",
      "595/595 [==============================] - 0s - loss: 0.0784 - val_loss: 0.0815\n",
      "Epoch 55/150\n",
      "595/595 [==============================] - 0s - loss: 0.0782 - val_loss: 0.0815\n",
      "Epoch 56/150\n",
      "595/595 [==============================] - 0s - loss: 0.0777 - val_loss: 0.0815\n",
      "Epoch 57/150\n",
      "595/595 [==============================] - 0s - loss: 0.0769 - val_loss: 0.0812\n",
      "Epoch 58/150\n",
      "595/595 [==============================] - 0s - loss: 0.0774 - val_loss: 0.0814\n",
      "Epoch 59/150\n",
      "595/595 [==============================] - 0s - loss: 0.0763 - val_loss: 0.0811\n",
      "Epoch 60/150\n",
      "595/595 [==============================] - 0s - loss: 0.0761 - val_loss: 0.0817\n",
      "Epoch 61/150\n",
      "595/595 [==============================] - 0s - loss: 0.0763 - val_loss: 0.0812\n",
      "Epoch 62/150\n",
      "595/595 [==============================] - 0s - loss: 0.0752 - val_loss: 0.0819\n",
      "Epoch 63/150\n",
      "595/595 [==============================] - 0s - loss: 0.0752 - val_loss: 0.0816\n",
      "Epoch 64/150\n",
      "595/595 [==============================] - 0s - loss: 0.0756 - val_loss: 0.0822\n",
      "Epoch 65/150\n",
      "595/595 [==============================] - 0s - loss: 0.0743 - val_loss: 0.0816\n",
      "Epoch 66/150\n",
      "595/595 [==============================] - 0s - loss: 0.0746 - val_loss: 0.0829\n",
      "Epoch 67/150\n",
      "595/595 [==============================] - 0s - loss: 0.0745 - val_loss: 0.0821\n",
      "Epoch 68/150\n",
      "595/595 [==============================] - 0s - loss: 0.0738 - val_loss: 0.0832\n",
      "Epoch 69/150\n",
      "595/595 [==============================] - 0s - loss: 0.0744 - val_loss: 0.0827\n",
      "Epoch 70/150\n",
      "595/595 [==============================] - 0s - loss: 0.0739 - val_loss: 0.0830\n",
      "Epoch 71/150\n",
      "595/595 [==============================] - 0s - loss: 0.0728 - val_loss: 0.0827\n",
      "Epoch 72/150\n",
      "595/595 [==============================] - 0s - loss: 0.0737 - val_loss: 0.0835\n",
      "Epoch 73/150\n",
      "595/595 [==============================] - 0s - loss: 0.0728 - val_loss: 0.0833\n",
      "Epoch 74/150\n",
      "595/595 [==============================] - 0s - loss: 0.0720 - val_loss: 0.0840\n",
      "Epoch 75/150\n",
      "595/595 [==============================] - 0s - loss: 0.0737 - val_loss: 0.0842\n",
      "Epoch 76/150\n",
      "595/595 [==============================] - 0s - loss: 0.0714 - val_loss: 0.0842\n",
      "Epoch 77/150\n",
      "595/595 [==============================] - 0s - loss: 0.0721 - val_loss: 0.0850\n",
      "Epoch 78/150\n",
      "595/595 [==============================] - 0s - loss: 0.0735 - val_loss: 0.0846\n",
      "Epoch 79/150\n",
      "595/595 [==============================] - 0s - loss: 0.0702 - val_loss: 0.0844\n",
      "Epoch 80/150\n",
      "595/595 [==============================] - 0s - loss: 0.0719 - val_loss: 0.0860\n",
      "Epoch 81/150\n",
      "595/595 [==============================] - 0s - loss: 0.0735 - val_loss: 0.0852\n",
      "Epoch 82/150\n",
      "595/595 [==============================] - 0s - loss: 0.0693 - val_loss: 0.0851\n",
      "Epoch 83/150\n",
      "595/595 [==============================] - 0s - loss: 0.0730 - val_loss: 0.0867\n",
      "Epoch 84/150\n",
      "595/595 [==============================] - 0s - loss: 0.0720 - val_loss: 0.0858\n",
      "Epoch 85/150\n",
      "595/595 [==============================] - 0s - loss: 0.0694 - val_loss: 0.0865\n",
      "Epoch 86/150\n",
      "595/595 [==============================] - 0s - loss: 0.0737 - val_loss: 0.0862\n",
      "Epoch 87/150\n",
      "595/595 [==============================] - 0s - loss: 0.0692 - val_loss: 0.0858\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595/595 [==============================] - 0s - loss: 0.0703 - val_loss: 0.0875\n",
      "Epoch 89/150\n",
      "595/595 [==============================] - 0s - loss: 0.0730 - val_loss: 0.0874\n",
      "Epoch 90/150\n",
      "595/595 [==============================] - 0s - loss: 0.0679 - val_loss: 0.0862\n",
      "Epoch 91/150\n",
      "595/595 [==============================] - 0s - loss: 0.0715 - val_loss: 0.0874\n",
      "Epoch 92/150\n",
      "595/595 [==============================] - 0s - loss: 0.0707 - val_loss: 0.0870\n",
      "Epoch 93/150\n",
      "595/595 [==============================] - 0s - loss: 0.0677 - val_loss: 0.0873\n",
      "Epoch 94/150\n",
      "595/595 [==============================] - 0s - loss: 0.0716 - val_loss: 0.0876\n",
      "Epoch 95/150\n",
      "595/595 [==============================] - 0s - loss: 0.0684 - val_loss: 0.0876\n",
      "Epoch 96/150\n",
      "595/595 [==============================] - 0s - loss: 0.0682 - val_loss: 0.0886\n",
      "Epoch 97/150\n",
      "595/595 [==============================] - 0s - loss: 0.0713 - val_loss: 0.0889\n",
      "Epoch 98/150\n",
      "595/595 [==============================] - 0s - loss: 0.0669 - val_loss: 0.0876\n",
      "Epoch 99/150\n",
      "595/595 [==============================] - 0s - loss: 0.0695 - val_loss: 0.0891\n",
      "Epoch 100/150\n",
      "595/595 [==============================] - 0s - loss: 0.0703 - val_loss: 0.0892\n",
      "Epoch 101/150\n",
      "595/595 [==============================] - 0s - loss: 0.0662 - val_loss: 0.0881\n",
      "Epoch 102/150\n",
      "595/595 [==============================] - 0s - loss: 0.0699 - val_loss: 0.0897\n",
      "Epoch 103/150\n",
      "595/595 [==============================] - 0s - loss: 0.0691 - val_loss: 0.0894\n",
      "Epoch 104/150\n",
      "595/595 [==============================] - 0s - loss: 0.0663 - val_loss: 0.0892\n",
      "Epoch 105/150\n",
      "595/595 [==============================] - 0s - loss: 0.0700 - val_loss: 0.0901\n",
      "Epoch 106/150\n",
      "595/595 [==============================] - 0s - loss: 0.0666 - val_loss: 0.0895\n",
      "Epoch 107/150\n",
      "595/595 [==============================] - 0s - loss: 0.0670 - val_loss: 0.0908\n",
      "Epoch 108/150\n",
      "595/595 [==============================] - 0s - loss: 0.0706 - val_loss: 0.0916\n",
      "Epoch 109/150\n",
      "595/595 [==============================] - 0s - loss: 0.0655 - val_loss: 0.0895\n",
      "Epoch 110/150\n",
      "595/595 [==============================] - 0s - loss: 0.0696 - val_loss: 0.0904\n",
      "Epoch 111/150\n",
      "595/595 [==============================] - 0s - loss: 0.0685 - val_loss: 0.0911\n",
      "Epoch 112/150\n",
      "595/595 [==============================] - 0s - loss: 0.0653 - val_loss: 0.0897\n",
      "Epoch 113/150\n",
      "595/595 [==============================] - 0s - loss: 0.0694 - val_loss: 0.0915\n",
      "Epoch 114/150\n",
      "595/595 [==============================] - 0s - loss: 0.0660 - val_loss: 0.0905\n",
      "Epoch 115/150\n",
      "595/595 [==============================] - 0s - loss: 0.0656 - val_loss: 0.0913\n",
      "Epoch 116/150\n",
      "595/595 [==============================] - 0s - loss: 0.0688 - val_loss: 0.0928\n",
      "Epoch 117/150\n",
      "595/595 [==============================] - 0s - loss: 0.0646 - val_loss: 0.0907\n",
      "Epoch 118/150\n",
      "595/595 [==============================] - 0s - loss: 0.0674 - val_loss: 0.0915\n",
      "Epoch 119/150\n",
      "595/595 [==============================] - 0s - loss: 0.0672 - val_loss: 0.0926\n",
      "Epoch 120/150\n",
      "595/595 [==============================] - 0s - loss: 0.0640 - val_loss: 0.0907\n",
      "Epoch 121/150\n",
      "595/595 [==============================] - 0s - loss: 0.0672 - val_loss: 0.0927\n",
      "Epoch 122/150\n",
      "595/595 [==============================] - 0s - loss: 0.0659 - val_loss: 0.0929\n",
      "Epoch 123/150\n",
      "595/595 [==============================] - 0s - loss: 0.0645 - val_loss: 0.0925\n",
      "Epoch 124/150\n",
      "595/595 [==============================] - 0s - loss: 0.0686 - val_loss: 0.0940\n",
      "Epoch 125/150\n",
      "595/595 [==============================] - 0s - loss: 0.0642 - val_loss: 0.0923\n",
      "Epoch 126/150\n",
      "595/595 [==============================] - 0s - loss: 0.0664 - val_loss: 0.0923\n",
      "Epoch 127/150\n",
      "595/595 [==============================] - 0s - loss: 0.0671 - val_loss: 0.0949\n",
      "Epoch 128/150\n",
      "595/595 [==============================] - 0s - loss: 0.0635 - val_loss: 0.0914\n",
      "Epoch 129/150\n",
      "595/595 [==============================] - 0s - loss: 0.0674 - val_loss: 0.0934\n",
      "Epoch 130/150\n",
      "595/595 [==============================] - 0s - loss: 0.0653 - val_loss: 0.0943\n",
      "Epoch 131/150\n",
      "595/595 [==============================] - 0s - loss: 0.0635 - val_loss: 0.0933\n",
      "Epoch 132/150\n",
      "595/595 [==============================] - 0s - loss: 0.0675 - val_loss: 0.0951\n",
      "Epoch 133/150\n",
      "595/595 [==============================] - 0s - loss: 0.0634 - val_loss: 0.0934\n",
      "Epoch 134/150\n",
      "595/595 [==============================] - 0s - loss: 0.0653 - val_loss: 0.0934\n",
      "Epoch 135/150\n",
      "595/595 [==============================] - 0s - loss: 0.0663 - val_loss: 0.0963\n",
      "Epoch 136/150\n",
      "595/595 [==============================] - 0s - loss: 0.0628 - val_loss: 0.0924\n",
      "Epoch 137/150\n",
      "595/595 [==============================] - 0s - loss: 0.0664 - val_loss: 0.0944\n",
      "Epoch 138/150\n",
      "595/595 [==============================] - 0s - loss: 0.0639 - val_loss: 0.0953\n",
      "Epoch 139/150\n",
      "595/595 [==============================] - 0s - loss: 0.0628 - val_loss: 0.0942\n",
      "Epoch 140/150\n",
      "595/595 [==============================] - 0s - loss: 0.0660 - val_loss: 0.0963\n",
      "Epoch 141/150\n",
      "595/595 [==============================] - 0s - loss: 0.0621 - val_loss: 0.0938\n",
      "Epoch 142/150\n",
      "595/595 [==============================] - 0s - loss: 0.0645 - val_loss: 0.0942\n",
      "Epoch 143/150\n",
      "595/595 [==============================] - 0s - loss: 0.0644 - val_loss: 0.0970\n",
      "Epoch 144/150\n",
      "595/595 [==============================] - 0s - loss: 0.0618 - val_loss: 0.0935\n",
      "Epoch 145/150\n",
      "595/595 [==============================] - 0s - loss: 0.0652 - val_loss: 0.0959\n",
      "Epoch 146/150\n",
      "595/595 [==============================] - 0s - loss: 0.0631 - val_loss: 0.0966\n",
      "Epoch 147/150\n",
      "595/595 [==============================] - 0s - loss: 0.0628 - val_loss: 0.0953\n",
      "Epoch 148/150\n",
      "595/595 [==============================] - 0s - loss: 0.0656 - val_loss: 0.0981\n",
      "Epoch 149/150\n",
      "595/595 [==============================] - 0s - loss: 0.0619 - val_loss: 0.0943\n",
      "Epoch 150/150\n",
      "595/595 [==============================] - 0s - loss: 0.0650 - val_loss: 0.0951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 15/22 [06:07<02:32, 21.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 594 samples, validate on 149 samples\n",
      "Epoch 1/150\n",
      "594/594 [==============================] - 5s - loss: 0.3321 - val_loss: 0.5082\n",
      "Epoch 2/150\n",
      "594/594 [==============================] - 0s - loss: 0.2522 - val_loss: 0.3809\n",
      "Epoch 3/150\n",
      "594/594 [==============================] - 0s - loss: 0.2035 - val_loss: 0.2902\n",
      "Epoch 4/150\n",
      "594/594 [==============================] - 0s - loss: 0.1729 - val_loss: 0.2327\n",
      "Epoch 5/150\n",
      "594/594 [==============================] - 0s - loss: 0.1560 - val_loss: 0.2069\n",
      "Epoch 6/150\n",
      "594/594 [==============================] - 0s - loss: 0.1465 - val_loss: 0.2015\n",
      "Epoch 7/150\n",
      "594/594 [==============================] - 0s - loss: 0.1385 - val_loss: 0.2011\n",
      "Epoch 8/150\n",
      "594/594 [==============================] - 0s - loss: 0.1315 - val_loss: 0.1976\n",
      "Epoch 9/150\n",
      "594/594 [==============================] - 0s - loss: 0.1251 - val_loss: 0.1888\n",
      "Epoch 10/150\n",
      "594/594 [==============================] - 0s - loss: 0.1185 - val_loss: 0.1744\n",
      "Epoch 11/150\n",
      "594/594 [==============================] - 0s - loss: 0.1120 - val_loss: 0.1587\n",
      "Epoch 12/150\n",
      "594/594 [==============================] - 0s - loss: 0.1062 - val_loss: 0.1462\n",
      "Epoch 13/150\n",
      "594/594 [==============================] - 0s - loss: 0.1012 - val_loss: 0.1372\n",
      "Epoch 14/150\n",
      "594/594 [==============================] - 0s - loss: 0.0971 - val_loss: 0.1303\n",
      "Epoch 15/150\n",
      "594/594 [==============================] - 0s - loss: 0.0937 - val_loss: 0.1244\n",
      "Epoch 16/150\n",
      "594/594 [==============================] - 0s - loss: 0.0908 - val_loss: 0.1190\n",
      "Epoch 17/150\n",
      "594/594 [==============================] - 0s - loss: 0.0883 - val_loss: 0.1147\n",
      "Epoch 18/150\n",
      "594/594 [==============================] - 0s - loss: 0.0861 - val_loss: 0.1113\n",
      "Epoch 19/150\n",
      "594/594 [==============================] - 0s - loss: 0.0841 - val_loss: 0.1084\n",
      "Epoch 20/150\n",
      "594/594 [==============================] - 0s - loss: 0.0824 - val_loss: 0.1061\n",
      "Epoch 21/150\n",
      "594/594 [==============================] - 0s - loss: 0.0808 - val_loss: 0.1031\n",
      "Epoch 22/150\n",
      "594/594 [==============================] - 0s - loss: 0.0794 - val_loss: 0.1014\n",
      "Epoch 23/150\n",
      "594/594 [==============================] - 0s - loss: 0.0780 - val_loss: 0.0988\n",
      "Epoch 24/150\n",
      "594/594 [==============================] - 0s - loss: 0.0767 - val_loss: 0.0974\n",
      "Epoch 25/150\n",
      "594/594 [==============================] - 0s - loss: 0.0755 - val_loss: 0.0955\n",
      "Epoch 26/150\n",
      "594/594 [==============================] - 0s - loss: 0.0743 - val_loss: 0.0934\n",
      "Epoch 27/150\n",
      "594/594 [==============================] - 0s - loss: 0.0732 - val_loss: 0.0926\n",
      "Epoch 28/150\n",
      "594/594 [==============================] - 0s - loss: 0.0721 - val_loss: 0.0905\n",
      "Epoch 29/150\n",
      "594/594 [==============================] - 0s - loss: 0.0711 - val_loss: 0.0892\n",
      "Epoch 30/150\n",
      "594/594 [==============================] - 0s - loss: 0.0700 - val_loss: 0.0883\n",
      "Epoch 31/150\n",
      "594/594 [==============================] - 0s - loss: 0.0691 - val_loss: 0.0865\n",
      "Epoch 32/150\n",
      "594/594 [==============================] - 0s - loss: 0.0683 - val_loss: 0.0864\n",
      "Epoch 33/150\n",
      "594/594 [==============================] - 0s - loss: 0.0674 - val_loss: 0.0862\n",
      "Epoch 34/150\n",
      "594/594 [==============================] - 0s - loss: 0.0667 - val_loss: 0.0840\n",
      "Epoch 35/150\n",
      "594/594 [==============================] - 0s - loss: 0.0659 - val_loss: 0.0837\n",
      "Epoch 36/150\n",
      "594/594 [==============================] - 0s - loss: 0.0654 - val_loss: 0.0844\n",
      "Epoch 37/150\n",
      "594/594 [==============================] - 0s - loss: 0.0647 - val_loss: 0.0834\n",
      "Epoch 38/150\n",
      "594/594 [==============================] - 0s - loss: 0.0643 - val_loss: 0.0822\n",
      "Epoch 39/150\n",
      "594/594 [==============================] - 0s - loss: 0.0636 - val_loss: 0.0820\n",
      "Epoch 40/150\n",
      "594/594 [==============================] - 0s - loss: 0.0634 - val_loss: 0.0838\n",
      "Epoch 41/150\n",
      "594/594 [==============================] - 0s - loss: 0.0626 - val_loss: 0.0809\n",
      "Epoch 42/150\n",
      "594/594 [==============================] - 0s - loss: 0.0620 - val_loss: 0.0814\n",
      "Epoch 43/150\n",
      "594/594 [==============================] - 0s - loss: 0.0623 - val_loss: 0.0829\n",
      "Epoch 44/150\n",
      "594/594 [==============================] - 0s - loss: 0.0612 - val_loss: 0.0819\n",
      "Epoch 45/150\n",
      "594/594 [==============================] - 0s - loss: 0.0611 - val_loss: 0.0809\n",
      "Epoch 46/150\n",
      "594/594 [==============================] - 0s - loss: 0.0613 - val_loss: 0.0816\n",
      "Epoch 47/150\n",
      "594/594 [==============================] - 0s - loss: 0.0607 - val_loss: 0.0828\n",
      "Epoch 48/150\n",
      "594/594 [==============================] - 0s - loss: 0.0601 - val_loss: 0.0796\n",
      "Epoch 49/150\n",
      "594/594 [==============================] - 0s - loss: 0.0601 - val_loss: 0.0811\n",
      "Epoch 50/150\n",
      "594/594 [==============================] - 0s - loss: 0.0601 - val_loss: 0.0822\n",
      "Epoch 51/150\n",
      "594/594 [==============================] - 0s - loss: 0.0585 - val_loss: 0.0790\n",
      "Epoch 52/150\n",
      "594/594 [==============================] - 0s - loss: 0.0582 - val_loss: 0.0806\n",
      "Epoch 53/150\n",
      "594/594 [==============================] - 0s - loss: 0.0586 - val_loss: 0.0810\n",
      "Epoch 54/150\n",
      "594/594 [==============================] - 0s - loss: 0.0575 - val_loss: 0.0806\n",
      "Epoch 55/150\n",
      "594/594 [==============================] - 0s - loss: 0.0576 - val_loss: 0.0806\n",
      "Epoch 56/150\n",
      "594/594 [==============================] - 0s - loss: 0.0579 - val_loss: 0.0815\n",
      "Epoch 57/150\n",
      "594/594 [==============================] - 0s - loss: 0.0577 - val_loss: 0.0810\n",
      "Epoch 58/150\n",
      "594/594 [==============================] - 0s - loss: 0.0565 - val_loss: 0.0801\n",
      "Epoch 59/150\n",
      "594/594 [==============================] - 0s - loss: 0.0568 - val_loss: 0.0815\n",
      "Epoch 60/150\n",
      "594/594 [==============================] - 0s - loss: 0.0572 - val_loss: 0.0813\n",
      "Epoch 61/150\n",
      "594/594 [==============================] - 0s - loss: 0.0560 - val_loss: 0.0796\n",
      "Epoch 62/150\n",
      "594/594 [==============================] - 0s - loss: 0.0557 - val_loss: 0.0815\n",
      "Epoch 63/150\n",
      "594/594 [==============================] - 0s - loss: 0.0567 - val_loss: 0.0814\n",
      "Epoch 64/150\n",
      "594/594 [==============================] - 0s - loss: 0.0561 - val_loss: 0.0806\n",
      "Epoch 65/150\n",
      "594/594 [==============================] - 0s - loss: 0.0553 - val_loss: 0.0797\n",
      "Epoch 66/150\n",
      "594/594 [==============================] - 0s - loss: 0.0551 - val_loss: 0.0816\n",
      "Epoch 67/150\n",
      "594/594 [==============================] - 0s - loss: 0.0562 - val_loss: 0.0808\n",
      "Epoch 68/150\n",
      "594/594 [==============================] - 0s - loss: 0.0543 - val_loss: 0.0787\n",
      "Epoch 69/150\n",
      "594/594 [==============================] - 0s - loss: 0.0546 - val_loss: 0.0820\n",
      "Epoch 70/150\n",
      "594/594 [==============================] - 0s - loss: 0.0557 - val_loss: 0.0813\n",
      "Epoch 71/150\n",
      "594/594 [==============================] - 0s - loss: 0.0549 - val_loss: 0.0808\n",
      "Epoch 72/150\n",
      "594/594 [==============================] - 0s - loss: 0.0546 - val_loss: 0.0801\n",
      "Epoch 73/150\n",
      "594/594 [==============================] - 0s - loss: 0.0548 - val_loss: 0.0811\n",
      "Epoch 74/150\n",
      "594/594 [==============================] - 0s - loss: 0.0551 - val_loss: 0.0811\n",
      "Epoch 75/150\n",
      "594/594 [==============================] - 0s - loss: 0.0530 - val_loss: 0.0789\n",
      "Epoch 76/150\n",
      "594/594 [==============================] - 0s - loss: 0.0536 - val_loss: 0.0813\n",
      "Epoch 77/150\n",
      "594/594 [==============================] - 0s - loss: 0.0541 - val_loss: 0.0810\n",
      "Epoch 78/150\n",
      "594/594 [==============================] - 0s - loss: 0.0535 - val_loss: 0.0799\n",
      "Epoch 79/150\n",
      "594/594 [==============================] - 0s - loss: 0.0530 - val_loss: 0.0806\n",
      "Epoch 80/150\n",
      "594/594 [==============================] - 0s - loss: 0.0537 - val_loss: 0.0813\n",
      "Epoch 81/150\n",
      "594/594 [==============================] - 0s - loss: 0.0538 - val_loss: 0.0807\n",
      "Epoch 82/150\n",
      "594/594 [==============================] - 0s - loss: 0.0521 - val_loss: 0.0794\n",
      "Epoch 83/150\n",
      "594/594 [==============================] - 0s - loss: 0.0522 - val_loss: 0.0817\n",
      "Epoch 84/150\n",
      "594/594 [==============================] - 0s - loss: 0.0535 - val_loss: 0.0814\n",
      "Epoch 85/150\n",
      "594/594 [==============================] - 0s - loss: 0.0521 - val_loss: 0.0796\n",
      "Epoch 86/150\n",
      "594/594 [==============================] - 0s - loss: 0.0524 - val_loss: 0.0814\n",
      "Epoch 87/150\n",
      "594/594 [==============================] - 0s - loss: 0.0529 - val_loss: 0.0816\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594/594 [==============================] - 0s - loss: 0.0532 - val_loss: 0.0803\n",
      "Epoch 89/150\n",
      "594/594 [==============================] - 0s - loss: 0.0517 - val_loss: 0.0795\n",
      "Epoch 90/150\n",
      "594/594 [==============================] - 0s - loss: 0.0523 - val_loss: 0.0816\n",
      "Epoch 91/150\n",
      "594/594 [==============================] - 0s - loss: 0.0531 - val_loss: 0.0804\n",
      "Epoch 92/150\n",
      "594/594 [==============================] - 0s - loss: 0.0514 - val_loss: 0.0788\n",
      "Epoch 93/150\n",
      "594/594 [==============================] - 0s - loss: 0.0512 - val_loss: 0.0808\n",
      "Epoch 94/150\n",
      "594/594 [==============================] - 0s - loss: 0.0520 - val_loss: 0.0817\n",
      "Epoch 95/150\n",
      "594/594 [==============================] - 0s - loss: 0.0518 - val_loss: 0.0798\n",
      "Epoch 96/150\n",
      "594/594 [==============================] - 0s - loss: 0.0510 - val_loss: 0.0795\n",
      "Epoch 97/150\n",
      "594/594 [==============================] - 0s - loss: 0.0512 - val_loss: 0.0812\n",
      "Epoch 98/150\n",
      "594/594 [==============================] - 0s - loss: 0.0522 - val_loss: 0.0807\n",
      "Epoch 99/150\n",
      "594/594 [==============================] - 0s - loss: 0.0505 - val_loss: 0.0787\n",
      "Epoch 100/150\n",
      "594/594 [==============================] - 0s - loss: 0.0513 - val_loss: 0.0809\n",
      "Epoch 101/150\n",
      "594/594 [==============================] - 0s - loss: 0.0518 - val_loss: 0.0806\n",
      "Epoch 102/150\n",
      "594/594 [==============================] - 0s - loss: 0.0516 - val_loss: 0.0795\n",
      "Epoch 103/150\n",
      "594/594 [==============================] - 0s - loss: 0.0503 - val_loss: 0.0786\n",
      "Epoch 104/150\n",
      "594/594 [==============================] - 0s - loss: 0.0506 - val_loss: 0.0808\n",
      "Epoch 105/150\n",
      "594/594 [==============================] - 0s - loss: 0.0514 - val_loss: 0.0798\n",
      "Epoch 106/150\n",
      "594/594 [==============================] - 0s - loss: 0.0495 - val_loss: 0.0779\n",
      "Epoch 107/150\n",
      "594/594 [==============================] - 0s - loss: 0.0496 - val_loss: 0.0809\n",
      "Epoch 108/150\n",
      "594/594 [==============================] - 0s - loss: 0.0507 - val_loss: 0.0808\n",
      "Epoch 109/150\n",
      "594/594 [==============================] - 0s - loss: 0.0507 - val_loss: 0.0799\n",
      "Epoch 110/150\n",
      "594/594 [==============================] - 0s - loss: 0.0504 - val_loss: 0.0788\n",
      "Epoch 111/150\n",
      "594/594 [==============================] - 0s - loss: 0.0507 - val_loss: 0.0804\n",
      "Epoch 112/150\n",
      "594/594 [==============================] - 0s - loss: 0.0512 - val_loss: 0.0799\n",
      "Epoch 113/150\n",
      "594/594 [==============================] - 0s - loss: 0.0490 - val_loss: 0.0778\n",
      "Epoch 114/150\n",
      "594/594 [==============================] - 0s - loss: 0.0493 - val_loss: 0.0803\n",
      "Epoch 115/150\n",
      "594/594 [==============================] - 0s - loss: 0.0498 - val_loss: 0.0803\n",
      "Epoch 116/150\n",
      "594/594 [==============================] - 0s - loss: 0.0494 - val_loss: 0.0791\n",
      "Epoch 117/150\n",
      "594/594 [==============================] - 0s - loss: 0.0491 - val_loss: 0.0785\n",
      "Epoch 118/150\n",
      "594/594 [==============================] - 0s - loss: 0.0492 - val_loss: 0.0811\n",
      "Epoch 119/150\n",
      "594/594 [==============================] - 0s - loss: 0.0509 - val_loss: 0.0800\n",
      "Epoch 120/150\n",
      "594/594 [==============================] - 0s - loss: 0.0486 - val_loss: 0.0775\n",
      "Epoch 121/150\n",
      "594/594 [==============================] - 0s - loss: 0.0491 - val_loss: 0.0811\n",
      "Epoch 122/150\n",
      "594/594 [==============================] - 0s - loss: 0.0502 - val_loss: 0.0803\n",
      "Epoch 123/150\n",
      "594/594 [==============================] - 0s - loss: 0.0492 - val_loss: 0.0789\n",
      "Epoch 124/150\n",
      "594/594 [==============================] - 0s - loss: 0.0488 - val_loss: 0.0795\n",
      "Epoch 125/150\n",
      "594/594 [==============================] - 0s - loss: 0.0491 - val_loss: 0.0800\n",
      "Epoch 126/150\n",
      "594/594 [==============================] - 0s - loss: 0.0497 - val_loss: 0.0795\n",
      "Epoch 127/150\n",
      "594/594 [==============================] - 0s - loss: 0.0479 - val_loss: 0.0777\n",
      "Epoch 128/150\n",
      "594/594 [==============================] - 0s - loss: 0.0481 - val_loss: 0.0804\n",
      "Epoch 129/150\n",
      "594/594 [==============================] - 0s - loss: 0.0491 - val_loss: 0.0797\n",
      "Epoch 130/150\n",
      "594/594 [==============================] - 0s - loss: 0.0476 - val_loss: 0.0786\n",
      "Epoch 131/150\n",
      "594/594 [==============================] - 0s - loss: 0.0484 - val_loss: 0.0799\n",
      "Epoch 132/150\n",
      "594/594 [==============================] - 0s - loss: 0.0486 - val_loss: 0.0807\n",
      "Epoch 133/150\n",
      "594/594 [==============================] - 0s - loss: 0.0497 - val_loss: 0.0796\n",
      "Epoch 134/150\n",
      "594/594 [==============================] - 0s - loss: 0.0477 - val_loss: 0.0774\n",
      "Epoch 135/150\n",
      "594/594 [==============================] - 0s - loss: 0.0481 - val_loss: 0.0804\n",
      "Epoch 136/150\n",
      "594/594 [==============================] - 0s - loss: 0.0491 - val_loss: 0.0797\n",
      "Epoch 137/150\n",
      "594/594 [==============================] - 0s - loss: 0.0473 - val_loss: 0.0781\n",
      "Epoch 138/150\n",
      "594/594 [==============================] - 0s - loss: 0.0477 - val_loss: 0.0804\n",
      "Epoch 139/150\n",
      "594/594 [==============================] - 0s - loss: 0.0487 - val_loss: 0.0800\n",
      "Epoch 140/150\n",
      "594/594 [==============================] - 0s - loss: 0.0487 - val_loss: 0.0794\n",
      "Epoch 141/150\n",
      "594/594 [==============================] - 0s - loss: 0.0479 - val_loss: 0.0785\n",
      "Epoch 142/150\n",
      "594/594 [==============================] - 0s - loss: 0.0482 - val_loss: 0.0796\n",
      "Epoch 143/150\n",
      "594/594 [==============================] - 0s - loss: 0.0484 - val_loss: 0.0793\n",
      "Epoch 144/150\n",
      "594/594 [==============================] - 0s - loss: 0.0463 - val_loss: 0.0777\n",
      "Epoch 145/150\n",
      "594/594 [==============================] - 0s - loss: 0.0466 - val_loss: 0.0804\n",
      "Epoch 146/150\n",
      "594/594 [==============================] - 0s - loss: 0.0476 - val_loss: 0.0799\n",
      "Epoch 147/150\n",
      "594/594 [==============================] - 0s - loss: 0.0471 - val_loss: 0.0789\n",
      "Epoch 148/150\n",
      "594/594 [==============================] - 0s - loss: 0.0470 - val_loss: 0.0788\n",
      "Epoch 149/150\n",
      "594/594 [==============================] - 0s - loss: 0.0472 - val_loss: 0.0809\n",
      "Epoch 150/150\n",
      "594/594 [==============================] - 0s - loss: 0.0491 - val_loss: 0.0798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 16/22 [06:28<02:09, 21.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 595 samples, validate on 149 samples\n",
      "Epoch 1/150\n",
      "595/595 [==============================] - 5s - loss: 0.2813 - val_loss: 0.4420\n",
      "Epoch 2/150\n",
      "595/595 [==============================] - 0s - loss: 0.2106 - val_loss: 0.3301\n",
      "Epoch 3/150\n",
      "595/595 [==============================] - 0s - loss: 0.1758 - val_loss: 0.2601\n",
      "Epoch 4/150\n",
      "595/595 [==============================] - 0s - loss: 0.1553 - val_loss: 0.2203\n",
      "Epoch 5/150\n",
      "595/595 [==============================] - 0s - loss: 0.1462 - val_loss: 0.2037\n",
      "Epoch 6/150\n",
      "595/595 [==============================] - 0s - loss: 0.1423 - val_loss: 0.1974\n",
      "Epoch 7/150\n",
      "595/595 [==============================] - 0s - loss: 0.1392 - val_loss: 0.1942\n",
      "Epoch 8/150\n",
      "595/595 [==============================] - 0s - loss: 0.1364 - val_loss: 0.1925\n",
      "Epoch 9/150\n",
      "595/595 [==============================] - 0s - loss: 0.1338 - val_loss: 0.1930\n",
      "Epoch 10/150\n",
      "595/595 [==============================] - 0s - loss: 0.1313 - val_loss: 0.1921\n",
      "Epoch 11/150\n",
      "595/595 [==============================] - 0s - loss: 0.1289 - val_loss: 0.1873\n",
      "Epoch 12/150\n",
      "595/595 [==============================] - 0s - loss: 0.1263 - val_loss: 0.1796\n",
      "Epoch 13/150\n",
      "595/595 [==============================] - 0s - loss: 0.1237 - val_loss: 0.1724\n",
      "Epoch 14/150\n",
      "595/595 [==============================] - 0s - loss: 0.1211 - val_loss: 0.1662\n",
      "Epoch 15/150\n",
      "595/595 [==============================] - 0s - loss: 0.1183 - val_loss: 0.1592\n",
      "Epoch 16/150\n",
      "595/595 [==============================] - 0s - loss: 0.1156 - val_loss: 0.1519\n",
      "Epoch 17/150\n",
      "595/595 [==============================] - 0s - loss: 0.1131 - val_loss: 0.1463\n",
      "Epoch 18/150\n",
      "595/595 [==============================] - 0s - loss: 0.1107 - val_loss: 0.1415\n",
      "Epoch 19/150\n",
      "595/595 [==============================] - 0s - loss: 0.1083 - val_loss: 0.1366\n",
      "Epoch 20/150\n",
      "595/595 [==============================] - 0s - loss: 0.1061 - val_loss: 0.1330\n",
      "Epoch 21/150\n",
      "595/595 [==============================] - 0s - loss: 0.1041 - val_loss: 0.1297\n",
      "Epoch 22/150\n",
      "595/595 [==============================] - 0s - loss: 0.1021 - val_loss: 0.1266\n",
      "Epoch 23/150\n",
      "595/595 [==============================] - 0s - loss: 0.1004 - val_loss: 0.1237\n",
      "Epoch 24/150\n",
      "595/595 [==============================] - 0s - loss: 0.0988 - val_loss: 0.1208\n",
      "Epoch 25/150\n",
      "595/595 [==============================] - 0s - loss: 0.0973 - val_loss: 0.1185\n",
      "Epoch 26/150\n",
      "595/595 [==============================] - 0s - loss: 0.0960 - val_loss: 0.1165\n",
      "Epoch 27/150\n",
      "595/595 [==============================] - 0s - loss: 0.0947 - val_loss: 0.1148\n",
      "Epoch 28/150\n",
      "595/595 [==============================] - 0s - loss: 0.0936 - val_loss: 0.1128\n",
      "Epoch 29/150\n",
      "595/595 [==============================] - 0s - loss: 0.0925 - val_loss: 0.1116\n",
      "Epoch 30/150\n",
      "595/595 [==============================] - 0s - loss: 0.0915 - val_loss: 0.1105\n",
      "Epoch 31/150\n",
      "595/595 [==============================] - 0s - loss: 0.0905 - val_loss: 0.1090\n",
      "Epoch 32/150\n",
      "595/595 [==============================] - 0s - loss: 0.0897 - val_loss: 0.1083\n",
      "Epoch 33/150\n",
      "595/595 [==============================] - 0s - loss: 0.0888 - val_loss: 0.1071\n",
      "Epoch 34/150\n",
      "595/595 [==============================] - 0s - loss: 0.0881 - val_loss: 0.1065\n",
      "Epoch 35/150\n",
      "595/595 [==============================] - 0s - loss: 0.0873 - val_loss: 0.1058\n",
      "Epoch 36/150\n",
      "595/595 [==============================] - 0s - loss: 0.0869 - val_loss: 0.1055\n",
      "Epoch 37/150\n",
      "595/595 [==============================] - 0s - loss: 0.0859 - val_loss: 0.1045\n",
      "Epoch 38/150\n",
      "595/595 [==============================] - 0s - loss: 0.0862 - val_loss: 0.1051\n",
      "Epoch 39/150\n",
      "595/595 [==============================] - 0s - loss: 0.0853 - val_loss: 0.1039\n",
      "Epoch 40/150\n",
      "595/595 [==============================] - 0s - loss: 0.0860 - val_loss: 0.1054\n",
      "Epoch 41/150\n",
      "595/595 [==============================] - 0s - loss: 0.0852 - val_loss: 0.1024\n",
      "Epoch 42/150\n",
      "595/595 [==============================] - 0s - loss: 0.0854 - val_loss: 0.1047\n",
      "Epoch 43/150\n",
      "595/595 [==============================] - 0s - loss: 0.0850 - val_loss: 0.1012\n",
      "Epoch 44/150\n",
      "595/595 [==============================] - 0s - loss: 0.0839 - val_loss: 0.1033\n",
      "Epoch 45/150\n",
      "595/595 [==============================] - 0s - loss: 0.0839 - val_loss: 0.1004\n",
      "Epoch 46/150\n",
      "595/595 [==============================] - 0s - loss: 0.0823 - val_loss: 0.1023\n",
      "Epoch 47/150\n",
      "595/595 [==============================] - 0s - loss: 0.0827 - val_loss: 0.1000\n",
      "Epoch 48/150\n",
      "595/595 [==============================] - 0s - loss: 0.0812 - val_loss: 0.1014\n",
      "Epoch 49/150\n",
      "595/595 [==============================] - 0s - loss: 0.0815 - val_loss: 0.1002\n",
      "Epoch 50/150\n",
      "595/595 [==============================] - 0s - loss: 0.0803 - val_loss: 0.1011\n",
      "Epoch 51/150\n",
      "595/595 [==============================] - 0s - loss: 0.0803 - val_loss: 0.1000\n",
      "Epoch 52/150\n",
      "595/595 [==============================] - 0s - loss: 0.0799 - val_loss: 0.1010\n",
      "Epoch 53/150\n",
      "595/595 [==============================] - 0s - loss: 0.0796 - val_loss: 0.1000\n",
      "Epoch 54/150\n",
      "595/595 [==============================] - 0s - loss: 0.0798 - val_loss: 0.1016\n",
      "Epoch 55/150\n",
      "595/595 [==============================] - 0s - loss: 0.0792 - val_loss: 0.0998\n",
      "Epoch 56/150\n",
      "595/595 [==============================] - 0s - loss: 0.0800 - val_loss: 0.1020\n",
      "Epoch 57/150\n",
      "595/595 [==============================] - 0s - loss: 0.0792 - val_loss: 0.0995\n",
      "Epoch 58/150\n",
      "595/595 [==============================] - 0s - loss: 0.0799 - val_loss: 0.1023\n",
      "Epoch 59/150\n",
      "595/595 [==============================] - 0s - loss: 0.0792 - val_loss: 0.0992\n",
      "Epoch 60/150\n",
      "595/595 [==============================] - 0s - loss: 0.0794 - val_loss: 0.1023\n",
      "Epoch 61/150\n",
      "595/595 [==============================] - 0s - loss: 0.0791 - val_loss: 0.0990\n",
      "Epoch 62/150\n",
      "595/595 [==============================] - 0s - loss: 0.0787 - val_loss: 0.1017\n",
      "Epoch 63/150\n",
      "595/595 [==============================] - 0s - loss: 0.0787 - val_loss: 0.0988\n",
      "Epoch 64/150\n",
      "595/595 [==============================] - 0s - loss: 0.0776 - val_loss: 0.1011\n",
      "Epoch 65/150\n",
      "595/595 [==============================] - 0s - loss: 0.0780 - val_loss: 0.0990\n",
      "Epoch 66/150\n",
      "595/595 [==============================] - 0s - loss: 0.0766 - val_loss: 0.1008\n",
      "Epoch 67/150\n",
      "595/595 [==============================] - 0s - loss: 0.0770 - val_loss: 0.0992\n",
      "Epoch 68/150\n",
      "595/595 [==============================] - 0s - loss: 0.0759 - val_loss: 0.1006\n",
      "Epoch 69/150\n",
      "595/595 [==============================] - 0s - loss: 0.0761 - val_loss: 0.0993\n",
      "Epoch 70/150\n",
      "595/595 [==============================] - 0s - loss: 0.0755 - val_loss: 0.1007\n",
      "Epoch 71/150\n",
      "595/595 [==============================] - 0s - loss: 0.0753 - val_loss: 0.0992\n",
      "Epoch 72/150\n",
      "595/595 [==============================] - 0s - loss: 0.0750 - val_loss: 0.1008\n",
      "Epoch 73/150\n",
      "595/595 [==============================] - 0s - loss: 0.0747 - val_loss: 0.0993\n",
      "Epoch 74/150\n",
      "595/595 [==============================] - 0s - loss: 0.0748 - val_loss: 0.1011\n",
      "Epoch 75/150\n",
      "595/595 [==============================] - 0s - loss: 0.0743 - val_loss: 0.0995\n",
      "Epoch 76/150\n",
      "595/595 [==============================] - 0s - loss: 0.0748 - val_loss: 0.1020\n",
      "Epoch 77/150\n",
      "595/595 [==============================] - 0s - loss: 0.0740 - val_loss: 0.0998\n",
      "Epoch 78/150\n",
      "595/595 [==============================] - 0s - loss: 0.0751 - val_loss: 0.1027\n",
      "Epoch 79/150\n",
      "595/595 [==============================] - 0s - loss: 0.0743 - val_loss: 0.0999\n",
      "Epoch 80/150\n",
      "595/595 [==============================] - 0s - loss: 0.0754 - val_loss: 0.1032\n",
      "Epoch 81/150\n",
      "595/595 [==============================] - 0s - loss: 0.0749 - val_loss: 0.1000\n",
      "Epoch 82/150\n",
      "595/595 [==============================] - 0s - loss: 0.0752 - val_loss: 0.1031\n",
      "Epoch 83/150\n",
      "595/595 [==============================] - 0s - loss: 0.0752 - val_loss: 0.0999\n",
      "Epoch 84/150\n",
      "595/595 [==============================] - 0s - loss: 0.0746 - val_loss: 0.1024\n",
      "Epoch 85/150\n",
      "595/595 [==============================] - 0s - loss: 0.0750 - val_loss: 0.0996\n",
      "Epoch 86/150\n",
      "595/595 [==============================] - 0s - loss: 0.0736 - val_loss: 0.1017\n",
      "Epoch 87/150\n",
      "595/595 [==============================] - 0s - loss: 0.0742 - val_loss: 0.0994\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595/595 [==============================] - 0s - loss: 0.0725 - val_loss: 0.1013\n",
      "Epoch 89/150\n",
      "595/595 [==============================] - 0s - loss: 0.0730 - val_loss: 0.0996\n",
      "Epoch 90/150\n",
      "595/595 [==============================] - 0s - loss: 0.0716 - val_loss: 0.1010\n",
      "Epoch 91/150\n",
      "595/595 [==============================] - 0s - loss: 0.0719 - val_loss: 0.0998\n",
      "Epoch 92/150\n",
      "595/595 [==============================] - 0s - loss: 0.0709 - val_loss: 0.1010\n",
      "Epoch 93/150\n",
      "595/595 [==============================] - 0s - loss: 0.0712 - val_loss: 0.1000\n",
      "Epoch 94/150\n",
      "595/595 [==============================] - 0s - loss: 0.0703 - val_loss: 0.1009\n",
      "Epoch 95/150\n",
      "595/595 [==============================] - 0s - loss: 0.0706 - val_loss: 0.1003\n",
      "Epoch 96/150\n",
      "595/595 [==============================] - 0s - loss: 0.0699 - val_loss: 0.1010\n",
      "Epoch 97/150\n",
      "595/595 [==============================] - 0s - loss: 0.0702 - val_loss: 0.1006\n",
      "Epoch 98/150\n",
      "595/595 [==============================] - 0s - loss: 0.0695 - val_loss: 0.1010\n",
      "Epoch 99/150\n",
      "595/595 [==============================] - 0s - loss: 0.0700 - val_loss: 0.1008\n",
      "Epoch 100/150\n",
      "595/595 [==============================] - 0s - loss: 0.0691 - val_loss: 0.1009\n",
      "Epoch 101/150\n",
      "595/595 [==============================] - ETA: 0s - loss: 0.068 - 0s - loss: 0.0697 - val_loss: 0.1009\n",
      "Epoch 102/150\n",
      "595/595 [==============================] - 0s - loss: 0.0686 - val_loss: 0.1008\n",
      "Epoch 103/150\n",
      "595/595 [==============================] - 0s - loss: 0.0692 - val_loss: 0.1011\n",
      "Epoch 104/150\n",
      "595/595 [==============================] - 0s - loss: 0.0680 - val_loss: 0.1007\n",
      "Epoch 105/150\n",
      "595/595 [==============================] - 0s - loss: 0.0687 - val_loss: 0.1015\n",
      "Epoch 106/150\n",
      "595/595 [==============================] - 0s - loss: 0.0675 - val_loss: 0.1009\n",
      "Epoch 107/150\n",
      "595/595 [==============================] - 0s - loss: 0.0686 - val_loss: 0.1020\n",
      "Epoch 108/150\n",
      "595/595 [==============================] - 0s - loss: 0.0671 - val_loss: 0.1010\n",
      "Epoch 109/150\n",
      "595/595 [==============================] - 0s - loss: 0.0688 - val_loss: 0.1026\n",
      "Epoch 110/150\n",
      "595/595 [==============================] - 0s - loss: 0.0667 - val_loss: 0.1013\n",
      "Epoch 111/150\n",
      "595/595 [==============================] - 0s - loss: 0.0689 - val_loss: 0.1030\n",
      "Epoch 112/150\n",
      "595/595 [==============================] - 0s - loss: 0.0665 - val_loss: 0.1017\n",
      "Epoch 113/150\n",
      "595/595 [==============================] - 0s - loss: 0.0693 - val_loss: 0.1039\n",
      "Epoch 114/150\n",
      "595/595 [==============================] - 0s - loss: 0.0665 - val_loss: 0.1022\n",
      "Epoch 115/150\n",
      "595/595 [==============================] - 0s - loss: 0.0700 - val_loss: 0.1048\n",
      "Epoch 116/150\n",
      "595/595 [==============================] - 0s - loss: 0.0669 - val_loss: 0.1024\n",
      "Epoch 117/150\n",
      "595/595 [==============================] - 0s - loss: 0.0704 - val_loss: 0.1059\n",
      "Epoch 118/150\n",
      "595/595 [==============================] - 0s - loss: 0.0680 - val_loss: 0.1027\n",
      "Epoch 119/150\n",
      "595/595 [==============================] - 0s - loss: 0.0714 - val_loss: 0.1068\n",
      "Epoch 120/150\n",
      "595/595 [==============================] - 0s - loss: 0.0691 - val_loss: 0.1033\n",
      "Epoch 121/150\n",
      "595/595 [==============================] - 0s - loss: 0.0724 - val_loss: 0.1073\n",
      "Epoch 122/150\n",
      "595/595 [==============================] - 0s - loss: 0.0709 - val_loss: 0.1017\n",
      "Epoch 123/150\n",
      "595/595 [==============================] - 0s - loss: 0.0714 - val_loss: 0.1048\n",
      "Epoch 124/150\n",
      "595/595 [==============================] - 0s - loss: 0.0715 - val_loss: 0.1000\n",
      "Epoch 125/150\n",
      "595/595 [==============================] - 0s - loss: 0.0688 - val_loss: 0.1024\n",
      "Epoch 126/150\n",
      "595/595 [==============================] - 0s - loss: 0.0696 - val_loss: 0.0996\n",
      "Epoch 127/150\n",
      "595/595 [==============================] - 0s - loss: 0.0669 - val_loss: 0.1017\n",
      "Epoch 128/150\n",
      "595/595 [==============================] - 0s - loss: 0.0672 - val_loss: 0.0998\n",
      "Epoch 129/150\n",
      "595/595 [==============================] - 0s - loss: 0.0661 - val_loss: 0.1020\n",
      "Epoch 130/150\n",
      "595/595 [==============================] - 0s - loss: 0.0657 - val_loss: 0.1000\n",
      "Epoch 131/150\n",
      "595/595 [==============================] - 0s - loss: 0.0657 - val_loss: 0.1021\n",
      "Epoch 132/150\n",
      "595/595 [==============================] - 0s - loss: 0.0648 - val_loss: 0.1002\n",
      "Epoch 133/150\n",
      "595/595 [==============================] - 0s - loss: 0.0652 - val_loss: 0.1023\n",
      "Epoch 134/150\n",
      "595/595 [==============================] - 0s - loss: 0.0641 - val_loss: 0.1003\n",
      "Epoch 135/150\n",
      "595/595 [==============================] - 0s - loss: 0.0650 - val_loss: 0.1026\n",
      "Epoch 136/150\n",
      "595/595 [==============================] - 0s - loss: 0.0635 - val_loss: 0.1006\n",
      "Epoch 137/150\n",
      "595/595 [==============================] - 0s - loss: 0.0651 - val_loss: 0.1029\n",
      "Epoch 138/150\n",
      "595/595 [==============================] - 0s - loss: 0.0629 - val_loss: 0.1007\n",
      "Epoch 139/150\n",
      "595/595 [==============================] - 0s - loss: 0.0652 - val_loss: 0.1033\n",
      "Epoch 140/150\n",
      "595/595 [==============================] - 0s - loss: 0.0628 - val_loss: 0.1010\n",
      "Epoch 141/150\n",
      "595/595 [==============================] - 0s - loss: 0.0655 - val_loss: 0.1040\n",
      "Epoch 142/150\n",
      "595/595 [==============================] - 0s - loss: 0.0628 - val_loss: 0.1012\n",
      "Epoch 143/150\n",
      "595/595 [==============================] - 0s - loss: 0.0660 - val_loss: 0.1046\n",
      "Epoch 144/150\n",
      "595/595 [==============================] - 0s - loss: 0.0632 - val_loss: 0.1013\n",
      "Epoch 145/150\n",
      "595/595 [==============================] - 0s - loss: 0.0665 - val_loss: 0.1049\n",
      "Epoch 146/150\n",
      "595/595 [==============================] - 0s - loss: 0.0643 - val_loss: 0.1011\n",
      "Epoch 147/150\n",
      "595/595 [==============================] - 0s - loss: 0.0668 - val_loss: 0.1046\n",
      "Epoch 148/150\n",
      "595/595 [==============================] - 0s - loss: 0.0654 - val_loss: 0.1005\n",
      "Epoch 149/150\n",
      "595/595 [==============================] - 0s - loss: 0.0660 - val_loss: 0.1036\n",
      "Epoch 150/150\n",
      "595/595 [==============================] - 0s - loss: 0.0655 - val_loss: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 17/22 [06:51<01:50, 22.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 595 samples, validate on 149 samples\n",
      "Epoch 1/150\n",
      "595/595 [==============================] - 7s - loss: 0.2797 - val_loss: 0.6011\n",
      "Epoch 2/150\n",
      "595/595 [==============================] - 0s - loss: 0.2347 - val_loss: 0.5239\n",
      "Epoch 3/150\n",
      "595/595 [==============================] - 0s - loss: 0.2110 - val_loss: 0.4607\n",
      "Epoch 4/150\n",
      "595/595 [==============================] - 0s - loss: 0.1975 - val_loss: 0.4136\n",
      "Epoch 5/150\n",
      "595/595 [==============================] - 0s - loss: 0.1896 - val_loss: 0.3817\n",
      "Epoch 6/150\n",
      "595/595 [==============================] - 0s - loss: 0.1834 - val_loss: 0.3610\n",
      "Epoch 7/150\n",
      "595/595 [==============================] - 0s - loss: 0.1773 - val_loss: 0.3461\n",
      "Epoch 8/150\n",
      "595/595 [==============================] - 0s - loss: 0.1714 - val_loss: 0.3318\n",
      "Epoch 9/150\n",
      "595/595 [==============================] - 0s - loss: 0.1660 - val_loss: 0.3167\n",
      "Epoch 10/150\n",
      "595/595 [==============================] - 0s - loss: 0.1610 - val_loss: 0.3020\n",
      "Epoch 11/150\n",
      "595/595 [==============================] - 0s - loss: 0.1563 - val_loss: 0.2886\n",
      "Epoch 12/150\n",
      "595/595 [==============================] - 0s - loss: 0.1517 - val_loss: 0.2759\n",
      "Epoch 13/150\n",
      "595/595 [==============================] - 0s - loss: 0.1472 - val_loss: 0.2637\n",
      "Epoch 14/150\n",
      "595/595 [==============================] - 0s - loss: 0.1428 - val_loss: 0.2531\n",
      "Epoch 15/150\n",
      "595/595 [==============================] - 0s - loss: 0.1383 - val_loss: 0.2432\n",
      "Epoch 16/150\n",
      "595/595 [==============================] - 0s - loss: 0.1335 - val_loss: 0.2323\n",
      "Epoch 17/150\n",
      "595/595 [==============================] - 0s - loss: 0.1285 - val_loss: 0.2196\n",
      "Epoch 18/150\n",
      "595/595 [==============================] - 0s - loss: 0.1235 - val_loss: 0.2068\n",
      "Epoch 19/150\n",
      "595/595 [==============================] - 0s - loss: 0.1183 - val_loss: 0.1929\n",
      "Epoch 20/150\n",
      "595/595 [==============================] - 0s - loss: 0.1133 - val_loss: 0.1802\n",
      "Epoch 21/150\n",
      "595/595 [==============================] - 0s - loss: 0.1083 - val_loss: 0.1672\n",
      "Epoch 22/150\n",
      "595/595 [==============================] - 0s - loss: 0.1034 - val_loss: 0.1544\n",
      "Epoch 23/150\n",
      "595/595 [==============================] - 0s - loss: 0.0987 - val_loss: 0.1423\n",
      "Epoch 24/150\n",
      "595/595 [==============================] - 0s - loss: 0.0944 - val_loss: 0.1322\n",
      "Epoch 25/150\n",
      "595/595 [==============================] - 0s - loss: 0.0904 - val_loss: 0.1227\n",
      "Epoch 26/150\n",
      "595/595 [==============================] - 0s - loss: 0.0869 - val_loss: 0.1146\n",
      "Epoch 27/150\n",
      "595/595 [==============================] - 0s - loss: 0.0836 - val_loss: 0.1077\n",
      "Epoch 28/150\n",
      "595/595 [==============================] - 0s - loss: 0.0811 - val_loss: 0.1030\n",
      "Epoch 29/150\n",
      "595/595 [==============================] - 0s - loss: 0.0783 - val_loss: 0.0994\n",
      "Epoch 30/150\n",
      "595/595 [==============================] - 0s - loss: 0.0764 - val_loss: 0.0966\n",
      "Epoch 31/150\n",
      "595/595 [==============================] - 0s - loss: 0.0747 - val_loss: 0.0948\n",
      "Epoch 32/150\n",
      "595/595 [==============================] - 0s - loss: 0.0733 - val_loss: 0.0927\n",
      "Epoch 33/150\n",
      "595/595 [==============================] - 0s - loss: 0.0721 - val_loss: 0.0920\n",
      "Epoch 34/150\n",
      "595/595 [==============================] - 0s - loss: 0.0713 - val_loss: 0.0907\n",
      "Epoch 35/150\n",
      "595/595 [==============================] - 0s - loss: 0.0706 - val_loss: 0.0923\n",
      "Epoch 36/150\n",
      "595/595 [==============================] - 0s - loss: 0.0703 - val_loss: 0.0907\n",
      "Epoch 37/150\n",
      "595/595 [==============================] - 0s - loss: 0.0699 - val_loss: 0.0936\n",
      "Epoch 38/150\n",
      "595/595 [==============================] - 0s - loss: 0.0703 - val_loss: 0.0912\n",
      "Epoch 39/150\n",
      "595/595 [==============================] - 0s - loss: 0.0707 - val_loss: 0.0923\n",
      "Epoch 40/150\n",
      "595/595 [==============================] - 0s - loss: 0.0714 - val_loss: 0.0903\n",
      "Epoch 41/150\n",
      "595/595 [==============================] - 0s - loss: 0.0697 - val_loss: 0.0907\n",
      "Epoch 42/150\n",
      "595/595 [==============================] - 0s - loss: 0.0692 - val_loss: 0.0912\n",
      "Epoch 43/150\n",
      "595/595 [==============================] - 0s - loss: 0.0685 - val_loss: 0.0896\n",
      "Epoch 44/150\n",
      "595/595 [==============================] - 0s - loss: 0.0672 - val_loss: 0.0892\n",
      "Epoch 45/150\n",
      "595/595 [==============================] - 0s - loss: 0.0664 - val_loss: 0.0898\n",
      "Epoch 46/150\n",
      "595/595 [==============================] - 0s - loss: 0.0648 - val_loss: 0.0894\n",
      "Epoch 47/150\n",
      "595/595 [==============================] - 0s - loss: 0.0661 - val_loss: 0.0932\n",
      "Epoch 48/150\n",
      "595/595 [==============================] - 0s - loss: 0.0647 - val_loss: 0.0915\n",
      "Epoch 49/150\n",
      "595/595 [==============================] - 0s - loss: 0.0674 - val_loss: 0.0993\n",
      "Epoch 50/150\n",
      "595/595 [==============================] - 0s - loss: 0.0684 - val_loss: 0.0939\n",
      "Epoch 51/150\n",
      "595/595 [==============================] - 0s - loss: 0.0727 - val_loss: 0.0978\n",
      "Epoch 52/150\n",
      "595/595 [==============================] - 0s - loss: 0.0761 - val_loss: 0.0860\n",
      "Epoch 53/150\n",
      "595/595 [==============================] - 0s - loss: 0.0674 - val_loss: 0.0881\n",
      "Epoch 54/150\n",
      "595/595 [==============================] - 0s - loss: 0.0694 - val_loss: 0.0858\n",
      "Epoch 55/150\n",
      "595/595 [==============================] - 0s - loss: 0.0624 - val_loss: 0.0874\n",
      "Epoch 56/150\n",
      "595/595 [==============================] - 0s - loss: 0.0633 - val_loss: 0.0872\n",
      "Epoch 57/150\n",
      "595/595 [==============================] - 0s - loss: 0.0614 - val_loss: 0.0894\n",
      "Epoch 58/150\n",
      "595/595 [==============================] - 0s - loss: 0.0617 - val_loss: 0.0877\n",
      "Epoch 59/150\n",
      "595/595 [==============================] - 0s - loss: 0.0612 - val_loss: 0.0906\n",
      "Epoch 60/150\n",
      "595/595 [==============================] - 0s - loss: 0.0620 - val_loss: 0.0879\n",
      "Epoch 61/150\n",
      "595/595 [==============================] - 0s - loss: 0.0619 - val_loss: 0.0907\n",
      "Epoch 62/150\n",
      "595/595 [==============================] - 0s - loss: 0.0629 - val_loss: 0.0876\n",
      "Epoch 63/150\n",
      "595/595 [==============================] - 0s - loss: 0.0619 - val_loss: 0.0901\n",
      "Epoch 64/150\n",
      "595/595 [==============================] - 0s - loss: 0.0630 - val_loss: 0.0871\n",
      "Epoch 65/150\n",
      "595/595 [==============================] - 0s - loss: 0.0610 - val_loss: 0.0894\n",
      "Epoch 66/150\n",
      "595/595 [==============================] - 0s - loss: 0.0621 - val_loss: 0.0871\n",
      "Epoch 67/150\n",
      "595/595 [==============================] - 0s - loss: 0.0600 - val_loss: 0.0892\n",
      "Epoch 68/150\n",
      "595/595 [==============================] - 0s - loss: 0.0609 - val_loss: 0.0872\n",
      "Epoch 69/150\n",
      "595/595 [==============================] - 0s - loss: 0.0593 - val_loss: 0.0890\n",
      "Epoch 70/150\n",
      "595/595 [==============================] - 0s - loss: 0.0602 - val_loss: 0.0872\n",
      "Epoch 71/150\n",
      "595/595 [==============================] - 0s - loss: 0.0586 - val_loss: 0.0886\n",
      "Epoch 72/150\n",
      "595/595 [==============================] - 0s - loss: 0.0593 - val_loss: 0.0870\n",
      "Epoch 73/150\n",
      "595/595 [==============================] - 0s - loss: 0.0580 - val_loss: 0.0885\n",
      "Epoch 74/150\n",
      "595/595 [==============================] - 0s - loss: 0.0587 - val_loss: 0.0867\n",
      "Epoch 75/150\n",
      "595/595 [==============================] - 0s - loss: 0.0575 - val_loss: 0.0884\n",
      "Epoch 76/150\n",
      "595/595 [==============================] - 0s - loss: 0.0583 - val_loss: 0.0865\n",
      "Epoch 77/150\n",
      "595/595 [==============================] - 0s - loss: 0.0572 - val_loss: 0.0882\n",
      "Epoch 78/150\n",
      "595/595 [==============================] - 0s - loss: 0.0578 - val_loss: 0.0862\n",
      "Epoch 79/150\n",
      "595/595 [==============================] - 0s - loss: 0.0569 - val_loss: 0.0882\n",
      "Epoch 80/150\n",
      "595/595 [==============================] - 0s - loss: 0.0574 - val_loss: 0.0861\n",
      "Epoch 81/150\n",
      "595/595 [==============================] - 0s - loss: 0.0567 - val_loss: 0.0883\n",
      "Epoch 82/150\n",
      "595/595 [==============================] - 0s - loss: 0.0572 - val_loss: 0.0861\n",
      "Epoch 83/150\n",
      "595/595 [==============================] - 0s - loss: 0.0567 - val_loss: 0.0884\n",
      "Epoch 84/150\n",
      "595/595 [==============================] - 0s - loss: 0.0572 - val_loss: 0.0861\n",
      "Epoch 85/150\n",
      "595/595 [==============================] - 0s - loss: 0.0567 - val_loss: 0.0885\n",
      "Epoch 86/150\n",
      "595/595 [==============================] - 0s - loss: 0.0573 - val_loss: 0.0862\n",
      "Epoch 87/150\n",
      "595/595 [==============================] - 0s - loss: 0.0567 - val_loss: 0.0881\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595/595 [==============================] - 0s - loss: 0.0573 - val_loss: 0.0861\n",
      "Epoch 89/150\n",
      "595/595 [==============================] - 0s - loss: 0.0565 - val_loss: 0.0875\n",
      "Epoch 90/150\n",
      "595/595 [==============================] - 0s - loss: 0.0571 - val_loss: 0.0856\n",
      "Epoch 91/150\n",
      "595/595 [==============================] - 0s - loss: 0.0561 - val_loss: 0.0872\n",
      "Epoch 92/150\n",
      "595/595 [==============================] - 0s - loss: 0.0568 - val_loss: 0.0851\n",
      "Epoch 93/150\n",
      "595/595 [==============================] - 0s - loss: 0.0556 - val_loss: 0.0870\n",
      "Epoch 94/150\n",
      "595/595 [==============================] - 0s - loss: 0.0566 - val_loss: 0.0850\n",
      "Epoch 95/150\n",
      "595/595 [==============================] - 0s - loss: 0.0551 - val_loss: 0.0866\n",
      "Epoch 96/150\n",
      "595/595 [==============================] - 0s - loss: 0.0560 - val_loss: 0.0846\n",
      "Epoch 97/150\n",
      "595/595 [==============================] - 0s - loss: 0.0545 - val_loss: 0.0858\n",
      "Epoch 98/150\n",
      "595/595 [==============================] - 0s - loss: 0.0552 - val_loss: 0.0843\n",
      "Epoch 99/150\n",
      "595/595 [==============================] - 0s - loss: 0.0538 - val_loss: 0.0853\n",
      "Epoch 100/150\n",
      "595/595 [==============================] - 0s - loss: 0.0542 - val_loss: 0.0839\n",
      "Epoch 101/150\n",
      "595/595 [==============================] - 0s - loss: 0.0533 - val_loss: 0.0849\n",
      "Epoch 102/150\n",
      "595/595 [==============================] - 0s - loss: 0.0536 - val_loss: 0.0835\n",
      "Epoch 103/150\n",
      "595/595 [==============================] - 0s - loss: 0.0529 - val_loss: 0.0845\n",
      "Epoch 104/150\n",
      "595/595 [==============================] - 0s - loss: 0.0529 - val_loss: 0.0830\n",
      "Epoch 105/150\n",
      "595/595 [==============================] - 0s - loss: 0.0525 - val_loss: 0.0845\n",
      "Epoch 106/150\n",
      "595/595 [==============================] - 0s - loss: 0.0524 - val_loss: 0.0829\n",
      "Epoch 107/150\n",
      "595/595 [==============================] - 0s - loss: 0.0525 - val_loss: 0.0847\n",
      "Epoch 108/150\n",
      "595/595 [==============================] - 0s - loss: 0.0522 - val_loss: 0.0827\n",
      "Epoch 109/150\n",
      "595/595 [==============================] - 0s - loss: 0.0525 - val_loss: 0.0847\n",
      "Epoch 110/150\n",
      "595/595 [==============================] - 0s - loss: 0.0520 - val_loss: 0.0825\n",
      "Epoch 111/150\n",
      "595/595 [==============================] - 0s - loss: 0.0526 - val_loss: 0.0847\n",
      "Epoch 112/150\n",
      "595/595 [==============================] - 0s - loss: 0.0519 - val_loss: 0.0825\n",
      "Epoch 113/150\n",
      "595/595 [==============================] - 0s - loss: 0.0526 - val_loss: 0.0846\n",
      "Epoch 114/150\n",
      "595/595 [==============================] - 0s - loss: 0.0517 - val_loss: 0.0825\n",
      "Epoch 115/150\n",
      "595/595 [==============================] - 0s - loss: 0.0527 - val_loss: 0.0845\n",
      "Epoch 116/150\n",
      "595/595 [==============================] - 0s - loss: 0.0518 - val_loss: 0.0825\n",
      "Epoch 117/150\n",
      "595/595 [==============================] - 0s - loss: 0.0530 - val_loss: 0.0846\n",
      "Epoch 118/150\n",
      "595/595 [==============================] - 0s - loss: 0.0522 - val_loss: 0.0826\n",
      "Epoch 119/150\n",
      "595/595 [==============================] - 0s - loss: 0.0534 - val_loss: 0.0848\n",
      "Epoch 120/150\n",
      "595/595 [==============================] - 0s - loss: 0.0532 - val_loss: 0.0823\n",
      "Epoch 121/150\n",
      "595/595 [==============================] - 0s - loss: 0.0542 - val_loss: 0.0851\n",
      "Epoch 122/150\n",
      "595/595 [==============================] - 0s - loss: 0.0546 - val_loss: 0.0816\n",
      "Epoch 123/150\n",
      "595/595 [==============================] - 0s - loss: 0.0539 - val_loss: 0.0844\n",
      "Epoch 124/150\n",
      "595/595 [==============================] - 0s - loss: 0.0547 - val_loss: 0.0814\n",
      "Epoch 125/150\n",
      "595/595 [==============================] - 0s - loss: 0.0529 - val_loss: 0.0836\n",
      "Epoch 126/150\n",
      "595/595 [==============================] - 0s - loss: 0.0538 - val_loss: 0.0813\n",
      "Epoch 127/150\n",
      "595/595 [==============================] - 0s - loss: 0.0516 - val_loss: 0.0825\n",
      "Epoch 128/150\n",
      "595/595 [==============================] - 0s - loss: 0.0524 - val_loss: 0.0808\n",
      "Epoch 129/150\n",
      "595/595 [==============================] - 0s - loss: 0.0509 - val_loss: 0.0818\n",
      "Epoch 130/150\n",
      "595/595 [==============================] - 0s - loss: 0.0515 - val_loss: 0.0804\n",
      "Epoch 131/150\n",
      "595/595 [==============================] - 0s - loss: 0.0504 - val_loss: 0.0811\n",
      "Epoch 132/150\n",
      "595/595 [==============================] - 0s - loss: 0.0504 - val_loss: 0.0800\n",
      "Epoch 133/150\n",
      "595/595 [==============================] - 0s - loss: 0.0498 - val_loss: 0.0808\n",
      "Epoch 134/150\n",
      "595/595 [==============================] - 0s - loss: 0.0495 - val_loss: 0.0797\n",
      "Epoch 135/150\n",
      "595/595 [==============================] - 0s - loss: 0.0491 - val_loss: 0.0806\n",
      "Epoch 136/150\n",
      "595/595 [==============================] - 0s - loss: 0.0487 - val_loss: 0.0792\n",
      "Epoch 137/150\n",
      "595/595 [==============================] - 0s - loss: 0.0486 - val_loss: 0.0802\n",
      "Epoch 138/150\n",
      "595/595 [==============================] - 0s - loss: 0.0480 - val_loss: 0.0789\n",
      "Epoch 139/150\n",
      "595/595 [==============================] - 0s - loss: 0.0482 - val_loss: 0.0800\n",
      "Epoch 140/150\n",
      "595/595 [==============================] - 0s - loss: 0.0474 - val_loss: 0.0785\n",
      "Epoch 141/150\n",
      "595/595 [==============================] - 0s - loss: 0.0477 - val_loss: 0.0793\n",
      "Epoch 142/150\n",
      "595/595 [==============================] - 0s - loss: 0.0466 - val_loss: 0.0783\n",
      "Epoch 143/150\n",
      "595/595 [==============================] - 0s - loss: 0.0471 - val_loss: 0.0787\n",
      "Epoch 144/150\n",
      "595/595 [==============================] - 0s - loss: 0.0461 - val_loss: 0.0778\n",
      "Epoch 145/150\n",
      "595/595 [==============================] - 0s - loss: 0.0467 - val_loss: 0.0781\n",
      "Epoch 146/150\n",
      "595/595 [==============================] - 0s - loss: 0.0456 - val_loss: 0.0776\n",
      "Epoch 147/150\n",
      "595/595 [==============================] - 0s - loss: 0.0462 - val_loss: 0.0774\n",
      "Epoch 148/150\n",
      "595/595 [==============================] - 0s - loss: 0.0454 - val_loss: 0.0777\n",
      "Epoch 149/150\n",
      "595/595 [==============================] - 0s - loss: 0.0455 - val_loss: 0.0772\n",
      "Epoch 150/150\n",
      "595/595 [==============================] - 0s - loss: 0.0456 - val_loss: 0.0777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 18/22 [07:16<01:31, 22.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 595 samples, validate on 149 samples\n",
      "Epoch 1/150\n",
      "595/595 [==============================] - 6s - loss: 0.3449 - val_loss: 0.5047\n",
      "Epoch 2/150\n",
      "595/595 [==============================] - 0s - loss: 0.2504 - val_loss: 0.3783\n",
      "Epoch 3/150\n",
      "595/595 [==============================] - 0s - loss: 0.1924 - val_loss: 0.2862\n",
      "Epoch 4/150\n",
      "595/595 [==============================] - 0s - loss: 0.1645 - val_loss: 0.2338\n",
      "Epoch 5/150\n",
      "595/595 [==============================] - 0s - loss: 0.1537 - val_loss: 0.2139\n",
      "Epoch 6/150\n",
      "595/595 [==============================] - 0s - loss: 0.1490 - val_loss: 0.2138\n",
      "Epoch 7/150\n",
      "595/595 [==============================] - 0s - loss: 0.1455 - val_loss: 0.2204\n",
      "Epoch 8/150\n",
      "595/595 [==============================] - 0s - loss: 0.1426 - val_loss: 0.2264\n",
      "Epoch 9/150\n",
      "595/595 [==============================] - 0s - loss: 0.1402 - val_loss: 0.2294\n",
      "Epoch 10/150\n",
      "595/595 [==============================] - 0s - loss: 0.1380 - val_loss: 0.2286\n",
      "Epoch 11/150\n",
      "595/595 [==============================] - 0s - loss: 0.1359 - val_loss: 0.2249\n",
      "Epoch 12/150\n",
      "595/595 [==============================] - 0s - loss: 0.1340 - val_loss: 0.2203\n",
      "Epoch 13/150\n",
      "595/595 [==============================] - 0s - loss: 0.1322 - val_loss: 0.2158\n",
      "Epoch 14/150\n",
      "595/595 [==============================] - 0s - loss: 0.1304 - val_loss: 0.2119\n",
      "Epoch 15/150\n",
      "595/595 [==============================] - 0s - loss: 0.1286 - val_loss: 0.2082\n",
      "Epoch 16/150\n",
      "595/595 [==============================] - 0s - loss: 0.1268 - val_loss: 0.2052\n",
      "Epoch 17/150\n",
      "595/595 [==============================] - 0s - loss: 0.1251 - val_loss: 0.2019\n",
      "Epoch 18/150\n",
      "595/595 [==============================] - 0s - loss: 0.1233 - val_loss: 0.1973\n",
      "Epoch 19/150\n",
      "595/595 [==============================] - 0s - loss: 0.1215 - val_loss: 0.1924\n",
      "Epoch 20/150\n",
      "595/595 [==============================] - 0s - loss: 0.1198 - val_loss: 0.1882\n",
      "Epoch 21/150\n",
      "595/595 [==============================] - 0s - loss: 0.1182 - val_loss: 0.1840\n",
      "Epoch 22/150\n",
      "595/595 [==============================] - 0s - loss: 0.1166 - val_loss: 0.1799\n",
      "Epoch 23/150\n",
      "595/595 [==============================] - 0s - loss: 0.1150 - val_loss: 0.1763\n",
      "Epoch 24/150\n",
      "595/595 [==============================] - 0s - loss: 0.1135 - val_loss: 0.1725\n",
      "Epoch 25/150\n",
      "595/595 [==============================] - 0s - loss: 0.1120 - val_loss: 0.1691\n",
      "Epoch 26/150\n",
      "595/595 [==============================] - 0s - loss: 0.1105 - val_loss: 0.1653\n",
      "Epoch 27/150\n",
      "595/595 [==============================] - 0s - loss: 0.1090 - val_loss: 0.1617\n",
      "Epoch 28/150\n",
      "595/595 [==============================] - 0s - loss: 0.1076 - val_loss: 0.1579\n",
      "Epoch 29/150\n",
      "595/595 [==============================] - 0s - loss: 0.1062 - val_loss: 0.1545\n",
      "Epoch 30/150\n",
      "595/595 [==============================] - 0s - loss: 0.1048 - val_loss: 0.1508\n",
      "Epoch 31/150\n",
      "595/595 [==============================] - 0s - loss: 0.1035 - val_loss: 0.1476\n",
      "Epoch 32/150\n",
      "595/595 [==============================] - 0s - loss: 0.1021 - val_loss: 0.1439\n",
      "Epoch 33/150\n",
      "595/595 [==============================] - 0s - loss: 0.1008 - val_loss: 0.1407\n",
      "Epoch 34/150\n",
      "595/595 [==============================] - 0s - loss: 0.0996 - val_loss: 0.1370\n",
      "Epoch 35/150\n",
      "595/595 [==============================] - 0s - loss: 0.0984 - val_loss: 0.1337\n",
      "Epoch 36/150\n",
      "595/595 [==============================] - 0s - loss: 0.0972 - val_loss: 0.1301\n",
      "Epoch 37/150\n",
      "595/595 [==============================] - 0s - loss: 0.0961 - val_loss: 0.1274\n",
      "Epoch 38/150\n",
      "595/595 [==============================] - 0s - loss: 0.0949 - val_loss: 0.1239\n",
      "Epoch 39/150\n",
      "595/595 [==============================] - 0s - loss: 0.0940 - val_loss: 0.1221\n",
      "Epoch 40/150\n",
      "595/595 [==============================] - 0s - loss: 0.0930 - val_loss: 0.1201\n",
      "Epoch 41/150\n",
      "595/595 [==============================] - 0s - loss: 0.0921 - val_loss: 0.1177\n",
      "Epoch 42/150\n",
      "595/595 [==============================] - 0s - loss: 0.0912 - val_loss: 0.1164\n",
      "Epoch 43/150\n",
      "595/595 [==============================] - 0s - loss: 0.0903 - val_loss: 0.1141\n",
      "Epoch 44/150\n",
      "595/595 [==============================] - 0s - loss: 0.0895 - val_loss: 0.1131\n",
      "Epoch 45/150\n",
      "595/595 [==============================] - 0s - loss: 0.0887 - val_loss: 0.1112\n",
      "Epoch 46/150\n",
      "595/595 [==============================] - 0s - loss: 0.0880 - val_loss: 0.1102\n",
      "Epoch 47/150\n",
      "595/595 [==============================] - 0s - loss: 0.0872 - val_loss: 0.1089\n",
      "Epoch 48/150\n",
      "595/595 [==============================] - 0s - loss: 0.0864 - val_loss: 0.1078\n",
      "Epoch 49/150\n",
      "595/595 [==============================] - 0s - loss: 0.0858 - val_loss: 0.1064\n",
      "Epoch 50/150\n",
      "595/595 [==============================] - 0s - loss: 0.0851 - val_loss: 0.1060\n",
      "Epoch 51/150\n",
      "595/595 [==============================] - 0s - loss: 0.0844 - val_loss: 0.1050\n",
      "Epoch 52/150\n",
      "595/595 [==============================] - 0s - loss: 0.0838 - val_loss: 0.1050\n",
      "Epoch 53/150\n",
      "595/595 [==============================] - 0s - loss: 0.0832 - val_loss: 0.1040\n",
      "Epoch 54/150\n",
      "595/595 [==============================] - 0s - loss: 0.0827 - val_loss: 0.1035\n",
      "Epoch 55/150\n",
      "595/595 [==============================] - 0s - loss: 0.0821 - val_loss: 0.1033\n",
      "Epoch 56/150\n",
      "595/595 [==============================] - 0s - loss: 0.0814 - val_loss: 0.1031\n",
      "Epoch 57/150\n",
      "595/595 [==============================] - 0s - loss: 0.0809 - val_loss: 0.1023\n",
      "Epoch 58/150\n",
      "595/595 [==============================] - 0s - loss: 0.0806 - val_loss: 0.1022\n",
      "Epoch 59/150\n",
      "595/595 [==============================] - 0s - loss: 0.0799 - val_loss: 0.1028\n",
      "Epoch 60/150\n",
      "595/595 [==============================] - 0s - loss: 0.0792 - val_loss: 0.1022\n",
      "Epoch 61/150\n",
      "595/595 [==============================] - 0s - loss: 0.0790 - val_loss: 0.1020\n",
      "Epoch 62/150\n",
      "595/595 [==============================] - 0s - loss: 0.0786 - val_loss: 0.1022\n",
      "Epoch 63/150\n",
      "595/595 [==============================] - 0s - loss: 0.0780 - val_loss: 0.1030\n",
      "Epoch 64/150\n",
      "595/595 [==============================] - 0s - loss: 0.0773 - val_loss: 0.1028\n",
      "Epoch 65/150\n",
      "595/595 [==============================] - 0s - loss: 0.0770 - val_loss: 0.1022\n",
      "Epoch 66/150\n",
      "595/595 [==============================] - 0s - loss: 0.0768 - val_loss: 0.1025\n",
      "Epoch 67/150\n",
      "595/595 [==============================] - 0s - loss: 0.0762 - val_loss: 0.1036\n",
      "Epoch 68/150\n",
      "595/595 [==============================] - 0s - loss: 0.0755 - val_loss: 0.1032\n",
      "Epoch 69/150\n",
      "595/595 [==============================] - 0s - loss: 0.0754 - val_loss: 0.1034\n",
      "Epoch 70/150\n",
      "595/595 [==============================] - 0s - loss: 0.0752 - val_loss: 0.1030\n",
      "Epoch 71/150\n",
      "595/595 [==============================] - 0s - loss: 0.0748 - val_loss: 0.1044\n",
      "Epoch 72/150\n",
      "595/595 [==============================] - 0s - loss: 0.0739 - val_loss: 0.1047\n",
      "Epoch 73/150\n",
      "595/595 [==============================] - 0s - loss: 0.0736 - val_loss: 0.1036\n",
      "Epoch 74/150\n",
      "595/595 [==============================] - 0s - loss: 0.0738 - val_loss: 0.1048\n",
      "Epoch 75/150\n",
      "595/595 [==============================] - 0s - loss: 0.0730 - val_loss: 0.1056\n",
      "Epoch 76/150\n",
      "595/595 [==============================] - 0s - loss: 0.0724 - val_loss: 0.1059\n",
      "Epoch 77/150\n",
      "595/595 [==============================] - 0s - loss: 0.0721 - val_loss: 0.1049\n",
      "Epoch 78/150\n",
      "595/595 [==============================] - 0s - loss: 0.0721 - val_loss: 0.1059\n",
      "Epoch 79/150\n",
      "595/595 [==============================] - 0s - loss: 0.0717 - val_loss: 0.1074\n",
      "Epoch 80/150\n",
      "595/595 [==============================] - 0s - loss: 0.0708 - val_loss: 0.1072\n",
      "Epoch 81/150\n",
      "595/595 [==============================] - 0s - loss: 0.0707 - val_loss: 0.1059\n",
      "Epoch 82/150\n",
      "595/595 [==============================] - 0s - loss: 0.0708 - val_loss: 0.1076\n",
      "Epoch 83/150\n",
      "595/595 [==============================] - 0s - loss: 0.0702 - val_loss: 0.1089\n",
      "Epoch 84/150\n",
      "595/595 [==============================] - 0s - loss: 0.0695 - val_loss: 0.1088\n",
      "Epoch 85/150\n",
      "595/595 [==============================] - 0s - loss: 0.0693 - val_loss: 0.1076\n",
      "Epoch 86/150\n",
      "595/595 [==============================] - 0s - loss: 0.0695 - val_loss: 0.1087\n",
      "Epoch 87/150\n",
      "595/595 [==============================] - 0s - loss: 0.0691 - val_loss: 0.1105\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595/595 [==============================] - 0s - loss: 0.0682 - val_loss: 0.1104\n",
      "Epoch 89/150\n",
      "595/595 [==============================] - 0s - loss: 0.0680 - val_loss: 0.1095\n",
      "Epoch 90/150\n",
      "595/595 [==============================] - 0s - loss: 0.0681 - val_loss: 0.1098\n",
      "Epoch 91/150\n",
      "595/595 [==============================] - 0s - loss: 0.0680 - val_loss: 0.1113\n",
      "Epoch 92/150\n",
      "595/595 [==============================] - 0s - loss: 0.0675 - val_loss: 0.1132\n",
      "Epoch 93/150\n",
      "595/595 [==============================] - 0s - loss: 0.0668 - val_loss: 0.1108\n",
      "Epoch 94/150\n",
      "595/595 [==============================] - 0s - loss: 0.0669 - val_loss: 0.1114\n",
      "Epoch 95/150\n",
      "595/595 [==============================] - 0s - loss: 0.0668 - val_loss: 0.1117\n",
      "Epoch 96/150\n",
      "595/595 [==============================] - 0s - loss: 0.0671 - val_loss: 0.1142\n",
      "Epoch 97/150\n",
      "595/595 [==============================] - 0s - loss: 0.0658 - val_loss: 0.1136\n",
      "Epoch 98/150\n",
      "595/595 [==============================] - 0s - loss: 0.0656 - val_loss: 0.1115\n",
      "Epoch 99/150\n",
      "595/595 [==============================] - 0s - loss: 0.0660 - val_loss: 0.1122\n",
      "Epoch 100/150\n",
      "595/595 [==============================] - 0s - loss: 0.0662 - val_loss: 0.1162\n",
      "Epoch 101/150\n",
      "595/595 [==============================] - 0s - loss: 0.0647 - val_loss: 0.1148\n",
      "Epoch 102/150\n",
      "595/595 [==============================] - 0s - loss: 0.0648 - val_loss: 0.1118\n",
      "Epoch 103/150\n",
      "595/595 [==============================] - 0s - loss: 0.0655 - val_loss: 0.1139\n",
      "Epoch 104/150\n",
      "595/595 [==============================] - 0s - loss: 0.0652 - val_loss: 0.1179\n",
      "Epoch 105/150\n",
      "595/595 [==============================] - 0s - loss: 0.0639 - val_loss: 0.1149\n",
      "Epoch 106/150\n",
      "595/595 [==============================] - 0s - loss: 0.0641 - val_loss: 0.1132\n",
      "Epoch 107/150\n",
      "595/595 [==============================] - 0s - loss: 0.0646 - val_loss: 0.1158\n",
      "Epoch 108/150\n",
      "595/595 [==============================] - 0s - loss: 0.0643 - val_loss: 0.1192\n",
      "Epoch 109/150\n",
      "595/595 [==============================] - 0s - loss: 0.0631 - val_loss: 0.1148\n",
      "Epoch 110/150\n",
      "595/595 [==============================] - 0s - loss: 0.0634 - val_loss: 0.1135\n",
      "Epoch 111/150\n",
      "595/595 [==============================] - 0s - loss: 0.0642 - val_loss: 0.1185\n",
      "Epoch 112/150\n",
      "595/595 [==============================] - 0s - loss: 0.0631 - val_loss: 0.1199\n",
      "Epoch 113/150\n",
      "595/595 [==============================] - 0s - loss: 0.0624 - val_loss: 0.1147\n",
      "Epoch 114/150\n",
      "595/595 [==============================] - 0s - loss: 0.0629 - val_loss: 0.1152\n",
      "Epoch 115/150\n",
      "595/595 [==============================] - 0s - loss: 0.0634 - val_loss: 0.1206\n",
      "Epoch 116/150\n",
      "595/595 [==============================] - 0s - loss: 0.0618 - val_loss: 0.1196\n",
      "Epoch 117/150\n",
      "595/595 [==============================] - 0s - loss: 0.0618 - val_loss: 0.1149\n",
      "Epoch 118/150\n",
      "595/595 [==============================] - 0s - loss: 0.0623 - val_loss: 0.1178\n",
      "Epoch 119/150\n",
      "595/595 [==============================] - 0s - loss: 0.0623 - val_loss: 0.1215\n",
      "Epoch 120/150\n",
      "595/595 [==============================] - 0s - loss: 0.0611 - val_loss: 0.1186\n",
      "Epoch 121/150\n",
      "595/595 [==============================] - 0s - loss: 0.0613 - val_loss: 0.1157\n",
      "Epoch 122/150\n",
      "595/595 [==============================] - 0s - loss: 0.0620 - val_loss: 0.1189\n",
      "Epoch 123/150\n",
      "595/595 [==============================] - 0s - loss: 0.0617 - val_loss: 0.1226\n",
      "Epoch 124/150\n",
      "595/595 [==============================] - 0s - loss: 0.0605 - val_loss: 0.1178\n",
      "Epoch 125/150\n",
      "595/595 [==============================] - 0s - loss: 0.0609 - val_loss: 0.1161\n",
      "Epoch 126/150\n",
      "595/595 [==============================] - 0s - loss: 0.0618 - val_loss: 0.1217\n",
      "Epoch 127/150\n",
      "595/595 [==============================] - 0s - loss: 0.0605 - val_loss: 0.1223\n",
      "Epoch 128/150\n",
      "595/595 [==============================] - 0s - loss: 0.0601 - val_loss: 0.1168\n",
      "Epoch 129/150\n",
      "595/595 [==============================] - 0s - loss: 0.0607 - val_loss: 0.1178\n",
      "Epoch 130/150\n",
      "595/595 [==============================] - 0s - loss: 0.0610 - val_loss: 0.1222\n",
      "Epoch 131/150\n",
      "595/595 [==============================] - 0s - loss: 0.0598 - val_loss: 0.1214\n",
      "Epoch 132/150\n",
      "595/595 [==============================] - 0s - loss: 0.0597 - val_loss: 0.1167\n",
      "Epoch 133/150\n",
      "595/595 [==============================] - 0s - loss: 0.0605 - val_loss: 0.1190\n",
      "Epoch 134/150\n",
      "595/595 [==============================] - 0s - loss: 0.0608 - val_loss: 0.1234\n",
      "Epoch 135/150\n",
      "595/595 [==============================] - 0s - loss: 0.0590 - val_loss: 0.1198\n",
      "Epoch 136/150\n",
      "595/595 [==============================] - 0s - loss: 0.0593 - val_loss: 0.1170\n",
      "Epoch 137/150\n",
      "595/595 [==============================] - 0s - loss: 0.0600 - val_loss: 0.1206\n",
      "Epoch 138/150\n",
      "595/595 [==============================] - 0s - loss: 0.0596 - val_loss: 0.1236\n",
      "Epoch 139/150\n",
      "595/595 [==============================] - 0s - loss: 0.0585 - val_loss: 0.1186\n",
      "Epoch 140/150\n",
      "595/595 [==============================] - 0s - loss: 0.0589 - val_loss: 0.1174\n",
      "Epoch 141/150\n",
      "595/595 [==============================] - 0s - loss: 0.0601 - val_loss: 0.1222\n",
      "Epoch 142/150\n",
      "595/595 [==============================] - 0s - loss: 0.0589 - val_loss: 0.1225\n",
      "Epoch 143/150\n",
      "595/595 [==============================] - 0s - loss: 0.0586 - val_loss: 0.1175\n",
      "Epoch 144/150\n",
      "595/595 [==============================] - 0s - loss: 0.0591 - val_loss: 0.1187\n",
      "Epoch 145/150\n",
      "595/595 [==============================] - 0s - loss: 0.0596 - val_loss: 0.1228\n",
      "Epoch 146/150\n",
      "595/595 [==============================] - 0s - loss: 0.0580 - val_loss: 0.1211\n",
      "Epoch 147/150\n",
      "595/595 [==============================] - 0s - loss: 0.0578 - val_loss: 0.1177\n",
      "Epoch 148/150\n",
      "595/595 [==============================] - 0s - loss: 0.0586 - val_loss: 0.1199\n",
      "Epoch 149/150\n",
      "595/595 [==============================] - 0s - loss: 0.0586 - val_loss: 0.1237\n",
      "Epoch 150/150\n",
      "595/595 [==============================] - 0s - loss: 0.0574 - val_loss: 0.1195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▋ | 19/22 [07:38<01:07, 22.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 595 samples, validate on 149 samples\n",
      "Epoch 1/150\n",
      "595/595 [==============================] - 6s - loss: 0.2660 - val_loss: 0.4793\n",
      "Epoch 2/150\n",
      "595/595 [==============================] - 0s - loss: 0.2163 - val_loss: 0.4036\n",
      "Epoch 3/150\n",
      "595/595 [==============================] - 0s - loss: 0.1947 - val_loss: 0.3545\n",
      "Epoch 4/150\n",
      "595/595 [==============================] - 0s - loss: 0.1839 - val_loss: 0.3251\n",
      "Epoch 5/150\n",
      "595/595 [==============================] - 0s - loss: 0.1760 - val_loss: 0.3081\n",
      "Epoch 6/150\n",
      "595/595 [==============================] - 0s - loss: 0.1689 - val_loss: 0.2968\n",
      "Epoch 7/150\n",
      "595/595 [==============================] - 0s - loss: 0.1621 - val_loss: 0.2854\n",
      "Epoch 8/150\n",
      "595/595 [==============================] - 0s - loss: 0.1552 - val_loss: 0.2710\n",
      "Epoch 9/150\n",
      "595/595 [==============================] - 0s - loss: 0.1482 - val_loss: 0.2547\n",
      "Epoch 10/150\n",
      "595/595 [==============================] - 0s - loss: 0.1411 - val_loss: 0.2367\n",
      "Epoch 11/150\n",
      "595/595 [==============================] - 0s - loss: 0.1339 - val_loss: 0.2181\n",
      "Epoch 12/150\n",
      "595/595 [==============================] - 0s - loss: 0.1263 - val_loss: 0.1975\n",
      "Epoch 13/150\n",
      "595/595 [==============================] - 0s - loss: 0.1188 - val_loss: 0.1777\n",
      "Epoch 14/150\n",
      "595/595 [==============================] - 0s - loss: 0.1115 - val_loss: 0.1577\n",
      "Epoch 15/150\n",
      "595/595 [==============================] - 0s - loss: 0.1046 - val_loss: 0.1410\n",
      "Epoch 16/150\n",
      "595/595 [==============================] - 0s - loss: 0.0980 - val_loss: 0.1263\n",
      "Epoch 17/150\n",
      "595/595 [==============================] - 0s - loss: 0.0919 - val_loss: 0.1134\n",
      "Epoch 18/150\n",
      "595/595 [==============================] - 0s - loss: 0.0863 - val_loss: 0.1028\n",
      "Epoch 19/150\n",
      "595/595 [==============================] - 0s - loss: 0.0813 - val_loss: 0.0942\n",
      "Epoch 20/150\n",
      "595/595 [==============================] - 0s - loss: 0.0769 - val_loss: 0.0869\n",
      "Epoch 21/150\n",
      "595/595 [==============================] - 0s - loss: 0.0732 - val_loss: 0.0821\n",
      "Epoch 22/150\n",
      "595/595 [==============================] - 0s - loss: 0.0702 - val_loss: 0.0778\n",
      "Epoch 23/150\n",
      "595/595 [==============================] - 0s - loss: 0.0675 - val_loss: 0.0753\n",
      "Epoch 24/150\n",
      "595/595 [==============================] - 0s - loss: 0.0653 - val_loss: 0.0735\n",
      "Epoch 25/150\n",
      "595/595 [==============================] - 0s - loss: 0.0634 - val_loss: 0.0730\n",
      "Epoch 26/150\n",
      "595/595 [==============================] - 0s - loss: 0.0619 - val_loss: 0.0727\n",
      "Epoch 27/150\n",
      "595/595 [==============================] - 0s - loss: 0.0607 - val_loss: 0.0723\n",
      "Epoch 28/150\n",
      "595/595 [==============================] - 0s - loss: 0.0596 - val_loss: 0.0732\n",
      "Epoch 29/150\n",
      "595/595 [==============================] - 0s - loss: 0.0586 - val_loss: 0.0729\n",
      "Epoch 30/150\n",
      "595/595 [==============================] - 0s - loss: 0.0579 - val_loss: 0.0740\n",
      "Epoch 31/150\n",
      "595/595 [==============================] - 0s - loss: 0.0572 - val_loss: 0.0740\n",
      "Epoch 32/150\n",
      "595/595 [==============================] - 0s - loss: 0.0564 - val_loss: 0.0751\n",
      "Epoch 33/150\n",
      "595/595 [==============================] - 0s - loss: 0.0561 - val_loss: 0.0749\n",
      "Epoch 34/150\n",
      "595/595 [==============================] - 0s - loss: 0.0552 - val_loss: 0.0764\n",
      "Epoch 35/150\n",
      "595/595 [==============================] - 0s - loss: 0.0548 - val_loss: 0.0763\n",
      "Epoch 36/150\n",
      "595/595 [==============================] - 0s - loss: 0.0542 - val_loss: 0.0770\n",
      "Epoch 37/150\n",
      "595/595 [==============================] - 0s - loss: 0.0535 - val_loss: 0.0776\n",
      "Epoch 38/150\n",
      "595/595 [==============================] - 0s - loss: 0.0531 - val_loss: 0.0781\n",
      "Epoch 39/150\n",
      "595/595 [==============================] - 0s - loss: 0.0527 - val_loss: 0.0784\n",
      "Epoch 40/150\n",
      "595/595 [==============================] - 0s - loss: 0.0523 - val_loss: 0.0793\n",
      "Epoch 41/150\n",
      "595/595 [==============================] - 0s - loss: 0.0520 - val_loss: 0.0795\n",
      "Epoch 42/150\n",
      "595/595 [==============================] - 0s - loss: 0.0516 - val_loss: 0.0797\n",
      "Epoch 43/150\n",
      "595/595 [==============================] - 0s - loss: 0.0512 - val_loss: 0.0810\n",
      "Epoch 44/150\n",
      "595/595 [==============================] - 0s - loss: 0.0509 - val_loss: 0.0807\n",
      "Epoch 45/150\n",
      "595/595 [==============================] - 0s - loss: 0.0505 - val_loss: 0.0808\n",
      "Epoch 46/150\n",
      "595/595 [==============================] - 0s - loss: 0.0502 - val_loss: 0.0823\n",
      "Epoch 47/150\n",
      "595/595 [==============================] - 0s - loss: 0.0496 - val_loss: 0.0818\n",
      "Epoch 48/150\n",
      "595/595 [==============================] - 0s - loss: 0.0495 - val_loss: 0.0815\n",
      "Epoch 49/150\n",
      "595/595 [==============================] - 0s - loss: 0.0489 - val_loss: 0.0846\n",
      "Epoch 50/150\n",
      "595/595 [==============================] - 0s - loss: 0.0488 - val_loss: 0.0824\n",
      "Epoch 51/150\n",
      "595/595 [==============================] - 0s - loss: 0.0488 - val_loss: 0.0825\n",
      "Epoch 52/150\n",
      "595/595 [==============================] - 0s - loss: 0.0480 - val_loss: 0.0887\n",
      "Epoch 53/150\n",
      "595/595 [==============================] - 0s - loss: 0.0487 - val_loss: 0.0833\n",
      "Epoch 54/150\n",
      "595/595 [==============================] - 0s - loss: 0.0494 - val_loss: 0.0837\n",
      "Epoch 55/150\n",
      "595/595 [==============================] - 0s - loss: 0.0474 - val_loss: 0.0908\n",
      "Epoch 56/150\n",
      "595/595 [==============================] - 0s - loss: 0.0490 - val_loss: 0.0840\n",
      "Epoch 57/150\n",
      "595/595 [==============================] - 0s - loss: 0.0491 - val_loss: 0.0864\n",
      "Epoch 58/150\n",
      "595/595 [==============================] - 0s - loss: 0.0471 - val_loss: 0.0881\n",
      "Epoch 59/150\n",
      "595/595 [==============================] - 0s - loss: 0.0475 - val_loss: 0.0845\n",
      "Epoch 60/150\n",
      "595/595 [==============================] - 0s - loss: 0.0475 - val_loss: 0.0902\n",
      "Epoch 61/150\n",
      "595/595 [==============================] - 0s - loss: 0.0466 - val_loss: 0.0872\n",
      "Epoch 62/150\n",
      "595/595 [==============================] - 0s - loss: 0.0467 - val_loss: 0.0852\n",
      "Epoch 63/150\n",
      "595/595 [==============================] - 0s - loss: 0.0460 - val_loss: 0.0920\n",
      "Epoch 64/150\n",
      "595/595 [==============================] - 0s - loss: 0.0460 - val_loss: 0.0878\n",
      "Epoch 65/150\n",
      "595/595 [==============================] - 0s - loss: 0.0460 - val_loss: 0.0872\n",
      "Epoch 66/150\n",
      "595/595 [==============================] - 0s - loss: 0.0448 - val_loss: 0.0927\n",
      "Epoch 67/150\n",
      "595/595 [==============================] - 0s - loss: 0.0452 - val_loss: 0.0879\n",
      "Epoch 68/150\n",
      "595/595 [==============================] - 0s - loss: 0.0455 - val_loss: 0.0889\n",
      "Epoch 69/150\n",
      "595/595 [==============================] - 0s - loss: 0.0440 - val_loss: 0.0944\n",
      "Epoch 70/150\n",
      "595/595 [==============================] - 0s - loss: 0.0450 - val_loss: 0.0882\n",
      "Epoch 71/150\n",
      "595/595 [==============================] - 0s - loss: 0.0456 - val_loss: 0.0915\n",
      "Epoch 72/150\n",
      "595/595 [==============================] - 0s - loss: 0.0440 - val_loss: 0.0935\n",
      "Epoch 73/150\n",
      "595/595 [==============================] - 0s - loss: 0.0446 - val_loss: 0.0879\n",
      "Epoch 74/150\n",
      "595/595 [==============================] - 0s - loss: 0.0453 - val_loss: 0.0946\n",
      "Epoch 75/150\n",
      "595/595 [==============================] - 0s - loss: 0.0441 - val_loss: 0.0923\n",
      "Epoch 76/150\n",
      "595/595 [==============================] - 0s - loss: 0.0444 - val_loss: 0.0884\n",
      "Epoch 77/150\n",
      "595/595 [==============================] - 0s - loss: 0.0447 - val_loss: 0.0975\n",
      "Epoch 78/150\n",
      "595/595 [==============================] - 0s - loss: 0.0442 - val_loss: 0.0916\n",
      "Epoch 79/150\n",
      "595/595 [==============================] - 0s - loss: 0.0444 - val_loss: 0.0903\n",
      "Epoch 80/150\n",
      "595/595 [==============================] - 0s - loss: 0.0433 - val_loss: 0.0963\n",
      "Epoch 81/150\n",
      "595/595 [==============================] - 0s - loss: 0.0432 - val_loss: 0.0911\n",
      "Epoch 82/150\n",
      "595/595 [==============================] - 0s - loss: 0.0436 - val_loss: 0.0929\n",
      "Epoch 83/150\n",
      "595/595 [==============================] - 0s - loss: 0.0419 - val_loss: 0.0962\n",
      "Epoch 84/150\n",
      "595/595 [==============================] - 0s - loss: 0.0425 - val_loss: 0.0911\n",
      "Epoch 85/150\n",
      "595/595 [==============================] - 0s - loss: 0.0430 - val_loss: 0.0952\n",
      "Epoch 86/150\n",
      "595/595 [==============================] - 0s - loss: 0.0416 - val_loss: 0.0960\n",
      "Epoch 87/150\n",
      "595/595 [==============================] - 0s - loss: 0.0422 - val_loss: 0.0906\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595/595 [==============================] - 0s - loss: 0.0433 - val_loss: 0.0978\n",
      "Epoch 89/150\n",
      "595/595 [==============================] - 0s - loss: 0.0422 - val_loss: 0.0951\n",
      "Epoch 90/150\n",
      "595/595 [==============================] - 0s - loss: 0.0427 - val_loss: 0.0906\n",
      "Epoch 91/150\n",
      "595/595 [==============================] - 0s - loss: 0.0432 - val_loss: 0.1000\n",
      "Epoch 92/150\n",
      "595/595 [==============================] - 0s - loss: 0.0425 - val_loss: 0.0940\n",
      "Epoch 93/150\n",
      "595/595 [==============================] - 0s - loss: 0.0429 - val_loss: 0.0924\n",
      "Epoch 94/150\n",
      "595/595 [==============================] - 0s - loss: 0.0418 - val_loss: 0.0988\n",
      "Epoch 95/150\n",
      "595/595 [==============================] - 0s - loss: 0.0416 - val_loss: 0.0934\n",
      "Epoch 96/150\n",
      "595/595 [==============================] - 0s - loss: 0.0421 - val_loss: 0.0947\n",
      "Epoch 97/150\n",
      "595/595 [==============================] - 0s - loss: 0.0405 - val_loss: 0.0984\n",
      "Epoch 98/150\n",
      "595/595 [==============================] - 0s - loss: 0.0409 - val_loss: 0.0932\n",
      "Epoch 99/150\n",
      "595/595 [==============================] - 0s - loss: 0.0416 - val_loss: 0.0965\n",
      "Epoch 100/150\n",
      "595/595 [==============================] - 0s - loss: 0.0401 - val_loss: 0.0991\n",
      "Epoch 101/150\n",
      "595/595 [==============================] - 0s - loss: 0.0408 - val_loss: 0.0927\n",
      "Epoch 102/150\n",
      "595/595 [==============================] - 0s - loss: 0.0421 - val_loss: 0.0978\n",
      "Epoch 103/150\n",
      "595/595 [==============================] - 0s - loss: 0.0403 - val_loss: 0.0986\n",
      "Epoch 104/150\n",
      "595/595 [==============================] - 0s - loss: 0.0412 - val_loss: 0.0926\n",
      "Epoch 105/150\n",
      "595/595 [==============================] - 0s - loss: 0.0420 - val_loss: 0.0999\n",
      "Epoch 106/150\n",
      "595/595 [==============================] - 0s - loss: 0.0407 - val_loss: 0.0964\n",
      "Epoch 107/150\n",
      "595/595 [==============================] - 0s - loss: 0.0413 - val_loss: 0.0933\n",
      "Epoch 108/150\n",
      "595/595 [==============================] - 0s - loss: 0.0410 - val_loss: 0.1011\n",
      "Epoch 109/150\n",
      "595/595 [==============================] - 0s - loss: 0.0404 - val_loss: 0.0955\n",
      "Epoch 110/150\n",
      "595/595 [==============================] - 0s - loss: 0.0410 - val_loss: 0.0945\n",
      "Epoch 111/150\n",
      "595/595 [==============================] - 0s - loss: 0.0398 - val_loss: 0.1016\n",
      "Epoch 112/150\n",
      "595/595 [==============================] - 0s - loss: 0.0401 - val_loss: 0.0958\n",
      "Epoch 113/150\n",
      "595/595 [==============================] - 0s - loss: 0.0405 - val_loss: 0.0959\n",
      "Epoch 114/150\n",
      "595/595 [==============================] - 0s - loss: 0.0390 - val_loss: 0.1016\n",
      "Epoch 115/150\n",
      "595/595 [==============================] - 0s - loss: 0.0397 - val_loss: 0.0952\n",
      "Epoch 116/150\n",
      "595/595 [==============================] - 0s - loss: 0.0406 - val_loss: 0.0973\n",
      "Epoch 117/150\n",
      "595/595 [==============================] - 0s - loss: 0.0388 - val_loss: 0.1017\n",
      "Epoch 118/150\n",
      "595/595 [==============================] - 0s - loss: 0.0397 - val_loss: 0.0946\n",
      "Epoch 119/150\n",
      "595/595 [==============================] - 0s - loss: 0.0408 - val_loss: 0.0987\n",
      "Epoch 120/150\n",
      "595/595 [==============================] - 0s - loss: 0.0388 - val_loss: 0.1008\n",
      "Epoch 121/150\n",
      "595/595 [==============================] - 0s - loss: 0.0396 - val_loss: 0.0944\n",
      "Epoch 122/150\n",
      "595/595 [==============================] - 0s - loss: 0.0410 - val_loss: 0.1000\n",
      "Epoch 123/150\n",
      "595/595 [==============================] - 0s - loss: 0.0388 - val_loss: 0.0992\n",
      "Epoch 124/150\n",
      "595/595 [==============================] - 0s - loss: 0.0397 - val_loss: 0.0941\n",
      "Epoch 125/150\n",
      "595/595 [==============================] - 0s - loss: 0.0404 - val_loss: 0.1017\n",
      "Epoch 126/150\n",
      "595/595 [==============================] - 0s - loss: 0.0389 - val_loss: 0.0980\n",
      "Epoch 127/150\n",
      "595/595 [==============================] - 0s - loss: 0.0397 - val_loss: 0.0953\n",
      "Epoch 128/150\n",
      "595/595 [==============================] - 0s - loss: 0.0393 - val_loss: 0.1026\n",
      "Epoch 129/150\n",
      "595/595 [==============================] - 0s - loss: 0.0387 - val_loss: 0.0974\n",
      "Epoch 130/150\n",
      "595/595 [==============================] - 0s - loss: 0.0395 - val_loss: 0.0957\n",
      "Epoch 131/150\n",
      "595/595 [==============================] - 0s - loss: 0.0385 - val_loss: 0.1030\n",
      "Epoch 132/150\n",
      "595/595 [==============================] - 0s - loss: 0.0387 - val_loss: 0.0972\n",
      "Epoch 133/150\n",
      "595/595 [==============================] - 0s - loss: 0.0393 - val_loss: 0.0972\n",
      "Epoch 134/150\n",
      "595/595 [==============================] - 0s - loss: 0.0378 - val_loss: 0.1030\n",
      "Epoch 135/150\n",
      "595/595 [==============================] - 0s - loss: 0.0384 - val_loss: 0.0970\n",
      "Epoch 136/150\n",
      "595/595 [==============================] - 0s - loss: 0.0392 - val_loss: 0.0982\n",
      "Epoch 137/150\n",
      "595/595 [==============================] - 0s - loss: 0.0373 - val_loss: 0.1026\n",
      "Epoch 138/150\n",
      "595/595 [==============================] - 0s - loss: 0.0382 - val_loss: 0.0964\n",
      "Epoch 139/150\n",
      "595/595 [==============================] - 0s - loss: 0.0395 - val_loss: 0.0999\n",
      "Epoch 140/150\n",
      "595/595 [==============================] - 0s - loss: 0.0374 - val_loss: 0.1022\n",
      "Epoch 141/150\n",
      "595/595 [==============================] - 0s - loss: 0.0382 - val_loss: 0.0957\n",
      "Epoch 142/150\n",
      "595/595 [==============================] - ETA: 0s - loss: 0.042 - 0s - loss: 0.0396 - val_loss: 0.1012\n",
      "Epoch 143/150\n",
      "595/595 [==============================] - 0s - loss: 0.0374 - val_loss: 0.1008\n",
      "Epoch 144/150\n",
      "595/595 [==============================] - 0s - loss: 0.0385 - val_loss: 0.0955\n",
      "Epoch 145/150\n",
      "595/595 [==============================] - 0s - loss: 0.0392 - val_loss: 0.1027\n",
      "Epoch 146/150\n",
      "595/595 [==============================] - 0s - loss: 0.0376 - val_loss: 0.0992\n",
      "Epoch 147/150\n",
      "595/595 [==============================] - 0s - loss: 0.0385 - val_loss: 0.0963\n",
      "Epoch 148/150\n",
      "595/595 [==============================] - 0s - loss: 0.0380 - val_loss: 0.1036\n",
      "Epoch 149/150\n",
      "595/595 [==============================] - 0s - loss: 0.0374 - val_loss: 0.0984\n",
      "Epoch 150/150\n",
      "595/595 [==============================] - 0s - loss: 0.0384 - val_loss: 0.0970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████ | 20/22 [07:59<00:44, 22.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 595 samples, validate on 149 samples\n",
      "Epoch 1/150\n",
      "595/595 [==============================] - 16s - loss: 0.3392 - val_loss: 0.5056\n",
      "Epoch 2/150\n",
      "595/595 [==============================] - 0s - loss: 0.2479 - val_loss: 0.3697\n",
      "Epoch 3/150\n",
      "595/595 [==============================] - 0s - loss: 0.1962 - val_loss: 0.2715\n",
      "Epoch 4/150\n",
      "595/595 [==============================] - 0s - loss: 0.1697 - val_loss: 0.2188\n",
      "Epoch 5/150\n",
      "595/595 [==============================] - 0s - loss: 0.1480 - val_loss: 0.2008\n",
      "Epoch 6/150\n",
      "595/595 [==============================] - 0s - loss: 0.1320 - val_loss: 0.1940\n",
      "Epoch 7/150\n",
      "595/595 [==============================] - 0s - loss: 0.1226 - val_loss: 0.1825\n",
      "Epoch 8/150\n",
      "595/595 [==============================] - 0s - loss: 0.1148 - val_loss: 0.1675\n",
      "Epoch 9/150\n",
      "595/595 [==============================] - 0s - loss: 0.1075 - val_loss: 0.1549\n",
      "Epoch 10/150\n",
      "595/595 [==============================] - 0s - loss: 0.1016 - val_loss: 0.1448\n",
      "Epoch 11/150\n",
      "595/595 [==============================] - 0s - loss: 0.0960 - val_loss: 0.1368\n",
      "Epoch 12/150\n",
      "595/595 [==============================] - 0s - loss: 0.0911 - val_loss: 0.1295\n",
      "Epoch 13/150\n",
      "595/595 [==============================] - 0s - loss: 0.0868 - val_loss: 0.1205\n",
      "Epoch 14/150\n",
      "595/595 [==============================] - 0s - loss: 0.0831 - val_loss: 0.1148\n",
      "Epoch 15/150\n",
      "595/595 [==============================] - 0s - loss: 0.0802 - val_loss: 0.1102\n",
      "Epoch 16/150\n",
      "595/595 [==============================] - 0s - loss: 0.0777 - val_loss: 0.1059\n",
      "Epoch 17/150\n",
      "595/595 [==============================] - 0s - loss: 0.0757 - val_loss: 0.1011\n",
      "Epoch 18/150\n",
      "595/595 [==============================] - 0s - loss: 0.0738 - val_loss: 0.0988\n",
      "Epoch 19/150\n",
      "595/595 [==============================] - 0s - loss: 0.0723 - val_loss: 0.0961\n",
      "Epoch 20/150\n",
      "595/595 [==============================] - 0s - loss: 0.0708 - val_loss: 0.0951\n",
      "Epoch 21/150\n",
      "595/595 [==============================] - 0s - loss: 0.0697 - val_loss: 0.0928\n",
      "Epoch 22/150\n",
      "595/595 [==============================] - 0s - loss: 0.0683 - val_loss: 0.0920\n",
      "Epoch 23/150\n",
      "595/595 [==============================] - 0s - loss: 0.0673 - val_loss: 0.0910\n",
      "Epoch 24/150\n",
      "595/595 [==============================] - 0s - loss: 0.0664 - val_loss: 0.0900\n",
      "Epoch 25/150\n",
      "595/595 [==============================] - 0s - loss: 0.0653 - val_loss: 0.0893\n",
      "Epoch 26/150\n",
      "595/595 [==============================] - 0s - loss: 0.0645 - val_loss: 0.0885\n",
      "Epoch 27/150\n",
      "595/595 [==============================] - 0s - loss: 0.0637 - val_loss: 0.0879\n",
      "Epoch 28/150\n",
      "595/595 [==============================] - 0s - loss: 0.0629 - val_loss: 0.0871\n",
      "Epoch 29/150\n",
      "595/595 [==============================] - 0s - loss: 0.0622 - val_loss: 0.0867\n",
      "Epoch 30/150\n",
      "595/595 [==============================] - 0s - loss: 0.0616 - val_loss: 0.0858\n",
      "Epoch 31/150\n",
      "595/595 [==============================] - 0s - loss: 0.0609 - val_loss: 0.0855\n",
      "Epoch 32/150\n",
      "595/595 [==============================] - 0s - loss: 0.0602 - val_loss: 0.0857\n",
      "Epoch 33/150\n",
      "595/595 [==============================] - 0s - loss: 0.0599 - val_loss: 0.0842\n",
      "Epoch 34/150\n",
      "595/595 [==============================] - 0s - loss: 0.0592 - val_loss: 0.0845\n",
      "Epoch 35/150\n",
      "595/595 [==============================] - 0s - loss: 0.0586 - val_loss: 0.0835\n",
      "Epoch 36/150\n",
      "595/595 [==============================] - 0s - loss: 0.0580 - val_loss: 0.0846\n",
      "Epoch 37/150\n",
      "595/595 [==============================] - 0s - loss: 0.0579 - val_loss: 0.0835\n",
      "Epoch 38/150\n",
      "595/595 [==============================] - 0s - loss: 0.0574 - val_loss: 0.0830\n",
      "Epoch 39/150\n",
      "595/595 [==============================] - 0s - loss: 0.0567 - val_loss: 0.0831\n",
      "Epoch 40/150\n",
      "595/595 [==============================] - 0s - loss: 0.0563 - val_loss: 0.0843\n",
      "Epoch 41/150\n",
      "595/595 [==============================] - 0s - loss: 0.0563 - val_loss: 0.0825\n",
      "Epoch 42/150\n",
      "595/595 [==============================] - 0s - loss: 0.0558 - val_loss: 0.0830\n",
      "Epoch 43/150\n",
      "595/595 [==============================] - 0s - loss: 0.0552 - val_loss: 0.0828\n",
      "Epoch 44/150\n",
      "595/595 [==============================] - 0s - loss: 0.0547 - val_loss: 0.0844\n",
      "Epoch 45/150\n",
      "595/595 [==============================] - 0s - loss: 0.0548 - val_loss: 0.0830\n",
      "Epoch 46/150\n",
      "595/595 [==============================] - 0s - loss: 0.0547 - val_loss: 0.0835\n",
      "Epoch 47/150\n",
      "595/595 [==============================] - 0s - loss: 0.0542 - val_loss: 0.0825\n",
      "Epoch 48/150\n",
      "595/595 [==============================] - 0s - loss: 0.0536 - val_loss: 0.0857\n",
      "Epoch 49/150\n",
      "595/595 [==============================] - 0s - loss: 0.0543 - val_loss: 0.0829\n",
      "Epoch 50/150\n",
      "595/595 [==============================] - 0s - loss: 0.0536 - val_loss: 0.0850\n",
      "Epoch 51/150\n",
      "595/595 [==============================] - 0s - loss: 0.0536 - val_loss: 0.0829\n",
      "Epoch 52/150\n",
      "595/595 [==============================] - 0s - loss: 0.0527 - val_loss: 0.0857\n",
      "Epoch 53/150\n",
      "595/595 [==============================] - 0s - loss: 0.0531 - val_loss: 0.0829\n",
      "Epoch 54/150\n",
      "595/595 [==============================] - 0s - loss: 0.0524 - val_loss: 0.0857\n",
      "Epoch 55/150\n",
      "595/595 [==============================] - 0s - loss: 0.0526 - val_loss: 0.0829\n",
      "Epoch 56/150\n",
      "595/595 [==============================] - 0s - loss: 0.0515 - val_loss: 0.0870\n",
      "Epoch 57/150\n",
      "595/595 [==============================] - 0s - loss: 0.0523 - val_loss: 0.0833\n",
      "Epoch 58/150\n",
      "595/595 [==============================] - 0s - loss: 0.0517 - val_loss: 0.0881\n",
      "Epoch 59/150\n",
      "595/595 [==============================] - 0s - loss: 0.0524 - val_loss: 0.0832\n",
      "Epoch 60/150\n",
      "595/595 [==============================] - 0s - loss: 0.0513 - val_loss: 0.0897\n",
      "Epoch 61/150\n",
      "595/595 [==============================] - 0s - loss: 0.0528 - val_loss: 0.0842\n",
      "Epoch 62/150\n",
      "595/595 [==============================] - 0s - loss: 0.0521 - val_loss: 0.0888\n",
      "Epoch 63/150\n",
      "595/595 [==============================] - 0s - loss: 0.0526 - val_loss: 0.0836\n",
      "Epoch 64/150\n",
      "595/595 [==============================] - 0s - loss: 0.0511 - val_loss: 0.0880\n",
      "Epoch 65/150\n",
      "595/595 [==============================] - 0s - loss: 0.0516 - val_loss: 0.0837\n",
      "Epoch 66/150\n",
      "595/595 [==============================] - 0s - loss: 0.0501 - val_loss: 0.0871\n",
      "Epoch 67/150\n",
      "595/595 [==============================] - 0s - loss: 0.0503 - val_loss: 0.0840\n",
      "Epoch 68/150\n",
      "595/595 [==============================] - 0s - loss: 0.0492 - val_loss: 0.0869\n",
      "Epoch 69/150\n",
      "595/595 [==============================] - 0s - loss: 0.0494 - val_loss: 0.0845\n",
      "Epoch 70/150\n",
      "595/595 [==============================] - 0s - loss: 0.0486 - val_loss: 0.0879\n",
      "Epoch 71/150\n",
      "595/595 [==============================] - 0s - loss: 0.0492 - val_loss: 0.0848\n",
      "Epoch 72/150\n",
      "595/595 [==============================] - 0s - loss: 0.0483 - val_loss: 0.0881\n",
      "Epoch 73/150\n",
      "595/595 [==============================] - 0s - loss: 0.0487 - val_loss: 0.0859\n",
      "Epoch 74/150\n",
      "595/595 [==============================] - 0s - loss: 0.0482 - val_loss: 0.0896\n",
      "Epoch 75/150\n",
      "595/595 [==============================] - 0s - loss: 0.0493 - val_loss: 0.0861\n",
      "Epoch 76/150\n",
      "595/595 [==============================] - 0s - loss: 0.0484 - val_loss: 0.0884\n",
      "Epoch 77/150\n",
      "595/595 [==============================] - 0s - loss: 0.0481 - val_loss: 0.0873\n",
      "Epoch 78/150\n",
      "595/595 [==============================] - 0s - loss: 0.0480 - val_loss: 0.0906\n",
      "Epoch 79/150\n",
      "595/595 [==============================] - 0s - loss: 0.0494 - val_loss: 0.0864\n",
      "Epoch 80/150\n",
      "595/595 [==============================] - 0s - loss: 0.0473 - val_loss: 0.0883\n",
      "Epoch 81/150\n",
      "595/595 [==============================] - 0s - loss: 0.0470 - val_loss: 0.0889\n",
      "Epoch 82/150\n",
      "595/595 [==============================] - 0s - loss: 0.0475 - val_loss: 0.0902\n",
      "Epoch 83/150\n",
      "595/595 [==============================] - 0s - loss: 0.0484 - val_loss: 0.0869\n",
      "Epoch 84/150\n",
      "595/595 [==============================] - 0s - loss: 0.0460 - val_loss: 0.0891\n",
      "Epoch 85/150\n",
      "595/595 [==============================] - 0s - loss: 0.0466 - val_loss: 0.0902\n",
      "Epoch 86/150\n",
      "595/595 [==============================] - 0s - loss: 0.0470 - val_loss: 0.0898\n",
      "Epoch 87/150\n",
      "595/595 [==============================] - 0s - loss: 0.0473 - val_loss: 0.0878\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595/595 [==============================] - 0s - loss: 0.0453 - val_loss: 0.0895\n",
      "Epoch 89/150\n",
      "595/595 [==============================] - 0s - loss: 0.0460 - val_loss: 0.0907\n",
      "Epoch 90/150\n",
      "595/595 [==============================] - 0s - loss: 0.0463 - val_loss: 0.0905\n",
      "Epoch 91/150\n",
      "595/595 [==============================] - 0s - loss: 0.0464 - val_loss: 0.0882\n",
      "Epoch 92/150\n",
      "595/595 [==============================] - 0s - loss: 0.0447 - val_loss: 0.0910\n",
      "Epoch 93/150\n",
      "595/595 [==============================] - 0s - loss: 0.0456 - val_loss: 0.0915\n",
      "Epoch 94/150\n",
      "595/595 [==============================] - 0s - loss: 0.0459 - val_loss: 0.0917\n",
      "Epoch 95/150\n",
      "595/595 [==============================] - 0s - loss: 0.0466 - val_loss: 0.0894\n",
      "Epoch 96/150\n",
      "595/595 [==============================] - 0s - loss: 0.0449 - val_loss: 0.0922\n",
      "Epoch 97/150\n",
      "595/595 [==============================] - 0s - loss: 0.0461 - val_loss: 0.0919\n",
      "Epoch 98/150\n",
      "595/595 [==============================] - 0s - loss: 0.0459 - val_loss: 0.0920\n",
      "Epoch 99/150\n",
      "595/595 [==============================] - 0s - loss: 0.0459 - val_loss: 0.0898\n",
      "Epoch 100/150\n",
      "595/595 [==============================] - 0s - loss: 0.0443 - val_loss: 0.0930\n",
      "Epoch 101/150\n",
      "595/595 [==============================] - 0s - loss: 0.0460 - val_loss: 0.0923\n",
      "Epoch 102/150\n",
      "595/595 [==============================] - 0s - loss: 0.0457 - val_loss: 0.0918\n",
      "Epoch 103/150\n",
      "595/595 [==============================] - 0s - loss: 0.0451 - val_loss: 0.0905\n",
      "Epoch 104/150\n",
      "595/595 [==============================] - 0s - loss: 0.0438 - val_loss: 0.0927\n",
      "Epoch 105/150\n",
      "595/595 [==============================] - 0s - loss: 0.0449 - val_loss: 0.0925\n",
      "Epoch 106/150\n",
      "595/595 [==============================] - 0s - loss: 0.0449 - val_loss: 0.0916\n",
      "Epoch 107/150\n",
      "595/595 [==============================] - 0s - loss: 0.0437 - val_loss: 0.0908\n",
      "Epoch 108/150\n",
      "595/595 [==============================] - 0s - loss: 0.0430 - val_loss: 0.0944\n",
      "Epoch 109/150\n",
      "595/595 [==============================] - 0s - loss: 0.0448 - val_loss: 0.0941\n",
      "Epoch 110/150\n",
      "595/595 [==============================] - 0s - loss: 0.0455 - val_loss: 0.0914\n",
      "Epoch 111/150\n",
      "595/595 [==============================] - 0s - loss: 0.0429 - val_loss: 0.0916\n",
      "Epoch 112/150\n",
      "595/595 [==============================] - 0s - loss: 0.0434 - val_loss: 0.0969\n",
      "Epoch 113/150\n",
      "595/595 [==============================] - 0s - loss: 0.0457 - val_loss: 0.0920\n",
      "Epoch 114/150\n",
      "595/595 [==============================] - 0s - loss: 0.0444 - val_loss: 0.0924\n",
      "Epoch 115/150\n",
      "595/595 [==============================] - 0s - loss: 0.0424 - val_loss: 0.0926\n",
      "Epoch 116/150\n",
      "595/595 [==============================] - 0s - loss: 0.0435 - val_loss: 0.0967\n",
      "Epoch 117/150\n",
      "595/595 [==============================] - 0s - loss: 0.0450 - val_loss: 0.0925\n",
      "Epoch 118/150\n",
      "595/595 [==============================] - 0s - loss: 0.0431 - val_loss: 0.0954\n",
      "Epoch 119/150\n",
      "595/595 [==============================] - 0s - loss: 0.0438 - val_loss: 0.0950\n",
      "Epoch 120/150\n",
      "595/595 [==============================] - 0s - loss: 0.0449 - val_loss: 0.0966\n",
      "Epoch 121/150\n",
      "595/595 [==============================] - 0s - loss: 0.0452 - val_loss: 0.0942\n",
      "Epoch 122/150\n",
      "595/595 [==============================] - 0s - loss: 0.0444 - val_loss: 0.0964\n",
      "Epoch 123/150\n",
      "595/595 [==============================] - 0s - loss: 0.0450 - val_loss: 0.0939\n",
      "Epoch 124/150\n",
      "595/595 [==============================] - 0s - loss: 0.0444 - val_loss: 0.0942\n",
      "Epoch 125/150\n",
      "595/595 [==============================] - 0s - loss: 0.0433 - val_loss: 0.0939\n",
      "Epoch 126/150\n",
      "595/595 [==============================] - 0s - loss: 0.0428 - val_loss: 0.0941\n",
      "Epoch 127/150\n",
      "595/595 [==============================] - 0s - loss: 0.0422 - val_loss: 0.0938\n",
      "Epoch 128/150\n",
      "595/595 [==============================] - 0s - loss: 0.0420 - val_loss: 0.0945\n",
      "Epoch 129/150\n",
      "595/595 [==============================] - 0s - loss: 0.0417 - val_loss: 0.0938\n",
      "Epoch 130/150\n",
      "595/595 [==============================] - 0s - loss: 0.0414 - val_loss: 0.0946\n",
      "Epoch 131/150\n",
      "595/595 [==============================] - 0s - loss: 0.0415 - val_loss: 0.0951\n",
      "Epoch 132/150\n",
      "595/595 [==============================] - 0s - loss: 0.0420 - val_loss: 0.0946\n",
      "Epoch 133/150\n",
      "595/595 [==============================] - 0s - loss: 0.0414 - val_loss: 0.0937\n",
      "Epoch 134/150\n",
      "595/595 [==============================] - 0s - loss: 0.0410 - val_loss: 0.0959\n",
      "Epoch 135/150\n",
      "595/595 [==============================] - 0s - loss: 0.0421 - val_loss: 0.0971\n",
      "Epoch 136/150\n",
      "595/595 [==============================] - 0s - loss: 0.0433 - val_loss: 0.0936\n",
      "Epoch 137/150\n",
      "595/595 [==============================] - 0s - loss: 0.0409 - val_loss: 0.0941\n",
      "Epoch 138/150\n",
      "595/595 [==============================] - 0s - loss: 0.0408 - val_loss: 0.0978\n",
      "Epoch 139/150\n",
      "595/595 [==============================] - 0s - loss: 0.0429 - val_loss: 0.0976\n",
      "Epoch 140/150\n",
      "595/595 [==============================] - 0s - loss: 0.0442 - val_loss: 0.0940\n",
      "Epoch 141/150\n",
      "595/595 [==============================] - 0s - loss: 0.0404 - val_loss: 0.0943\n",
      "Epoch 142/150\n",
      "595/595 [==============================] - 0s - loss: 0.0415 - val_loss: 0.0999\n",
      "Epoch 143/150\n",
      "595/595 [==============================] - 0s - loss: 0.0435 - val_loss: 0.0947\n",
      "Epoch 144/150\n",
      "595/595 [==============================] - 0s - loss: 0.0419 - val_loss: 0.0937\n",
      "Epoch 145/150\n",
      "595/595 [==============================] - 0s - loss: 0.0394 - val_loss: 0.0951\n",
      "Epoch 146/150\n",
      "595/595 [==============================] - 0s - loss: 0.0412 - val_loss: 0.0992\n",
      "Epoch 147/150\n",
      "595/595 [==============================] - 0s - loss: 0.0428 - val_loss: 0.0942\n",
      "Epoch 148/150\n",
      "595/595 [==============================] - 0s - loss: 0.0404 - val_loss: 0.0943\n",
      "Epoch 149/150\n",
      "595/595 [==============================] - 0s - loss: 0.0390 - val_loss: 0.0970\n",
      "Epoch 150/150\n",
      "595/595 [==============================] - 0s - loss: 0.0412 - val_loss: 0.0987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 21/22 [08:32<00:25, 25.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 595 samples, validate on 149 samples\n",
      "Epoch 1/150\n",
      "595/595 [==============================] - 5s - loss: 0.3103 - val_loss: 0.4610\n",
      "Epoch 2/150\n",
      "595/595 [==============================] - 0s - loss: 0.2365 - val_loss: 0.3485\n",
      "Epoch 3/150\n",
      "595/595 [==============================] - 0s - loss: 0.1926 - val_loss: 0.2647\n",
      "Epoch 4/150\n",
      "595/595 [==============================] - 0s - loss: 0.1625 - val_loss: 0.2087\n",
      "Epoch 5/150\n",
      "595/595 [==============================] - 0s - loss: 0.1424 - val_loss: 0.1744\n",
      "Epoch 6/150\n",
      "595/595 [==============================] - 0s - loss: 0.1296 - val_loss: 0.1517\n",
      "Epoch 7/150\n",
      "595/595 [==============================] - 0s - loss: 0.1201 - val_loss: 0.1363\n",
      "Epoch 8/150\n",
      "595/595 [==============================] - 0s - loss: 0.1120 - val_loss: 0.1248\n",
      "Epoch 9/150\n",
      "595/595 [==============================] - 0s - loss: 0.1054 - val_loss: 0.1130\n",
      "Epoch 10/150\n",
      "595/595 [==============================] - 0s - loss: 0.0995 - val_loss: 0.1028\n",
      "Epoch 11/150\n",
      "595/595 [==============================] - 0s - loss: 0.0939 - val_loss: 0.0968\n",
      "Epoch 12/150\n",
      "595/595 [==============================] - 0s - loss: 0.0890 - val_loss: 0.0925\n",
      "Epoch 13/150\n",
      "595/595 [==============================] - 0s - loss: 0.0849 - val_loss: 0.0869\n",
      "Epoch 14/150\n",
      "595/595 [==============================] - 0s - loss: 0.0810 - val_loss: 0.0824\n",
      "Epoch 15/150\n",
      "595/595 [==============================] - 0s - loss: 0.0777 - val_loss: 0.0792\n",
      "Epoch 16/150\n",
      "595/595 [==============================] - 0s - loss: 0.0750 - val_loss: 0.0761\n",
      "Epoch 17/150\n",
      "595/595 [==============================] - 0s - loss: 0.0726 - val_loss: 0.0736\n",
      "Epoch 18/150\n",
      "595/595 [==============================] - 0s - loss: 0.0707 - val_loss: 0.0718\n",
      "Epoch 19/150\n",
      "595/595 [==============================] - 0s - loss: 0.0690 - val_loss: 0.0704\n",
      "Epoch 20/150\n",
      "595/595 [==============================] - 0s - loss: 0.0675 - val_loss: 0.0688\n",
      "Epoch 21/150\n",
      "595/595 [==============================] - 0s - loss: 0.0660 - val_loss: 0.0676\n",
      "Epoch 22/150\n",
      "595/595 [==============================] - 0s - loss: 0.0647 - val_loss: 0.0668\n",
      "Epoch 23/150\n",
      "595/595 [==============================] - 0s - loss: 0.0637 - val_loss: 0.0656\n",
      "Epoch 24/150\n",
      "595/595 [==============================] - 0s - loss: 0.0626 - val_loss: 0.0662\n",
      "Epoch 25/150\n",
      "595/595 [==============================] - 0s - loss: 0.0622 - val_loss: 0.0641\n",
      "Epoch 26/150\n",
      "595/595 [==============================] - 0s - loss: 0.0610 - val_loss: 0.0661\n",
      "Epoch 27/150\n",
      "595/595 [==============================] - 0s - loss: 0.0610 - val_loss: 0.0634\n",
      "Epoch 28/150\n",
      "595/595 [==============================] - 0s - loss: 0.0598 - val_loss: 0.0661\n",
      "Epoch 29/150\n",
      "595/595 [==============================] - 0s - loss: 0.0600 - val_loss: 0.0629\n",
      "Epoch 30/150\n",
      "595/595 [==============================] - 0s - loss: 0.0587 - val_loss: 0.0657\n",
      "Epoch 31/150\n",
      "595/595 [==============================] - 0s - loss: 0.0590 - val_loss: 0.0628\n",
      "Epoch 32/150\n",
      "595/595 [==============================] - 0s - loss: 0.0579 - val_loss: 0.0658\n",
      "Epoch 33/150\n",
      "595/595 [==============================] - 0s - loss: 0.0582 - val_loss: 0.0623\n",
      "Epoch 34/150\n",
      "595/595 [==============================] - 0s - loss: 0.0569 - val_loss: 0.0654\n",
      "Epoch 35/150\n",
      "595/595 [==============================] - 0s - loss: 0.0572 - val_loss: 0.0623\n",
      "Epoch 36/150\n",
      "595/595 [==============================] - 0s - loss: 0.0559 - val_loss: 0.0647\n",
      "Epoch 37/150\n",
      "595/595 [==============================] - 0s - loss: 0.0562 - val_loss: 0.0622\n",
      "Epoch 38/150\n",
      "595/595 [==============================] - 0s - loss: 0.0549 - val_loss: 0.0637\n",
      "Epoch 39/150\n",
      "595/595 [==============================] - 0s - loss: 0.0551 - val_loss: 0.0628\n",
      "Epoch 40/150\n",
      "595/595 [==============================] - 0s - loss: 0.0541 - val_loss: 0.0630\n",
      "Epoch 41/150\n",
      "595/595 [==============================] - 0s - loss: 0.0541 - val_loss: 0.0629\n",
      "Epoch 42/150\n",
      "595/595 [==============================] - 0s - loss: 0.0534 - val_loss: 0.0626\n",
      "Epoch 43/150\n",
      "595/595 [==============================] - 0s - loss: 0.0533 - val_loss: 0.0630\n",
      "Epoch 44/150\n",
      "595/595 [==============================] - 0s - loss: 0.0528 - val_loss: 0.0628\n",
      "Epoch 45/150\n",
      "595/595 [==============================] - 0s - loss: 0.0526 - val_loss: 0.0637\n",
      "Epoch 46/150\n",
      "595/595 [==============================] - 0s - loss: 0.0523 - val_loss: 0.0632\n",
      "Epoch 47/150\n",
      "595/595 [==============================] - 0s - loss: 0.0521 - val_loss: 0.0643\n",
      "Epoch 48/150\n",
      "595/595 [==============================] - ETA: 0s - loss: 0.050 - 0s - loss: 0.0521 - val_loss: 0.0632\n",
      "Epoch 49/150\n",
      "595/595 [==============================] - 0s - loss: 0.0514 - val_loss: 0.0646\n",
      "Epoch 50/150\n",
      "595/595 [==============================] - 0s - loss: 0.0516 - val_loss: 0.0632\n",
      "Epoch 51/150\n",
      "595/595 [==============================] - 0s - loss: 0.0508 - val_loss: 0.0646\n",
      "Epoch 52/150\n",
      "595/595 [==============================] - 0s - loss: 0.0508 - val_loss: 0.0634\n",
      "Epoch 53/150\n",
      "595/595 [==============================] - 0s - loss: 0.0501 - val_loss: 0.0647\n",
      "Epoch 54/150\n",
      "595/595 [==============================] - 0s - loss: 0.0503 - val_loss: 0.0632\n",
      "Epoch 55/150\n",
      "595/595 [==============================] - 0s - loss: 0.0493 - val_loss: 0.0648\n",
      "Epoch 56/150\n",
      "595/595 [==============================] - 0s - loss: 0.0495 - val_loss: 0.0637\n",
      "Epoch 57/150\n",
      "595/595 [==============================] - 0s - loss: 0.0488 - val_loss: 0.0653\n",
      "Epoch 58/150\n",
      "595/595 [==============================] - 0s - loss: 0.0490 - val_loss: 0.0639\n",
      "Epoch 59/150\n",
      "595/595 [==============================] - 0s - loss: 0.0480 - val_loss: 0.0665\n",
      "Epoch 60/150\n",
      "595/595 [==============================] - 0s - loss: 0.0487 - val_loss: 0.0650\n",
      "Epoch 61/150\n",
      "595/595 [==============================] - 0s - loss: 0.0482 - val_loss: 0.0676\n",
      "Epoch 62/150\n",
      "595/595 [==============================] - 0s - loss: 0.0487 - val_loss: 0.0657\n",
      "Epoch 63/150\n",
      "595/595 [==============================] - 0s - loss: 0.0481 - val_loss: 0.0685\n",
      "Epoch 64/150\n",
      "595/595 [==============================] - 0s - loss: 0.0491 - val_loss: 0.0661\n",
      "Epoch 65/150\n",
      "595/595 [==============================] - 0s - loss: 0.0481 - val_loss: 0.0683\n",
      "Epoch 66/150\n",
      "595/595 [==============================] - 0s - loss: 0.0485 - val_loss: 0.0661\n",
      "Epoch 67/150\n",
      "595/595 [==============================] - 0s - loss: 0.0475 - val_loss: 0.0680\n",
      "Epoch 68/150\n",
      "595/595 [==============================] - 0s - loss: 0.0478 - val_loss: 0.0666\n",
      "Epoch 69/150\n",
      "595/595 [==============================] - 0s - loss: 0.0471 - val_loss: 0.0678\n",
      "Epoch 70/150\n",
      "595/595 [==============================] - 0s - loss: 0.0471 - val_loss: 0.0674\n",
      "Epoch 71/150\n",
      "595/595 [==============================] - 0s - loss: 0.0468 - val_loss: 0.0677\n",
      "Epoch 72/150\n",
      "595/595 [==============================] - 0s - loss: 0.0468 - val_loss: 0.0680\n",
      "Epoch 73/150\n",
      "595/595 [==============================] - 0s - loss: 0.0464 - val_loss: 0.0679\n",
      "Epoch 74/150\n",
      "595/595 [==============================] - 0s - loss: 0.0464 - val_loss: 0.0682\n",
      "Epoch 75/150\n",
      "595/595 [==============================] - 0s - loss: 0.0460 - val_loss: 0.0680\n",
      "Epoch 76/150\n",
      "595/595 [==============================] - 0s - loss: 0.0457 - val_loss: 0.0683\n",
      "Epoch 77/150\n",
      "595/595 [==============================] - 0s - loss: 0.0453 - val_loss: 0.0684\n",
      "Epoch 78/150\n",
      "595/595 [==============================] - 0s - loss: 0.0450 - val_loss: 0.0688\n",
      "Epoch 79/150\n",
      "595/595 [==============================] - 0s - loss: 0.0449 - val_loss: 0.0692\n",
      "Epoch 80/150\n",
      "595/595 [==============================] - 0s - loss: 0.0449 - val_loss: 0.0694\n",
      "Epoch 81/150\n",
      "595/595 [==============================] - 0s - loss: 0.0447 - val_loss: 0.0693\n",
      "Epoch 82/150\n",
      "595/595 [==============================] - 0s - loss: 0.0441 - val_loss: 0.0706\n",
      "Epoch 83/150\n",
      "595/595 [==============================] - 0s - loss: 0.0449 - val_loss: 0.0697\n",
      "Epoch 84/150\n",
      "595/595 [==============================] - 0s - loss: 0.0447 - val_loss: 0.0702\n",
      "Epoch 85/150\n",
      "595/595 [==============================] - 0s - loss: 0.0440 - val_loss: 0.0701\n",
      "Epoch 86/150\n",
      "595/595 [==============================] - 0s - loss: 0.0435 - val_loss: 0.0721\n",
      "Epoch 87/150\n",
      "595/595 [==============================] - 0s - loss: 0.0451 - val_loss: 0.0705\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595/595 [==============================] - 0s - loss: 0.0446 - val_loss: 0.0718\n",
      "Epoch 89/150\n",
      "595/595 [==============================] - 0s - loss: 0.0429 - val_loss: 0.0710\n",
      "Epoch 90/150\n",
      "595/595 [==============================] - 0s - loss: 0.0442 - val_loss: 0.0750\n",
      "Epoch 91/150\n",
      "595/595 [==============================] - 0s - loss: 0.0468 - val_loss: 0.0717\n",
      "Epoch 92/150\n",
      "595/595 [==============================] - 0s - loss: 0.0436 - val_loss: 0.0713\n",
      "Epoch 93/150\n",
      "595/595 [==============================] - 0s - loss: 0.0427 - val_loss: 0.0730\n",
      "Epoch 94/150\n",
      "595/595 [==============================] - 0s - loss: 0.0448 - val_loss: 0.0727\n",
      "Epoch 95/150\n",
      "595/595 [==============================] - 0s - loss: 0.0430 - val_loss: 0.0711\n",
      "Epoch 96/150\n",
      "595/595 [==============================] - 0s - loss: 0.0421 - val_loss: 0.0734\n",
      "Epoch 97/150\n",
      "595/595 [==============================] - 0s - loss: 0.0436 - val_loss: 0.0719\n",
      "Epoch 98/150\n",
      "595/595 [==============================] - 0s - loss: 0.0426 - val_loss: 0.0727\n",
      "Epoch 99/150\n",
      "595/595 [==============================] - 0s - loss: 0.0418 - val_loss: 0.0731\n",
      "Epoch 100/150\n",
      "595/595 [==============================] - 0s - loss: 0.0431 - val_loss: 0.0737\n",
      "Epoch 101/150\n",
      "595/595 [==============================] - 0s - loss: 0.0434 - val_loss: 0.0729\n",
      "Epoch 102/150\n",
      "595/595 [==============================] - 0s - loss: 0.0419 - val_loss: 0.0734\n",
      "Epoch 103/150\n",
      "595/595 [==============================] - 0s - loss: 0.0420 - val_loss: 0.0739\n",
      "Epoch 104/150\n",
      "595/595 [==============================] - 0s - loss: 0.0429 - val_loss: 0.0738\n",
      "Epoch 105/150\n",
      "595/595 [==============================] - 0s - loss: 0.0421 - val_loss: 0.0738\n",
      "Epoch 106/150\n",
      "595/595 [==============================] - 0s - loss: 0.0414 - val_loss: 0.0747\n",
      "Epoch 107/150\n",
      "595/595 [==============================] - 0s - loss: 0.0420 - val_loss: 0.0749\n",
      "Epoch 108/150\n",
      "595/595 [==============================] - 0s - loss: 0.0429 - val_loss: 0.0750\n",
      "Epoch 109/150\n",
      "595/595 [==============================] - 0s - loss: 0.0419 - val_loss: 0.0743\n",
      "Epoch 110/150\n",
      "595/595 [==============================] - 0s - loss: 0.0416 - val_loss: 0.0752\n",
      "Epoch 111/150\n",
      "595/595 [==============================] - 0s - loss: 0.0420 - val_loss: 0.0744\n",
      "Epoch 112/150\n",
      "595/595 [==============================] - 0s - loss: 0.0412 - val_loss: 0.0749\n",
      "Epoch 113/150\n",
      "595/595 [==============================] - 0s - loss: 0.0404 - val_loss: 0.0749\n",
      "Epoch 114/150\n",
      "595/595 [==============================] - 0s - loss: 0.0412 - val_loss: 0.0761\n",
      "Epoch 115/150\n",
      "595/595 [==============================] - 0s - loss: 0.0423 - val_loss: 0.0755\n",
      "Epoch 116/150\n",
      "595/595 [==============================] - 0s - loss: 0.0411 - val_loss: 0.0759\n",
      "Epoch 117/150\n",
      "595/595 [==============================] - 0s - loss: 0.0402 - val_loss: 0.0770\n",
      "Epoch 118/150\n",
      "595/595 [==============================] - 0s - loss: 0.0423 - val_loss: 0.0754\n",
      "Epoch 119/150\n",
      "595/595 [==============================] - 0s - loss: 0.0418 - val_loss: 0.0763\n",
      "Epoch 120/150\n",
      "595/595 [==============================] - 0s - loss: 0.0395 - val_loss: 0.0757\n",
      "Epoch 121/150\n",
      "595/595 [==============================] - 0s - loss: 0.0402 - val_loss: 0.0786\n",
      "Epoch 122/150\n",
      "595/595 [==============================] - 0s - loss: 0.0426 - val_loss: 0.0759\n",
      "Epoch 123/150\n",
      "595/595 [==============================] - 0s - loss: 0.0407 - val_loss: 0.0759\n",
      "Epoch 124/150\n",
      "595/595 [==============================] - 0s - loss: 0.0388 - val_loss: 0.0783\n",
      "Epoch 125/150\n",
      "595/595 [==============================] - 0s - loss: 0.0410 - val_loss: 0.0759\n",
      "Epoch 126/150\n",
      "595/595 [==============================] - 0s - loss: 0.0406 - val_loss: 0.0776\n",
      "Epoch 127/150\n",
      "595/595 [==============================] - 0s - loss: 0.0387 - val_loss: 0.0762\n",
      "Epoch 128/150\n",
      "595/595 [==============================] - 0s - loss: 0.0393 - val_loss: 0.0807\n",
      "Epoch 129/150\n",
      "595/595 [==============================] - 0s - loss: 0.0425 - val_loss: 0.0764\n",
      "Epoch 130/150\n",
      "595/595 [==============================] - 0s - loss: 0.0397 - val_loss: 0.0781\n",
      "Epoch 131/150\n",
      "595/595 [==============================] - 0s - loss: 0.0388 - val_loss: 0.0783\n",
      "Epoch 132/150\n",
      "595/595 [==============================] - 0s - loss: 0.0405 - val_loss: 0.0794\n",
      "Epoch 133/150\n",
      "595/595 [==============================] - 0s - loss: 0.0404 - val_loss: 0.0773\n",
      "Epoch 134/150\n",
      "595/595 [==============================] - 0s - loss: 0.0386 - val_loss: 0.0804\n",
      "Epoch 135/150\n",
      "595/595 [==============================] - 0s - loss: 0.0404 - val_loss: 0.0781\n",
      "Epoch 136/150\n",
      "595/595 [==============================] - 0s - loss: 0.0408 - val_loss: 0.0801\n",
      "Epoch 137/150\n",
      "595/595 [==============================] - 0s - loss: 0.0398 - val_loss: 0.0773\n",
      "Epoch 138/150\n",
      "595/595 [==============================] - 0s - loss: 0.0392 - val_loss: 0.0794\n",
      "Epoch 139/150\n",
      "595/595 [==============================] - 0s - loss: 0.0401 - val_loss: 0.0780\n",
      "Epoch 140/150\n",
      "595/595 [==============================] - 0s - loss: 0.0386 - val_loss: 0.0786\n",
      "Epoch 141/150\n",
      "595/595 [==============================] - 0s - loss: 0.0380 - val_loss: 0.0789\n",
      "Epoch 142/150\n",
      "595/595 [==============================] - 0s - loss: 0.0387 - val_loss: 0.0799\n",
      "Epoch 143/150\n",
      "595/595 [==============================] - 0s - loss: 0.0399 - val_loss: 0.0799\n",
      "Epoch 144/150\n",
      "595/595 [==============================] - 0s - loss: 0.0385 - val_loss: 0.0796\n",
      "Epoch 145/150\n",
      "595/595 [==============================] - 0s - loss: 0.0384 - val_loss: 0.0812\n",
      "Epoch 146/150\n",
      "595/595 [==============================] - 0s - loss: 0.0406 - val_loss: 0.0790\n",
      "Epoch 147/150\n",
      "595/595 [==============================] - 0s - loss: 0.0397 - val_loss: 0.0797\n",
      "Epoch 148/150\n",
      "595/595 [==============================] - 0s - loss: 0.0376 - val_loss: 0.0795\n",
      "Epoch 149/150\n",
      "595/595 [==============================] - 0s - loss: 0.0387 - val_loss: 0.0802\n",
      "Epoch 150/150\n",
      "595/595 [==============================] - 0s - loss: 0.0397 - val_loss: 0.0796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 22/22 [08:53<00:00, 24.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min, sys: 1min 47s, total: 10min 47s\n",
      "Wall time: 8min 53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for cur_tsID in tqdm(ids):\n",
    "    # prepare data\n",
    "    ts = Preprocessor.prepare_ts(Train, cur_tsID)\n",
    "    scaled = Preprocessor.scale(ts)\n",
    "    reframed = SamplesGenerator.series_to_supervised(scaled, n_in=2*DAYS_FOR_PREDICT, n_out=DAYS_FOR_PREDICT)\n",
    "    train_X, train_y, test_X, test_y = SamplesGenerator.train_and_test(reframed, test_prop=0.2)\n",
    "    \n",
    "    \n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    \n",
    "    # fitting model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(DAYS_FOR_PREDICT))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    history = model.fit(train_X, train_y, epochs=150, batch_size=128, validation_data=(test_X, test_y), \n",
    "                        verbose=1, shuffle=False)\n",
    "    \n",
    "    # make a prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    # invert scaling for forecast\n",
    "    inv_yhat = Preprocessor.invert_scaling(yhat)\n",
    "    \n",
    "    test_y = test_y.reshape((len(test_y), DAYS_FOR_PREDICT))\n",
    "    # invert scaling for actual\n",
    "    inv_y = Preprocessor.invert_scaling(test_y)\n",
    "    \n",
    "    # write forecast to sample_submission\n",
    "    Predictor.write_forecast(Train, cur_tsID, inv_yhat[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_133 (LSTM)              (None, 50)                58200     \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 30)                1530      \n",
      "=================================================================\n",
      "Total params: 59,730\n",
      "Trainable params: 59,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PREDICTED</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16707</th>\n",
       "      <td>96125.781079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16708</th>\n",
       "      <td>96897.754579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16709</th>\n",
       "      <td>97213.763991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16710</th>\n",
       "      <td>97137.992482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16711</th>\n",
       "      <td>90016.573546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16712</th>\n",
       "      <td>85677.058707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16713</th>\n",
       "      <td>96504.265901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16714</th>\n",
       "      <td>98413.493107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16715</th>\n",
       "      <td>99528.579869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16716</th>\n",
       "      <td>100408.480840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16717</th>\n",
       "      <td>98106.076689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16718</th>\n",
       "      <td>91495.913829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16719</th>\n",
       "      <td>89183.747684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16720</th>\n",
       "      <td>100057.418379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16721</th>\n",
       "      <td>101100.328730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16722</th>\n",
       "      <td>102806.919161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16723</th>\n",
       "      <td>102238.497306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16724</th>\n",
       "      <td>101188.247670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16725</th>\n",
       "      <td>93587.955642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16726</th>\n",
       "      <td>91588.417282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16727</th>\n",
       "      <td>101992.610931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16728</th>\n",
       "      <td>103229.382089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16729</th>\n",
       "      <td>104196.382006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16730</th>\n",
       "      <td>104662.731553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16731</th>\n",
       "      <td>105619.081714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16732</th>\n",
       "      <td>96308.968410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16733</th>\n",
       "      <td>93168.452486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16734</th>\n",
       "      <td>104652.230886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16735</th>\n",
       "      <td>105941.939093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16736</th>\n",
       "      <td>107119.728287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9765</th>\n",
       "      <td>12264.373335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9766</th>\n",
       "      <td>12512.831768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9767</th>\n",
       "      <td>12436.848965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9768</th>\n",
       "      <td>12518.479566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9769</th>\n",
       "      <td>12501.734392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9770</th>\n",
       "      <td>12518.726882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9771</th>\n",
       "      <td>11911.328013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9772</th>\n",
       "      <td>12015.124378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9773</th>\n",
       "      <td>12117.525115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9774</th>\n",
       "      <td>12545.260510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9775</th>\n",
       "      <td>12042.542546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9776</th>\n",
       "      <td>12049.406033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9777</th>\n",
       "      <td>11388.745608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9778</th>\n",
       "      <td>11058.424143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9779</th>\n",
       "      <td>10902.796061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9780</th>\n",
       "      <td>11058.678772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9781</th>\n",
       "      <td>11478.107261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9782</th>\n",
       "      <td>11149.409937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9783</th>\n",
       "      <td>11407.471663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9784</th>\n",
       "      <td>11355.427363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9785</th>\n",
       "      <td>10682.474503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9786</th>\n",
       "      <td>10548.735619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9787</th>\n",
       "      <td>10627.404690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9788</th>\n",
       "      <td>10575.423590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9789</th>\n",
       "      <td>10641.610366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9790</th>\n",
       "      <td>10565.566452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9791</th>\n",
       "      <td>10544.595224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9792</th>\n",
       "      <td>10216.259211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9793</th>\n",
       "      <td>10182.919552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9794</th>\n",
       "      <td>10297.419728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>660 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PREDICTED\n",
       "ID                  \n",
       "16707   96125.781079\n",
       "16708   96897.754579\n",
       "16709   97213.763991\n",
       "16710   97137.992482\n",
       "16711   90016.573546\n",
       "16712   85677.058707\n",
       "16713   96504.265901\n",
       "16714   98413.493107\n",
       "16715   99528.579869\n",
       "16716  100408.480840\n",
       "16717   98106.076689\n",
       "16718   91495.913829\n",
       "16719   89183.747684\n",
       "16720  100057.418379\n",
       "16721  101100.328730\n",
       "16722  102806.919161\n",
       "16723  102238.497306\n",
       "16724  101188.247670\n",
       "16725   93587.955642\n",
       "16726   91588.417282\n",
       "16727  101992.610931\n",
       "16728  103229.382089\n",
       "16729  104196.382006\n",
       "16730  104662.731553\n",
       "16731  105619.081714\n",
       "16732   96308.968410\n",
       "16733   93168.452486\n",
       "16734  104652.230886\n",
       "16735  105941.939093\n",
       "16736  107119.728287\n",
       "...              ...\n",
       "9765    12264.373335\n",
       "9766    12512.831768\n",
       "9767    12436.848965\n",
       "9768    12518.479566\n",
       "9769    12501.734392\n",
       "9770    12518.726882\n",
       "9771    11911.328013\n",
       "9772    12015.124378\n",
       "9773    12117.525115\n",
       "9774    12545.260510\n",
       "9775    12042.542546\n",
       "9776    12049.406033\n",
       "9777    11388.745608\n",
       "9778    11058.424143\n",
       "9779    10902.796061\n",
       "9780    11058.678772\n",
       "9781    11478.107261\n",
       "9782    11149.409937\n",
       "9783    11407.471663\n",
       "9784    11355.427363\n",
       "9785    10682.474503\n",
       "9786    10548.735619\n",
       "9787    10627.404690\n",
       "9788    10575.423590\n",
       "9789    10641.610366\n",
       "9790    10565.566452\n",
       "9791    10544.595224\n",
       "9792    10216.259211\n",
       "9793    10182.919552\n",
       "9794    10297.419728\n",
       "\n",
       "[660 rows x 1 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save forecast to disk for further submission\n",
    "sample_submission.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
